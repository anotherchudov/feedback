{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "593ec51e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-20T05:06:04.848073Z",
     "iopub.status.busy": "2022-02-20T05:06:04.847473Z",
     "iopub.status.idle": "2022-02-20T05:06:07.582488Z",
     "shell.execute_reply": "2022-02-20T05:06:07.581768Z",
     "shell.execute_reply.started": "2022-02-20T05:00:51.280128Z"
    },
    "papermill": {
     "duration": 2.75737,
     "end_time": "2022-02-20T05:06:07.582630",
     "exception": false,
     "start_time": "2022-02-20T05:06:04.825260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../input/feedbackv1/codes')\n",
    "import torch as t\n",
    "t.autograd.set_grad_enabled(False)\n",
    "from longformer.longformer import Longformer, LongformerConfig\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import RobertaTokenizerFast, DebertaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff8be2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T05:06:07.607828Z",
     "iopub.status.busy": "2022-02-20T05:06:07.607205Z",
     "iopub.status.idle": "2022-02-20T05:06:44.708742Z",
     "shell.execute_reply": "2022-02-20T05:06:44.709389Z",
     "shell.execute_reply.started": "2022-02-20T05:00:52.983953Z"
    },
    "papermill": {
     "duration": 37.118545,
     "end_time": "2022-02-20T05:06:44.709586",
     "exception": false,
     "start_time": "2022-02-20T05:06:07.591041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/feedbackv1/pretrained_checkpoints/longformer-large-4096/ were not used when initializing Longformer: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Longformer from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Longformer from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class TvmLongformer(t.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        config = LongformerConfig.from_pretrained(\n",
    "            '../input/feedbackv1/pretrained_checkpoints/longformer-large-4096/') \n",
    "        config.attention_mode = 'sliding_chunks'\n",
    "        self.feats = Longformer.from_pretrained(\n",
    "            '../input/feedbackv1/pretrained_checkpoints/longformer-large-4096/', config=config)\n",
    "        self.feats.pooler = None\n",
    "        self.class_projector = t.nn.Sequential(\n",
    "            t.nn.LayerNorm(1024),\n",
    "            t.nn.Linear(1024, 15)\n",
    "        )\n",
    "    def forward(self, tokens, mask):\n",
    "        return self.class_projector(self.feats(tokens, mask, return_dict=False)[0])\n",
    "    \n",
    "class Deberta(t.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feats = DebertaModel.from_pretrained(\n",
    "            '../input/feedbackv1/pretrained_checkpoints/deberta_large/')\n",
    "        self.feats.pooler = None\n",
    "        self.class_projector = t.nn.Sequential(\n",
    "            t.nn.LayerNorm(1024),\n",
    "            t.nn.Linear(1024, 15)\n",
    "        )\n",
    "    def forward(self, tokens, mask):\n",
    "        return self.class_projector(self.feats(tokens, mask, return_dict=False)[0])\n",
    "    \n",
    "deberta = Deberta()\n",
    "longformer = TvmLongformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "459c02c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T05:06:44.743268Z",
     "iopub.status.busy": "2022-02-20T05:06:44.739895Z",
     "iopub.status.idle": "2022-02-20T05:06:44.881554Z",
     "shell.execute_reply": "2022-02-20T05:06:44.880995Z",
     "shell.execute_reply.started": "2022-02-20T05:01:29.235432Z"
    },
    "papermill": {
     "duration": 0.163083,
     "end_time": "2022-02-20T05:06:44.881700",
     "exception": false,
     "start_time": "2022-02-20T05:06:44.718617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LongformerDataset(t.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained('../input/feedbackv1/tokenizer')\n",
    "        tokenizer.model_max_length = 4096\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = {}\n",
    "        for fname in glob('../input/feedback-prize-2021/test/*.txt'):\n",
    "            with open(fname) as f:\n",
    "                self.texts[fname.split('/')[-1].split('.')[0]] = f.read().strip()\n",
    "        self.keys = list(self.texts.keys())\n",
    "        self.space_regex = re.compile('[\\s\\n]')\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    def __getitem__(self, ix):\n",
    "        tokens_array = np.zeros(4096, 'i8')\n",
    "        mask_array = np.zeros(4096, 'f4')\n",
    "        offsets_array = np.zeros((4096, 2), 'i4')\n",
    "        \n",
    "        text = self.texts[self.keys[ix]]\n",
    "        key = self.keys[ix]\n",
    "        tokenizer_outs = self.tokenizer(text, return_offsets_mapping=True)\n",
    "        tokens = np.array(tokenizer_outs['input_ids'], 'i8')\n",
    "        mask = np.array(tokenizer_outs['attention_mask'], 'f4')\n",
    "        offsets = np.vstack(tokenizer_outs['offset_mapping']).astype('i4')\n",
    "        \n",
    "        mask[0] = 2\n",
    "        mask[-1] = 2\n",
    "        mask[tokens==4] = 2\n",
    "        mask[tokens==116] = 2\n",
    "        mask[tokens==328] = 2\n",
    "        \n",
    "        tokens_array[:len(tokens)] = tokens\n",
    "        mask_array[:len(tokens)] = mask\n",
    "        offsets_array[:len(tokens)] = offsets\n",
    "        \n",
    "        index_map = []\n",
    "        current_word = 0\n",
    "        blank = False\n",
    "        for char_ix in range(text.index(text.strip()[0]), len(text)):\n",
    "            if self.space_regex.match(text[char_ix]) is not None:\n",
    "                blank = True\n",
    "            elif blank:\n",
    "                current_word += 1\n",
    "                blank = False\n",
    "            index_map.append(current_word)\n",
    "            \n",
    "        return tokens_array, mask_array, offsets_array, index_map, key, len(tokens)\n",
    "    \n",
    "def collate_fn(ins):\n",
    "    max_len = (max(x[-1] for x in ins) + 511) // 512 * 512\n",
    "    return tuple(t.from_numpy(np.concatenate([ins[z][x][None, :max_len]\n",
    "                                              for z in range(len(ins))]))\n",
    "                 for x in range(len(ins[0]) - 3)) \\\n",
    "                 + ([x[-3] for x in ins], [x[-2] for x in ins], np.array([x[-1] for x in ins]),)    \n",
    "\n",
    "\n",
    "label_names = ['None', 'Lead', 'Position', 'Evidence', 'Claim',\n",
    "               'Concluding Statement', 'Counterclaim', 'Rebuttal']\n",
    "\n",
    "longformer_dataset = t.utils.data.DataLoader(LongformerDataset(), collate_fn=collate_fn,\n",
    "                                  batch_size=4, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6fa4aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T05:06:44.901066Z",
     "iopub.status.busy": "2022-02-20T05:06:44.900252Z",
     "iopub.status.idle": "2022-02-20T05:06:44.994685Z",
     "shell.execute_reply": "2022-02-20T05:06:44.994208Z",
     "shell.execute_reply.started": "2022-02-20T05:01:29.338667Z"
    },
    "papermill": {
     "duration": 0.104877,
     "end_time": "2022-02-20T05:06:44.994817",
     "exception": false,
     "start_time": "2022-02-20T05:06:44.889940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DebertaDataset(t.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained('../input/feedbackv1/tokenizer')\n",
    "        tokenizer.model_max_length = 4096\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = {}\n",
    "        for fname in glob('../input/feedback-prize-2021/test/*.txt'):\n",
    "            with open(fname) as f:\n",
    "                self.texts[fname.split('/')[-1].split('.')[0]] = f.read().strip()\n",
    "        self.keys = list(self.texts.keys())\n",
    "        self.space_regex = re.compile('[\\s\\n]')\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    def __getitem__(self, ix):\n",
    "        tokens_array = np.zeros(4096, 'i8')\n",
    "        mask_array = np.zeros(4096, 'f4')\n",
    "        offsets_array = np.zeros((4096, 2), 'i4')\n",
    "        \n",
    "        text = self.texts[self.keys[ix]]\n",
    "        key = self.keys[ix]\n",
    "        tokenizer_outs = self.tokenizer(text, return_offsets_mapping=True)\n",
    "        tokens = np.array(tokenizer_outs['input_ids'], 'i8')\n",
    "        mask = np.array(tokenizer_outs['attention_mask'], 'f4')\n",
    "        offsets = np.vstack(tokenizer_outs['offset_mapping']).astype('i4')\n",
    "        \n",
    "        tokens_array[:len(tokens)] = tokens\n",
    "        mask_array[:len(tokens)] = mask\n",
    "        offsets_array[:len(tokens)] = offsets\n",
    "        \n",
    "        index_map = []\n",
    "        current_word = 0\n",
    "        blank = False\n",
    "        for char_ix in range(text.index(text.strip()[0]), len(text)):\n",
    "            if self.space_regex.match(text[char_ix]) is not None:\n",
    "                blank = True\n",
    "            elif blank:\n",
    "                current_word += 1\n",
    "                blank = False\n",
    "            index_map.append(current_word)\n",
    "            \n",
    "        return tokens_array, mask_array, offsets_array, index_map, key, len(tokens)\n",
    "    \n",
    "first_batch = True\n",
    "def collate_fn(ins):\n",
    "    global first_batch\n",
    "    if first_batch:\n",
    "        max_len = 2048\n",
    "        first_batch = False\n",
    "    else:\n",
    "        max_len = (max(x[-1] for x in ins) + 7) // 8 * 8\n",
    "    return tuple(t.from_numpy(np.concatenate([ins[z][x][None, :max_len]\n",
    "                                              for z in range(len(ins))]))\n",
    "                 for x in range(len(ins[0]) - 3)) \\\n",
    "                 + ([x[-3] for x in ins], [x[-2] for x in ins], np.array([x[-1] for x in ins]),)    \n",
    "\n",
    "\n",
    "label_names = ['None', 'Lead', 'Position', 'Evidence', 'Claim',\n",
    "               'Concluding Statement', 'Counterclaim', 'Rebuttal']\n",
    "\n",
    "deberta_dataset = t.utils.data.DataLoader(DebertaDataset(), collate_fn=collate_fn,\n",
    "                                  batch_size=1, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0da6600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T05:06:45.094466Z",
     "iopub.status.busy": "2022-02-20T05:06:45.093673Z",
     "iopub.status.idle": "2022-02-20T05:06:45.097461Z",
     "shell.execute_reply": "2022-02-20T05:06:45.096959Z",
     "shell.execute_reply.started": "2022-02-20T05:01:29.447313Z"
    },
    "papermill": {
     "duration": 0.094467,
     "end_time": "2022-02-20T05:06:45.097580",
     "exception": false,
     "start_time": "2022-02-20T05:06:45.003113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_span_to_word_indices(span, index_map, bounds):\n",
    "    return (index_map[bounds[span[0], 0]], \n",
    "            index_map[bounds[span[1], 1] - 1])\n",
    "\n",
    "def calc_entity_score(span, ps, c):\n",
    "    s, e = span\n",
    "    score = (ps[s, c * 2 - 1] + ps[s + 1: e + 1, c * 2].sum())/(e - s + 1)\n",
    "    return score\n",
    "\n",
    "def extract_entities(ps, n):\n",
    "    cat_ps = ps.argmax(-1)\n",
    "    all_entities = {}\n",
    "    current_cat = None\n",
    "    current_start = None\n",
    "    for ix in range(1, n - 1):\n",
    "        if cat_ps[ix] % 2 == 1:\n",
    "            if current_cat is not None:\n",
    "                if current_cat not in all_entities:\n",
    "                    all_entities[current_cat] = []\n",
    "                all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = (cat_ps[ix] + 1) // 2\n",
    "            current_start = ix        \n",
    "        elif cat_ps[ix] == 0:\n",
    "            if current_cat is not None:\n",
    "                if current_cat not in all_entities:\n",
    "                    all_entities[current_cat] = []\n",
    "                all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = None\n",
    "        elif current_cat is not None and cat_ps[ix] != current_cat * 2:\n",
    "            if current_cat not in all_entities:\n",
    "                all_entities[current_cat] = []\n",
    "            all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = None\n",
    "    if current_cat is not None:\n",
    "        if current_cat not in all_entities:\n",
    "            all_entities[current_cat] = []\n",
    "        all_entities[current_cat].append((current_start, ix))\n",
    "\n",
    "    score_thresholds = [None, -.9, -.56, -.55, -.65, -.56, -.76, -.68]\n",
    "    score_thresholds = [None] + [x * 5 for x in [-5, -3.4, -2.065, -2.5, -6, -5.75, -5.5]]\n",
    "    \n",
    "    for cat_ix, min_len in zip(range(1, 8), (2, 2, 5, 2, 4, 3, 2)):\n",
    "        if cat_ix in all_entities:\n",
    "            all_entities[cat_ix] = [x for x in all_entities[cat_ix] if x[1] - x[0] + 1 >= min_len and calc_entity_score(x, ps, cat_ix) > score_thresholds[cat_ix]]\n",
    "            \n",
    "        \n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00cabccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T05:06:45.117889Z",
     "iopub.status.busy": "2022-02-20T05:06:45.117303Z",
     "iopub.status.idle": "2022-02-20T05:06:48.785422Z",
     "shell.execute_reply": "2022-02-20T05:06:48.784498Z",
     "shell.execute_reply.started": "2022-02-20T05:01:29.541742Z"
    },
    "papermill": {
     "duration": 3.679965,
     "end_time": "2022-02-20T05:06:48.785559",
     "exception": false,
     "start_time": "2022-02-20T05:06:45.105594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoints = glob('../input/feedbackv1/weights/*attn2') + glob('../input/feedbackv1/weights/mnli*')\n",
    "checkpoints.extend(glob('../input/feedbackv2/debertav1/*'))\n",
    "longformer.eval().cuda();\n",
    "deberta.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e6f9aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T05:06:48.811580Z",
     "iopub.status.busy": "2022-02-20T05:06:48.810743Z",
     "iopub.status.idle": "2022-02-20T05:10:49.622443Z",
     "shell.execute_reply": "2022-02-20T05:10:49.621806Z",
     "shell.execute_reply.started": "2022-02-20T05:01:33.242015Z"
    },
    "papermill": {
     "duration": 240.829005,
     "end_time": "2022-02-20T05:10:49.622605",
     "exception": false,
     "start_time": "2022-02-20T05:06:48.793600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_outs = np.zeros((len(glob('../input/feedback-prize-2021/test/*.txt')), 2048, 15), 'f4')\n",
    "all_bounds = np.zeros((len(glob('../input/feedback-prize-2021/test/*.txt')), 2048, 2), 'i4')\n",
    "all_token_nums = np.zeros((len(glob('../input/feedback-prize-2021/test/*.txt')),), 'i4')\n",
    "all_word_indices = []\n",
    "all_sample_ids = []\n",
    "for checkpoint_ix, checkpoint in enumerate(checkpoints):\n",
    "    if checkpoint[-1] == '2':\n",
    "        longformer.load_state_dict(t.load(checkpoint))\n",
    "        model = longformer\n",
    "        dataset = longformer_dataset\n",
    "        is_longformer = True\n",
    "    else:\n",
    "        deberta.load_state_dict(t.load(checkpoint))\n",
    "        model = deberta\n",
    "        dataset = deberta_dataset\n",
    "        is_longformer = False\n",
    "    ix = 0\n",
    "    for batch in dataset:\n",
    "        tokens, mask, bounds, word_indices, sample_ids, num_tokens = batch\n",
    "        batch_size, batch_len = tokens.shape[:2]\n",
    "        if is_longformer:\n",
    "            outs = t.log_softmax(model(tokens.cuda(), mask.cuda()), -1)\n",
    "        else:\n",
    "            outs = t.log_softmax(model(tokens.cuda(), (mask.cuda() > 0).float()), -1)\n",
    "        all_outs[ix: ix + batch_size, :batch_len] += outs.cpu().numpy()\n",
    "        if checkpoint_ix == 0:\n",
    "            all_bounds[ix: ix + batch_size, :batch_len] = bounds\n",
    "            all_token_nums[ix: ix + batch_size] = num_tokens\n",
    "            all_word_indices.extend(word_indices)\n",
    "            all_sample_ids.extend(sample_ids)\n",
    "        ix += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c834a381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T05:10:49.648147Z",
     "iopub.status.busy": "2022-02-20T05:10:49.645915Z",
     "iopub.status.idle": "2022-02-20T05:10:49.657481Z",
     "shell.execute_reply": "2022-02-20T05:10:49.657057Z",
     "shell.execute_reply.started": "2022-02-20T05:05:30.563613Z"
    },
    "papermill": {
     "duration": 0.026326,
     "end_time": "2022-02-20T05:10:49.657591",
     "exception": false,
     "start_time": "2022-02-20T05:10:49.631265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_sample_ids = []\n",
    "sub_cat_names = []\n",
    "sub_spans = []\n",
    "for sample_ix in range(len(all_token_nums)):\n",
    "    predicted_spans = {x: [map_span_to_word_indices(span, all_word_indices[sample_ix],\n",
    "                                                    all_bounds[sample_ix]) for span in y] \n",
    "                       for x, y in extract_entities(all_outs[sample_ix], \n",
    "                                                    all_token_nums[sample_ix]).items()}\n",
    "    for cat_ix in predicted_spans:\n",
    "        for entity in predicted_spans[cat_ix]:\n",
    "            sub_sample_ids.append(all_sample_ids[sample_ix])\n",
    "            sub_cat_names.append(label_names[cat_ix])\n",
    "            sub_spans.append(' '.join(str(x) for x in range(entity[0], entity[1] + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfba98ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T05:10:49.687227Z",
     "iopub.status.busy": "2022-02-20T05:10:49.674557Z",
     "iopub.status.idle": "2022-02-20T05:10:49.695287Z",
     "shell.execute_reply": "2022-02-20T05:10:49.694841Z",
     "shell.execute_reply.started": "2022-02-20T05:05:30.583938Z"
    },
    "papermill": {
     "duration": 0.03014,
     "end_time": "2022-02-20T05:10:49.695402",
     "exception": false,
     "start_time": "2022-02-20T05:10:49.665262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'id': sub_sample_ids, \n",
    "              'class': sub_cat_names,\n",
    "              'predictionstring': sub_spans}).to_csv('submission.csv', index=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 296.46522,
   "end_time": "2022-02-20T05:10:53.272549",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-20T05:05:56.807329",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
