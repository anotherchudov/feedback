{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16869ad8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-15T12:56:09.078687Z",
     "iopub.status.busy": "2022-02-15T12:56:09.077771Z",
     "iopub.status.idle": "2022-02-15T12:56:11.596438Z",
     "shell.execute_reply": "2022-02-15T12:56:11.596915Z",
     "shell.execute_reply.started": "2022-02-15T12:54:17.185492Z"
    },
    "papermill": {
     "duration": 2.541992,
     "end_time": "2022-02-15T12:56:11.597181",
     "exception": false,
     "start_time": "2022-02-15T12:56:09.055189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../input/feedbackv1/codes')\n",
    "import torch as t\n",
    "t.autograd.set_grad_enabled(False)\n",
    "from longformer.longformer import Longformer, LongformerConfig\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import RobertaTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f08417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T12:56:11.618796Z",
     "iopub.status.busy": "2022-02-15T12:56:11.617083Z",
     "iopub.status.idle": "2022-02-15T12:56:11.619447Z",
     "shell.execute_reply": "2022-02-15T12:56:11.619845Z",
     "shell.execute_reply.started": "2022-02-15T12:54:18.188215Z"
    },
    "papermill": {
     "duration": 0.014728,
     "end_time": "2022-02-15T12:56:11.619996",
     "exception": false,
     "start_time": "2022-02-15T12:56:11.605268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec28167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T12:56:11.640774Z",
     "iopub.status.busy": "2022-02-15T12:56:11.640124Z",
     "iopub.status.idle": "2022-02-15T12:56:29.165846Z",
     "shell.execute_reply": "2022-02-15T12:56:29.165432Z",
     "shell.execute_reply.started": "2022-02-15T12:54:18.199048Z"
    },
    "papermill": {
     "duration": 17.538995,
     "end_time": "2022-02-15T12:56:29.165998",
     "exception": false,
     "start_time": "2022-02-15T12:56:11.627003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/feedbackv1/pretrained_checkpoints/longformer-large-4096/ were not used when initializing Longformer: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing Longformer from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Longformer from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class TvmLongformer(t.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        config = LongformerConfig.from_pretrained(\n",
    "            '../input/feedbackv1/pretrained_checkpoints/longformer-large-4096/') \n",
    "        config.attention_mode = 'sliding_chunks'\n",
    "        self.feats = Longformer.from_pretrained(\n",
    "            '../input/feedbackv1/pretrained_checkpoints/longformer-large-4096/', config=config)\n",
    "        self.feats.pooler = None\n",
    "        self.class_projector = t.nn.Sequential(\n",
    "            t.nn.LayerNorm(1024),\n",
    "            t.nn.Linear(1024, 15)\n",
    "        )\n",
    "    def forward(self, tokens, mask):\n",
    "        return self.class_projector(self.feats(tokens, mask, return_dict=False)[0])\n",
    "    \n",
    "model = TvmLongformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b170f20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T12:56:29.198372Z",
     "iopub.status.busy": "2022-02-15T12:56:29.195655Z",
     "iopub.status.idle": "2022-02-15T12:56:29.320446Z",
     "shell.execute_reply": "2022-02-15T12:56:29.320843Z",
     "shell.execute_reply.started": "2022-02-15T12:54:23.895622Z"
    },
    "papermill": {
     "duration": 0.146747,
     "end_time": "2022-02-15T12:56:29.321019",
     "exception": false,
     "start_time": "2022-02-15T12:56:29.174272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(t.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained('../input/feedbackv1/tokenizer')\n",
    "        tokenizer.model_max_length = 4096\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = {}\n",
    "        for fname in glob('../input/feedback-prize-2021/test/*.txt'):\n",
    "            with open(fname) as f:\n",
    "                self.texts[fname.split('/')[-1].split('.')[0]] = f.read().strip()\n",
    "        self.keys = list(self.texts.keys())\n",
    "        self.space_regex = re.compile('[\\s\\n]')\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    def __getitem__(self, ix):\n",
    "        tokens_array = np.zeros(4096, 'i8')\n",
    "        mask_array = np.zeros(4096, 'f4')\n",
    "        offsets_array = np.zeros((4096, 2), 'i4')\n",
    "        \n",
    "        text = self.texts[self.keys[ix]]\n",
    "        key = self.keys[ix]\n",
    "        tokenizer_outs = self.tokenizer(text, return_offsets_mapping=True)\n",
    "        tokens = np.array(tokenizer_outs['input_ids'], 'i8')\n",
    "        mask = np.array(tokenizer_outs['attention_mask'], 'f4')\n",
    "        offsets = np.vstack(tokenizer_outs['offset_mapping']).astype('i4')\n",
    "        \n",
    "        mask[0] = 2\n",
    "        mask[-1] = 2\n",
    "        mask[tokens==4] = 2\n",
    "        mask[tokens==116] = 2\n",
    "        mask[tokens==328] = 2\n",
    "        \n",
    "        tokens_array[:len(tokens)] = tokens\n",
    "        mask_array[:len(tokens)] = mask\n",
    "        offsets_array[:len(tokens)] = offsets\n",
    "        \n",
    "        index_map = []\n",
    "        current_word = 0\n",
    "        blank = False\n",
    "        for char_ix in range(text.index(text.strip()[0]), len(text)):\n",
    "            if self.space_regex.match(text[char_ix]) is not None:\n",
    "                blank = True\n",
    "            elif blank:\n",
    "                current_word += 1\n",
    "                blank = False\n",
    "            index_map.append(current_word)\n",
    "            \n",
    "        return tokens_array, mask_array, offsets_array, index_map, key, len(tokens)\n",
    "    \n",
    "def collate_fn(ins):\n",
    "    max_len = (max(x[-1] for x in ins) + 511) // 512 * 512\n",
    "    return tuple(t.from_numpy(np.concatenate([ins[z][x][None, :max_len]\n",
    "                                              for z in range(len(ins))]))\n",
    "                 for x in range(len(ins[0]) - 3)) \\\n",
    "                 + ([x[-3] for x in ins], [x[-2] for x in ins], np.array([x[-1] for x in ins]),)    \n",
    "\n",
    "\n",
    "label_names = ['None', 'Lead', 'Position', 'Evidence', 'Claim',\n",
    "               'Concluding Statement', 'Counterclaim', 'Rebuttal']\n",
    "\n",
    "dataset = t.utils.data.DataLoader(Dataset(), collate_fn=collate_fn,\n",
    "                                  batch_size=4, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "885aeb63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T12:56:29.343959Z",
     "iopub.status.busy": "2022-02-15T12:56:29.343417Z",
     "iopub.status.idle": "2022-02-15T12:56:29.346981Z",
     "shell.execute_reply": "2022-02-15T12:56:29.346559Z",
     "shell.execute_reply.started": "2022-02-15T12:54:23.994605Z"
    },
    "papermill": {
     "duration": 0.018468,
     "end_time": "2022-02-15T12:56:29.347089",
     "exception": false,
     "start_time": "2022-02-15T12:56:29.328621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_entities(ps, n):\n",
    "    cat_ps = ps.argmax(-1)\n",
    "    all_entities = {}\n",
    "    current_cat = None\n",
    "    current_start = None\n",
    "    for ix in range(1, n - 1):\n",
    "        if cat_ps[ix] % 2 == 1:\n",
    "            if current_cat is not None:\n",
    "                if current_cat not in all_entities:\n",
    "                    all_entities[current_cat] = []\n",
    "                all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = (cat_ps[ix] + 1) // 2\n",
    "            current_start = ix\n",
    "    if current_cat is not None:\n",
    "        if current_cat not in all_entities:\n",
    "            all_entities[current_cat] = []\n",
    "        all_entities[current_cat].append((current_start, ix))\n",
    "    return all_entities\n",
    "        \n",
    "\n",
    "def map_span_to_word_indices(span, index_map, bounds):\n",
    "    return (index_map[bounds[span[0], 0]], \n",
    "            index_map[bounds[span[1], 1] - 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6f89e30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T12:56:29.409450Z",
     "iopub.status.busy": "2022-02-15T12:56:29.408764Z",
     "iopub.status.idle": "2022-02-15T12:56:32.492798Z",
     "shell.execute_reply": "2022-02-15T12:56:32.493558Z",
     "shell.execute_reply.started": "2022-02-15T12:54:24.004238Z"
    },
    "papermill": {
     "duration": 3.139145,
     "end_time": "2022-02-15T12:56:32.493780",
     "exception": false,
     "start_time": "2022-02-15T12:56:29.354635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_checkpoint_template = '../input/feedbackv1/weights/fold{}_minlr3.2e-05_maxlr3.2e-05_wd0.01_warmup500_gradnorm280_biasdecayFalse_ls0.1_wp0.1_data1_attn2'\n",
    "model.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d6e586c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T12:56:32.527130Z",
     "iopub.status.busy": "2022-02-15T12:56:32.526370Z",
     "iopub.status.idle": "2022-02-15T12:57:43.967473Z",
     "shell.execute_reply": "2022-02-15T12:57:43.967926Z",
     "shell.execute_reply.started": "2022-02-15T12:54:26.050539Z"
    },
    "papermill": {
     "duration": 71.461684,
     "end_time": "2022-02-15T12:57:43.968110",
     "exception": false,
     "start_time": "2022-02-15T12:56:32.506426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_outs = np.zeros((len(glob('../input/feedback-prize-2021/test/*.txt')), 2048, 15), 'f4')\n",
    "all_bounds = np.zeros((len(glob('../input/feedback-prize-2021/test/*.txt')), 2048, 2), 'i4')\n",
    "all_token_nums = np.zeros((len(glob('../input/feedback-prize-2021/test/*.txt')),), 'i4')\n",
    "all_word_indices = []\n",
    "all_sample_ids = []\n",
    "for fold in range(5):\n",
    "    model.load_state_dict(t.load(weight_checkpoint_template.format(fold)));\n",
    "    ix = 0\n",
    "    for batch in dataset:\n",
    "        tokens, mask, bounds, word_indices, sample_ids, num_tokens = batch\n",
    "        batch_size, batch_len = tokens.shape[:2]\n",
    "        outs = t.log_softmax(model(tokens.cuda(), mask.cuda()), -1)\n",
    "        all_outs[ix: ix + batch_size, :batch_len] += outs.cpu().numpy()\n",
    "        if fold == 0:\n",
    "            all_bounds[ix: ix + batch_size, :batch_len] = bounds\n",
    "            all_token_nums[ix: ix + batch_size] = num_tokens\n",
    "            all_word_indices.extend(word_indices)\n",
    "            all_sample_ids.extend(sample_ids)\n",
    "        ix += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f4a1124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T12:57:43.994337Z",
     "iopub.status.busy": "2022-02-15T12:57:43.993479Z",
     "iopub.status.idle": "2022-02-15T12:57:43.999826Z",
     "shell.execute_reply": "2022-02-15T12:57:43.999382Z",
     "shell.execute_reply.started": "2022-02-15T12:55:38.509745Z"
    },
    "papermill": {
     "duration": 0.023559,
     "end_time": "2022-02-15T12:57:43.999952",
     "exception": false,
     "start_time": "2022-02-15T12:57:43.976393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_sample_ids = []\n",
    "sub_cat_names = []\n",
    "sub_spans = []\n",
    "for sample_ix in range(len(all_token_nums)):\n",
    "    predicted_spans = {x: [map_span_to_word_indices(span, all_word_indices[sample_ix],\n",
    "                                                    all_bounds[sample_ix]) for span in y] \n",
    "                       for x, y in extract_entities(all_outs[sample_ix], \n",
    "                                                    all_token_nums[sample_ix]).items()}\n",
    "    predicted_spans = {x: [z for z in y if z[1] - z[0] >= min_len] \n",
    "                       for x, y in predicted_spans.items()}\n",
    "    for cat_ix in predicted_spans:\n",
    "        for entity in predicted_spans[cat_ix]:\n",
    "            sub_sample_ids.append(all_sample_ids[sample_ix])\n",
    "            sub_cat_names.append(label_names[cat_ix])\n",
    "            sub_spans.append(' '.join(str(x) for x in range(entity[0], entity[1] + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a00c951b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T12:57:44.028176Z",
     "iopub.status.busy": "2022-02-15T12:57:44.027449Z",
     "iopub.status.idle": "2022-02-15T12:57:44.035211Z",
     "shell.execute_reply": "2022-02-15T12:57:44.035668Z",
     "shell.execute_reply.started": "2022-02-15T12:55:38.526440Z"
    },
    "papermill": {
     "duration": 0.028181,
     "end_time": "2022-02-15T12:57:44.035782",
     "exception": false,
     "start_time": "2022-02-15T12:57:44.007601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'id': sub_sample_ids, \n",
    "              'class': sub_cat_names,\n",
    "              'predictionstring': sub_spans}).to_csv('submission.csv', index=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 106.396967,
   "end_time": "2022-02-15T12:57:46.771934",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-15T12:56:00.374967",
   "version": "2.3.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
