{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b200f4",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "✅ **needed to be checked**\n",
    "- loss function (cross entropy loss and rce loss)\n",
    "- collate (does it properly implemented?)\n",
    "    - assumed torch padding function but was not used\n",
    "- entity_extraction function\n",
    "- does deverta accepted long sequence length even the config setted by 512 seq len?\n",
    "\n",
    "✅ **needed to be added**\n",
    "- training depend on device\n",
    "- model output to prediction string converter\n",
    "- f1macro calculation by prediction string\n",
    "- training mode without gradient accumulation\n",
    "\n",
    "---\n",
    "\n",
    "- Environment Setting\n",
    "- Argument Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd7c2d",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ab4178",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T14:08:17.822819Z",
     "start_time": "2022-02-24T14:08:17.819459Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "DATASET_PATH = ('../../feedback-prize-2021')\n",
    "\n",
    "sys.path.insert(0, './codes')\n",
    "sys.path.append('longformer/tvm/python/')\n",
    "sys.path.append('longformer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a253e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_WANDB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a83c79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:04:30.516769Z",
     "start_time": "2022-02-26T15:04:28.810953Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import easydict\n",
    "import argparse\n",
    "\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import h5py\n",
    "import ftfy\n",
    "import dill as pickle\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from transformers import DebertaV2Model\n",
    "\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# from longformer.longformer import Longformer, LongformerConfig, RobertaModel\n",
    "# from longformer.sliding_chunks import pad_to_window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb3b605",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T14:15:34.246714Z",
     "start_time": "2022-02-24T14:15:34.242649Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6997bb5",
   "metadata": {},
   "source": [
    "**why using `os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"`?**\n",
    "- [torch.use_deterministic_algorithms](https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html)\n",
    "> **A handful of CUDA operations are nondeterministic if the CUDA version is 10.2 or greater**, unless the environment variable `CUBLAS_WORKSPACE_CONFIG=:4096:8` or `CUBLAS_WORKSPACE_CONFIG=:16:8` is set. See the CUDA documentation for more details: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility If one of these environment variable configurations is not set, a RuntimeError will be raised from these operations when called with CUDA tensors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c96213b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Argument Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d834b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T14:23:23.261447Z",
     "start_time": "2022-02-24T14:23:23.254004Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    parser = argparse.ArgumentParser(description=\"use huggingface models\")\n",
    "    parser.add_argument(\"--wandb_user\", default='ducky', type=str)\n",
    "    parser.add_argument(\"--wandb_project\", default='feedback_deberta_large', type=str)\n",
    "    parser.add_argument(\"--dataset_path\", default='../../feedback-prize-2021', type=str)\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument(\"--min_len\", default=0, type=int)\n",
    "    parser.add_argument(\"--wd\", default=1e-2, type=float)\n",
    "    parser.add_argument(\"--weights_pow\", default=0.1, type=float)\n",
    "    parser.add_argument(\"--use_groupped_weights\", default=False, type=bool)\n",
    "    parser.add_argument(\"--global_attn\", default=False, type=int)\n",
    "    parser.add_argument(\"--label_smoothing\", default= 0.1, type=float)\n",
    "    parser.add_argument(\"--extra_dense\", default= False, type=bool)\n",
    "    parser.add_argument(\"--epochs\", default=9, type=int)\n",
    "    parser.add_argument(\"--batch_size\", default=4, type=int)\n",
    "    parser.add_argument(\"--grad_acc_steps\", default=2, type=int)\n",
    "    parser.add_argument(\"--grad_checkpt\", default=True, type=bool)\n",
    "    parser.add_argument(\"--data_prefix\", default='', type=str)\n",
    "    parser.add_argument(\"--max_grad_norm\", default=35 * 8, type=int)\n",
    "    parser.add_argument(\"--start_eval_at\", default=0, type=int)\n",
    "    parser.add_argument(\"--lr\", default=32e-6, type=float)\n",
    "    parser.add_argument(\"--min_lr\", default=32e-6, type=float)\n",
    "    parser.add_argument(\"--dataset_version\", default=2, type=int)\n",
    "    parser.add_argument(\"--warmup_steps\", default=500, type=int)\n",
    "    parser.add_argument(\"--rce_weight\", default=0.1, type=float)\n",
    "    parser.add_argument(\"--ce_weight\", default=0.9, type=float)\n",
    "    parser.add_argument(\"--dropout_ratio\", default=0.0, type=float)\n",
    "    parser.add_argument(\"--decay_bias\", default=False, type=bool)\n",
    "    parser.add_argument(\"--val_fold\", default=0, type=int)\n",
    "    parser.add_argument(\"--num_worker\", default=8, type=int)\n",
    "    parser.add_argument(\"--model_name\", default=\"microsoft/deberta-v3-large\", type=str)\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"do not modify!\")\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    if args.local_rank !=-1:\n",
    "        print('[ DDP ] local rank', args.local_rank)\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        dist.init_process_group(backend='nccl')\n",
    "        args.device = torch.device(\"cuda\", args.local_rank)\n",
    "        args.rank = torch.distributed.get_rank()\n",
    "        args.world_size = torch.distributed.get_world_size()  \n",
    "\n",
    "        # checking settings for distributed training\n",
    "        assert args.batch_size % args.world_size == 0, f'--batch_size {args.batch_size} must be multiple of world size'\n",
    "        assert torch.cuda.device_count() > args.local_rank, 'insufficient CUDA devices for DDP command'\n",
    "\n",
    "        args.ddp = True\n",
    "    else:\n",
    "        args.device = torch.device(\"cuda\")\n",
    "        args.rank = 0\n",
    "        args.ddp = False\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b2ca90",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "args = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00e56b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b57f981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.utils import get_token_weights\n",
    "from module.utils import get_prepare_data\n",
    "from module.utils import get_all_texts\n",
    "from module.utils import get_id_to_ix_map\n",
    "from module.utils import get_fold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c3dca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_files(args):\n",
    "    token_weights = get_token_weights(args.use_groupped_weights, args.weights_pow)\n",
    "    data = get_prepare_data()\n",
    "    csv = pd.read_csv(osp.join(args.dataset_path, 'train.csv'))\n",
    "    all_texts = get_all_texts(args)\n",
    "    id_to_ix_map = get_id_to_ix_map()\n",
    "    data_splits = get_fold_data()\n",
    "\n",
    "    # text_id example `16585724607E`\n",
    "    train_text_ids = [text_id for fold in range(5) if fold != args.val_fold for text_id in data_splits[args.seed][250]['normed'][fold]]\n",
    "    val_text_ids = data_splits[args.seed][250]['normed'][args.val_fold]\n",
    "\n",
    "    train_ids = [id_to_ix_map[text_id] for text_id in train_text_ids]\n",
    "    val_ids = [id_to_ix_map[text_id] for text_id in val_text_ids]\n",
    "\n",
    "    return all_texts, token_weights, data, csv, train_ids, val_ids, train_text_ids, val_text_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7774e93",
   "metadata": {},
   "source": [
    "## Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5387adc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:08:49.547749Z",
     "start_time": "2022-02-26T15:08:49.534120Z"
    }
   },
   "outputs": [],
   "source": [
    "if USE_WANDB:\n",
    "    run = wandb.init(entity='ducky', project='feedback_debertav3_large')\n",
    "    run.name = f'v3_fold{args.val_fold}_minlr{args.min_lr}_maxlr{args.lr}_wd{args.weight_decay}_warmup{args.warmup_steps}_gradnorm{args.max_grad_norm}_biasdecay{args.decay_bias}_ls{args.label_smoothing}_wp{args.weights_pow}_data{args.dataset_version}_rce{args.rce_weight}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2ad9dab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T14:26:46.305086Z",
     "start_time": "2022-02-24T14:26:46.301370Z"
    }
   },
   "outputs": [],
   "source": [
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d7840",
   "metadata": {},
   "source": [
    "## Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "362dcb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, ids):\n",
    "        self.args = args\n",
    "        self.ids = ids\n",
    "        self.data = h5py.File(args.h5py_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_id = self.ids[idx]\n",
    "        \n",
    "        # TODO: Add Tokenizer and directly tokenize for typo injections\n",
    "        tokens = self.data['tokens'][text_id]\n",
    "        attention_mask = self.data['attention_masks'][text_id]\n",
    "        num_tokens = self.data['num_tokens'][text_id, 0]\n",
    "        \n",
    "        # label\n",
    "        cbio_labels = self.data[f'{self.args.data_prefix}cbio_labels'][text_id]\n",
    "        cbio_labels *= (1 - self.args.label_smoothing)\n",
    "        cbio_labels += self.args.label_smoothing / 15\n",
    "        \n",
    "        # label mask\n",
    "        label_mask = np.zeros_like(attention_mask)\n",
    "        argmax_labels = cbio_labels.argmax(-1)\n",
    "        \n",
    "        for i in range(1, 15):\n",
    "            label_mask[argmax_labels == i] = self.args.token_weights[i]\n",
    "        \n",
    "        zero_label_mask = argmax_labels == 0\n",
    "        zero_label_mask[num_tokens - 1:] = False\n",
    "\n",
    "        label_mask[zero_label_mask] = self.args.token_weights[0]\n",
    "        label_mask[0] = 0\n",
    "\n",
    "        return tokens, attention_mask, cbio_labels, label_mask, num_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, ids, val_files):\n",
    "        self.args = args\n",
    "        self.ids = ids\n",
    "        self.val_files = val_files\n",
    "\n",
    "        self.data = h5py.File(args.h5py_path)\n",
    "        self.csv = pd.read_csv(args.csv_path)\n",
    "        self.space_regex = re.compile('[\\s\\n]')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def split_predstring(self, x):\n",
    "        vals = x.split()\n",
    "        return int(vals[0]), int(vals[-1])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_id = self.ids[idx]\n",
    "        text = all_texts[self.val_files[idx]]\n",
    "        gt_dict = {}\n",
    "        sample_df = self.csv.loc[self.csv.id == self.val_files[idx]]\n",
    "        sample_df = self.csv.query(\"id == @self.val_files[idx]\")\n",
    "        for class_i in range(1, 8):\n",
    "            class_name = self.args.label_names[class_i]\n",
    "            class_entities = sample_df.loc[sample_df.discourse_type == class_name]\n",
    "            if len(class_entities):\n",
    "                gt_dict[class_i] = [(x[0], x[1]) for x in class_entities.predictionstring.map(self.split_predstring)]\n",
    "        \n",
    "        tokens = self.data['tokens'][text_id]\n",
    "        attention_mask = self.data['attention_masks'][text_id]\n",
    "        num_tokens = self.data['num_tokens'][text_id, 0]\n",
    "        token_bounds = self.data['token_offsets'][text_id]\n",
    "        cbio_labels = self.data['cbio_labels'][text_id]\n",
    "        \n",
    "        label_mask = np.zeros_like(attention_mask)\n",
    "        argmax_labels = cbio_labels.argmax(-1)\n",
    "        for class_i in range(1, 15):\n",
    "            label_mask[argmax_labels == class_i] = self.args.token_weights[class_i]\n",
    "\n",
    "        zero_label_mask = argmax_labels == 0\n",
    "        zero_label_mask[num_tokens - 1:] = False\n",
    "        label_mask[zero_label_mask] = self.args.token_weights[0]\n",
    "        label_mask[0] = 0 # what is this for?\n",
    "        \n",
    "        index_map = []\n",
    "        current_word = 0\n",
    "        blank = False\n",
    "        for char_ix in range(text.index(text.strip()[0]), len(text)):\n",
    "            if self.space_regex.match(text[char_ix]) is not None:\n",
    "                blank = True\n",
    "            elif blank:\n",
    "                current_word += 1\n",
    "                blank = False\n",
    "            index_map.append(current_word)\n",
    "        \n",
    "        return tokens, attention_mask, cbio_labels, label_mask, token_bounds, gt_dict, index_map, num_tokens"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
