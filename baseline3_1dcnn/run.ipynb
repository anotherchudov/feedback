{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61afee34",
   "metadata": {},
   "source": [
    "# run training\n",
    "\n",
    "> notebook to run the `train.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f259f5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T02:58:00.112394Z",
     "start_time": "2022-02-26T02:58:00.108480Z"
    }
   },
   "outputs": [],
   "source": [
    "install_pytorch = False\n",
    "if install_pytorch:\n",
    "    !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d4a01",
   "metadata": {},
   "source": [
    "## Single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80571bec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T09:03:14.575848Z",
     "start_time": "2022-02-26T02:58:05.034274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mducky\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchocolate-glitter-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ducky/feedback_deberta_large\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ducky/feedback_deberta_large/runs/2ensz94m\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/feedback/working/feedback/baseline3_1dcnn/wandb/run-20220226_025808-2ensz94m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "TRAIN  STEP 3113 loss:  1.3162: 100%|███████| 3114/3114 [38:55<00:00,  1.33it/s]\n",
      "100%|█████████████████████████████████████████| 785/785 [02:06<00:00,  6.20it/s]\n",
      "0.5961010131020282\n",
      "TRAIN  STEP 6227 loss:  1.1472: 100%|███████| 3114/3114 [38:27<00:00,  1.35it/s]\n",
      "100%|█████████████████████████████████████████| 785/785 [02:06<00:00,  6.23it/s]\n",
      "0.6157632277731396\n",
      "TRAIN  STEP 9341 loss:  1.0721: 100%|███████| 3114/3114 [38:22<00:00,  1.35it/s]\n",
      "100%|█████████████████████████████████████████| 785/785 [02:06<00:00,  6.23it/s]\n",
      "0.6345435222207689\n",
      "TRAIN  STEP 12455 loss:  1.0088: 100%|██████| 3114/3114 [38:24<00:00,  1.35it/s]\n",
      "100%|█████████████████████████████████████████| 785/785 [02:05<00:00,  6.23it/s]\n",
      "0.6456521608467563\n",
      "TRAIN  STEP 15569 loss:  0.9590: 100%|██████| 3114/3114 [38:23<00:00,  1.35it/s]\n",
      "100%|█████████████████████████████████████████| 785/785 [02:05<00:00,  6.23it/s]\n",
      "0.6500347755930033\n",
      "TRAIN  STEP 18683 loss:  0.9193: 100%|██████| 3114/3114 [38:20<00:00,  1.35it/s]\n",
      "100%|█████████████████████████████████████████| 785/785 [02:06<00:00,  6.22it/s]\n",
      "0.6543366567930071\n",
      "TRAIN  STEP 21797 loss:  0.8901: 100%|██████| 3114/3114 [38:14<00:00,  1.36it/s]\n",
      "100%|█████████████████████████████████████████| 785/785 [02:06<00:00,  6.23it/s]\n",
      "0.6560694170373368\n",
      "TRAIN  STEP 24911 loss:  0.8663: 100%|██████| 3114/3114 [38:21<00:00,  1.35it/s]\n",
      "100%|█████████████████████████████████████████| 785/785 [02:06<00:00,  6.23it/s]\n",
      "0.6558614189264559\n",
      "TRAIN  STEP 28025 loss:  0.8426: 100%|██████| 3114/3114 [38:18<00:00,  1.35it/s]\n",
      "100%|█████████████████████████████████████████| 785/785 [02:06<00:00,  6.23it/s]\n",
      "0.653895407018805\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 175, in <module>\n",
      "    'Rebuttal']\n",
      "  File \"/home/feedback/working/feedback/baseline3_1dcnn/module/trainer.py\", line 134, in train\n",
      "    val_ce, val_accs, val_labels, f1s, rec, prec = self.valid_one_epoch(epoch)\n",
      "UnboundLocalError: local variable 'best_f1' referenced before assignment\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1982315... (failed 1). Press ctrl-c to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      ValSWA_ACC#ALL ▁▄█▆█▇▅▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  ValSWA_ACC#Claim_B ▁▃█▇▆▅█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  ValSWA_ACC#Claim_I ▁▃██▄▄█▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ValSWA_ACC#Concluding Statement_B ▅█▄▃▁▂▁▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ValSWA_ACC#Concluding Statement_I ▆█▆▃▃▁▄▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ValSWA_ACC#Counterclaim_B ▄█▅▄▁▃▁▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ValSWA_ACC#Counterclaim_I ██▅▃▃▅▁▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               ValSWA_ACC#Evidence_B ▁▆█▃▁▃▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               ValSWA_ACC#Evidence_I ▃▃▄▅█▇▃▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   ValSWA_ACC#Lead_B ▅▇▆▆▁▄▆▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   ValSWA_ACC#Lead_I ▆▅▇▂▁▄▅▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     ValSWA_ACC#None ▇▅▁▃▄▆███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               ValSWA_ACC#Position_B ▃▆█▂▂▃▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               ValSWA_ACC#Position_I ▅▇█▅▃▅▁▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               ValSWA_ACC#Rebuttal_B ▆█▅▃▁▆▅▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               ValSWA_ACC#Rebuttal_I ▁▆▅▅▃▆▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          ValSWA_A_B ▃██▄▁▄▃▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          ValSWA_A_I ▄█▇▄▁▄▅▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       ValSWA_A_MEAN ▄██▄▁▄▄▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     ValSWA_F1_Claim ▁▂▂▆▅▇▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  ValSWA_F1_Evidence ▂▁▃▄▆██▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      ValSWA_F1_Lead ▁▂▅█▆▇▃█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  ValSWA_F1_Position ▁▄▇█▇█▇█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      ValSWA_MacroF1 ▁▃▅▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   ValSWA_Prec_Claim ▁▁▃▇█▆▇▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                ValSWA_Prec_Evidence ▃▁▃▅█▇█▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    ValSWA_Prec_Lead ▃▁▂██▇▆█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                ValSWA_Prec_Position ▁▄▃▆▇█▄▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    ValSWA_Rec_Claim ▅▇▅▄▁█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 ValSWA_Rec_Evidence ▄█▅▃▁▅▃▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     ValSWA_Rec_Lead ▂▅█▄▂▄▁▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 ValSWA_Rec_Position ▁▂█▇▅▅▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           ValSWA_ce ▂▁▁▃▅▅▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      ValSWA_ACC#ALL 0.79345\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  ValSWA_ACC#Claim_B 0.63233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  ValSWA_ACC#Claim_I 0.61282\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ValSWA_ACC#Concluding Statement_B 0.76084\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ValSWA_ACC#Concluding Statement_I 0.91683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ValSWA_ACC#Counterclaim_B 0.55219\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ValSWA_ACC#Counterclaim_I 0.53652\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               ValSWA_ACC#Evidence_B 0.59905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               ValSWA_ACC#Evidence_I 0.86192\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   ValSWA_ACC#Lead_B 0.91966\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   ValSWA_ACC#Lead_I 0.89623\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     ValSWA_ACC#None 0.52693\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               ValSWA_ACC#Position_B 0.70666\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               ValSWA_ACC#Position_I 0.66678\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               ValSWA_ACC#Rebuttal_B 0.51267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               ValSWA_ACC#Rebuttal_I 0.51515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          ValSWA_A_B 0.66906\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          ValSWA_A_I 0.69165\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       ValSWA_A_MEAN 0.6811\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     ValSWA_F1_Claim 0.43529\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  ValSWA_F1_Evidence 0.51714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      ValSWA_F1_Lead 0.67162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  ValSWA_F1_Position 0.59831\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      ValSWA_MacroF1 0.6539\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   ValSWA_Prec_Claim 0.44471\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                ValSWA_Prec_Evidence 0.53357\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    ValSWA_Prec_Lead 0.65984\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                ValSWA_Prec_Position 0.55767\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    ValSWA_Rec_Claim 0.42627\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 ValSWA_Rec_Evidence 0.50168\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     ValSWA_Rec_Lead 0.68382\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 ValSWA_Rec_Position 0.64533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           ValSWA_ce 0.86903\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mchocolate-glitter-2\u001b[0m: \u001b[34mhttps://wandb.ai/ducky/feedback_deberta_large/runs/2ensz94m\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20220226_025808-2ensz94m/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b975d",
   "metadata": {},
   "source": [
    "## DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d912e17",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-25T16:24:15.710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "ddp local rank 1\n",
      "ddp local rank 2\n",
      "ddp local rank 0\n",
      "ddp local rank 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mducky\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mducky\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mducky\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mducky\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdrawn-wood-14\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ducky/feedback_deberta_large\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ducky/feedback_deberta_large/runs/3hiottye\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/feedback/working/feedback/baseline3_1dcnn/wandb/run-20220225_162420-3hiottye\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdrawn-monkey-13\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ducky/feedback_deberta_large\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ducky/feedback_deberta_large/runs/1z9sruex\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/feedback/working/feedback/baseline3_1dcnn/wandb/run-20220225_162420-1z9sruex\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mprime-dragon-15\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ducky/feedback_deberta_large\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ducky/feedback_deberta_large/runs/1oy5ewdr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/feedback/working/feedback/baseline3_1dcnn/wandb/run-20220225_162420-1oy5ewdr\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvibrant-sea-16\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ducky/feedback_deberta_large\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ducky/feedback_deberta_large/runs/1xjpxn6x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/feedback/working/feedback/baseline3_1dcnn/wandb/run-20220225_162420-1xjpxn6x\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "TRAIN  STEP 3 loss:  2.8038:   0%|           | 5/3114 [00:23<4:05:48,  4.74s/it]"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "282.255px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
