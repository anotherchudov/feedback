{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "#bert_normalizer.normalize_str, ftfy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import sys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "sys.path.insert(0, '/home/dlrgy22/Feedback/baseline3/codes/new_transformers_branch/transformers/src')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "import torch as t\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from transformers import DebertaV2TokenizerFast\n",
    "import h5py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# 토크나이저\n",
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('microsoft/deberta-v3-large')\n",
    "tokenizer.model_max_length = 2048"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# train_data\n",
    "data = pd.read_csv('../../input/feedback-prize-2021/sergei_train.csv')\n",
    "# florida -> Location_Name으로 변경 (xx학교 -> School name 이렇게 사용하는듯)\n",
    "data.loc[data.discourse_id==1623258656795.0, 'discourse_text'] =  data.loc[data.discourse_id==1623258656795.0, 'discourse_text'].map(lambda x: x.replace('florida', 'LOCATION_NAME')).values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# 라벨\n",
    "label_names = ['None', 'Lead', 'Position', 'Evidence', 'Claim',\n",
    "               'Concluding Statement', 'Counterclaim', 'Rebuttal']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "colors = {\n",
    "            'Lead': '#8000ff',\n",
    "            'Position': '#2b7ff6',\n",
    "            'Evidence': '#2adddd',\n",
    "            'Claim': '#80ffb4',\n",
    "            'Concluding Statement': 'd4dd80',\n",
    "            'Counterclaim': '#ff8042',\n",
    "            'Rebuttal': '#ff0000'\n",
    "         }\n",
    "         \n",
    "options = {\"ents\": list(colors.keys()), \"colors\": colors}\n",
    "token_maps = dict(zip(colors, range(1, 2 * len(colors), 2)))\n",
    "token_maps"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Lead': 1,\n",
       " 'Position': 3,\n",
       " 'Evidence': 5,\n",
       " 'Claim': 7,\n",
       " 'Concluding Statement': 9,\n",
       " 'Counterclaim': 11,\n",
       " 'Rebuttal': 13}"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "regexp = re.compile('[0-9a-zA-z]')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# ? 이해 필요\n",
    "def make_more_targets(targets):\n",
    "    # target -> NER target\n",
    "    linkage = np.zeros((len(targets), 2), 'f4')\n",
    "    class_index = np.zeros((len(targets),), 'f4')\n",
    "    linkage_mask = np.ones((len(targets),), 'f4')\n",
    "    \n",
    "    current_target = -2\n",
    "    for ix in range(1, len(targets) -1):\n",
    "        if ((current_target % 2 == 0 and current_target == targets[ix]) or (targets[ix] == current_target + 1 and current_target %2 == 1)):\n",
    "        # current_target이 처음나오는 start이고  current target이 ix번째 target과 같을때 or current_target이 나온적이 있는 target이고  ix번째 target과 current_target + 1이 같을때\n",
    "        # linkage [0] -> 앞에것과 같은가?, linkage[1] -> 뒤에것과 같은가?\n",
    "            linkage[ix - 1, 1] = 1\n",
    "            linkage[ix, 0] = 1\n",
    "            \n",
    "        current_target = targets[ix]\n",
    "        # class_index -> NER 테스크의 시작점 구분없이\n",
    "        class_index[:] = [x // 2 for x in targets + 1]\n",
    "\n",
    "    #link_sums : 3-> 앞뒤가 같은것, 2 -> 앞에만 같은것(마지막이라는 의미), 1 -> 뒤에만 같은것(처음이라는 의미)\n",
    "    link_sums = (linkage * np.array([2, 1])).sum(-1).astype('i4')\n",
    "    \n",
    "    # bi [1, 0] -> 앞이 다른경우 (처음 나온경우) [0, 1] -> 앞이 같은경우 (나온적이 있는경우) ->binary\n",
    "    bi =  np.zeros((len(targets), 2), 'f4')\n",
    "    bi[link_sums < 2, 0] = 1\n",
    "    bi[link_sums >= 2, 1] = 1\n",
    "    #bio -> target이 0 인경우 [0, 0] ex) padding\n",
    "    bio = np.array(bi)\n",
    "    bio[targets==0] = 0\n",
    "    #bies -> link_sum : 0 -> (0, 0, 0, 1),  link_sum : 1 -> (1, 0, 0, 0),  link_sum : 2 -> (0, 0, 1, 0),  link_sum : 3 -> (0, 1, 0, 0), \n",
    "    bies =  np.zeros((len(targets), 4), 'f4')\n",
    "    bies[:, :2] = bi\n",
    "    bies[link_sums == 0] = (0, 0, 0, 1)\n",
    "    bies[link_sums == 2] = (0, 0, 1, 0)\n",
    "    #bieso -> target이 0 인경우 [0, 0] ex) padding\n",
    "    bieso = np.array(bies)\n",
    "    bieso[targets==0] = 0\n",
    "    return class_index, bi, bio, bies, bieso"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def combine_labels(class_index, bi, bio, bies, bieso):\n",
    "    # class_index와 bi, bio, bies, bideso 결합\n",
    "    combined_bi = class_index * 2 + bi[:, 0]\n",
    "    combined_bies = class_index * 4 + bies @ np.array([0, 1, 2, 3])\n",
    "    \n",
    "    # class_index 라벨이 0(padding)이 아닌곳 \n",
    "    non_o_index = np.where(class_index != 0)[0]\n",
    "    combined_bieso = np.array(class_index)\n",
    "    combined_bieso[non_o_index] = (class_index[non_o_index] - 1) * 4 + bieso[non_o_index] @ np.array([1, 2, 3, 4])\n",
    "    combined_bio = np.array(class_index)\n",
    "    combined_bio[non_o_index] = (class_index[non_o_index] - 1) * 2 + bio[non_o_index] @ np.array([1, 2])\n",
    "    return combined_bi, combined_bio, combined_bies, combined_bieso"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "import h5py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# data_file 생성\n",
    "data_file = h5py.File('../data_file/deberta_spm_data_v2.h5py', 'w')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# txt 개수 + 1 -> 1을 왜 더함?\n",
    "num_texts = len(glob('../../input/feedback-prize-2021/train/*.txt')) + 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# (numtext, 2048) zeros\n",
    "tokens_dataset = data_file.create_dataset('tokens', (num_texts, 2048), 'i8')\n",
    "attention_masks_dataset = data_file.create_dataset('attention_masks', (num_texts, 2048), 'f4')\n",
    "token_offsets_dataset = data_file.create_dataset('token_offsets', (num_texts, 2048, 2), 'i4')\n",
    "class_labels_dataset = data_file.create_dataset('class_labels', (num_texts, 2048, 8), 'f4')\n",
    "num_tokens_dataset = data_file.create_dataset('num_tokens', (num_texts, 2048), 'i4')\n",
    "\n",
    "bi_labels_dataset = data_file.create_dataset('bi_labels', (num_texts, 2048, 2), 'f4')\n",
    "bio_labels_dataset = data_file.create_dataset('bio_labels', (num_texts, 2048, 2), 'f4')\n",
    "bies_labels_dataset = data_file.create_dataset('bies_labels', (num_texts, 2048, 4), 'f4')\n",
    "bieso_labels_dataset = data_file.create_dataset('bieso_labels', (num_texts, 2048, 4), 'f4')\n",
    "\n",
    "cbi_labels_dataset = data_file.create_dataset('cbi_labels', (num_texts, 2048, 16), 'f4')\n",
    "cbio_labels_dataset = data_file.create_dataset('cbio_labels', (num_texts, 2048, 15), 'f4')\n",
    "cbies_labels_dataset = data_file.create_dataset('cbies_labels', (num_texts, 2048, 32), 'f4')\n",
    "cbieso_labels_dataset = data_file.create_dataset('cbieso_labels', (num_texts, 2048, 29), 'f4')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "for dataset in tqdm((tokens_dataset, \n",
    "               attention_masks_dataset,\n",
    "               token_offsets_dataset,\n",
    "               class_labels_dataset,\n",
    "               num_tokens_dataset,\n",
    "               bi_labels_dataset,\n",
    "               bio_labels_dataset,\n",
    "               bies_labels_dataset,\n",
    "               bieso_labels_dataset,\n",
    "               cbi_labels_dataset,\n",
    "               cbio_labels_dataset,\n",
    "               cbies_labels_dataset,\n",
    "               cbieso_labels_dataset,)):\n",
    "    dataset[-1] = 0\n",
    "    \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 51.61it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# onehot encoding으로 변경\n",
    "def make_one_hot(indices, num_labels):\n",
    "    array = np.zeros((len(indices), num_labels))\n",
    "    array[np.arange(len(indices)), indices.astype('i4')] = 1\n",
    "    return array"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "id_to_ix_map = {filename: ix for ix, filename in enumerate(glob('../../input/feedback-prize-2021/train/*.txt'))}\n",
    "with open('../data_file/id_to_ix_map.pickle', 'wb') as f:\n",
    "    import dill as pickle\n",
    "    pickle.dump(id_to_ix_map, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# '/n' -> ‽로 변경\n",
    "fix_text = lambda x: x.replace('\\n', '‽')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "from collections import Counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# 에러\n",
    "broken_indices = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "print(tokenizer.decode([126861]))\n",
    "print(tokenizer.decode([128000]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‽\n",
      "[MASK]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "id_to_ix_map = {}\n",
    "for filename_ix, filename in tqdm(enumerate(glob('../../input/feedback-prize-2021/train/*.txt')), total = num_texts-1):\n",
    "    ID = filename.split('/')[-1].split('.')[0]\n",
    "    with open(filename) as f:\n",
    "        # /n 제거\n",
    "        text = fix_text(f.read().strip())\n",
    "\n",
    "    tokenizer_outs = tokenizer(text, return_offsets_mapping=True)\n",
    "    # 토크나이저 ‽일경우 -> MASK 토큰으로 변경\n",
    "    tokenizer_outs['input_ids'] = [x if x != 126861 else 128000 for x in tokenizer_outs['input_ids']]\n",
    "    # data -> train.csv, txt파일과 같은 ID를가지는 row들 중에서 discourse_start순으로 정렬\n",
    "    text_data = data.loc[data.id==ID].sort_values('discourse_start')\n",
    "\n",
    "    ent_boundaries = []\n",
    "    # pointer -> text에서 시작하는 시점\n",
    "    pointer = 0\n",
    "    for row_id, row in text_data.iterrows():\n",
    "        # discourse_text에서 '/n' -> ‽로 변경\n",
    "        entity_text = fix_text(row.discourse_text.strip())\n",
    "        # 정규표현식\n",
    "        entity_text = entity_text[next(regexp.finditer(entity_text)).start():]\n",
    "        # print(entity_text.split()[0])\n",
    "\n",
    "        if len(entity_text.split()[0]) == 1 and pointer != 0:\n",
    "            # i, a, 등등 처음 오는 단어가 한자리고 pointer가 0이 아닐때 -> 어떤의미?\n",
    "            # entity_start_ix -> pointer에서부터 entity_text가 나올때 까지의 index\n",
    "            entity_start_ix = text[pointer:].index(entity_text)\n",
    "\n",
    "            # prev_text -> entity_text가 나오기 이전까지의 text\n",
    "            prev_text = text[:pointer + entity_start_ix]\n",
    "\n",
    "            # 시작하는 시점이 0보다 prev_text의 마지막이 알파벳일경우 \n",
    "            if pointer + entity_start_ix > 0 and prev_text[-1].isalpha():\n",
    "                broken_indices.append((filename_ix, ID))\n",
    "                print('cut entity ', filename_ix, ID)\n",
    "                cut_word_chunk_size = len(prev_text.split()[-1])\n",
    "                if cut_word_chunk_size > 1:\n",
    "                    entity_text = entity_text[next(regexp.finditer(entity_text[1:])).start() + 1:]\n",
    "        \n",
    "        if row.discourse_id in (1620147556527.0, 1622983056026.0):\n",
    "            # id가 1620147556527, 1622983056026일경우 pointer + 10 -> 이유는?\n",
    "            pointer += 10\n",
    "\n",
    "        offset = text[pointer:].index(entity_text)\n",
    "        # text에서 시작하는 시점\n",
    "        starts_at = offset + pointer\n",
    "        # (text에서 시작하는 시점, 끝나는 시점, row.discourse_type)\n",
    "        ent_boundaries.append((starts_at, starts_at + len(entity_text), row.discourse_type))\n",
    "        # 다음 pointer는 starts_at + len(entity_text) -> 부터 시작 -> 시간감소를 위한것?\n",
    "        pointer = starts_at + len(entity_text)\n",
    "    \n",
    "    # (location, ent, start or end)\n",
    "    all_boundaries = [(z, x[-1], t) for x in ent_boundaries for z, t in zip(x[:2], ('start', 'end'))]\n",
    "    current_target = 0\n",
    "    targets = np.zeros(len(tokenizer_outs['input_ids']), 'i8')\n",
    "    token_positions = np.vstack(tokenizer_outs['offset_mapping']).astype('i4')\n",
    "\n",
    "    for token_ix in range(len(tokenizer_outs['input_ids'])):\n",
    "        # 단어 시작 끝?\n",
    "        token_start_ix, token_end_ix = tokenizer_outs['offset_mapping'][token_ix]\n",
    "        # all_boundaries 가장 앞에있는것이 end 일경우 -> token_end_ix 가 all_boundaries[0] (location) 보다 크거나 같을\n",
    "        # all_boundaries 가장 앞에있는것이 start 일경우 -> token_end_ix 가 all_boundaries[0] (location) 보다 클경우\n",
    "        if token_end_ix != 0 and (all_boundaries[0][2] == 'end' and token_end_ix >= all_boundaries[0][0])\\\n",
    "                            or (all_boundaries[0][2] == 'start' and token_end_ix > all_boundaries[0][0]):\n",
    "\n",
    "            if all_boundaries[0][2] == 'start':\n",
    "                #target\n",
    "                current_target = token_maps[all_boundaries[0][1]]\n",
    "                targets[token_ix] = current_target\n",
    "                # offset mapping의 end가 start 위치와 같을경우 0 -> \"\" 빈칸이기 때문??\n",
    "                if token_end_ix == all_boundaries[1][0]:\n",
    "                    current_target = 0\n",
    "                    # 처음이 끝이기 때문에 pop\n",
    "                    all_boundaries.pop(0)\n",
    "                else:\n",
    "                    # 처음 나온 start 다음은 + 1\n",
    "                    current_target += 1\n",
    "            else:\n",
    "                # end 일경우 token_end_ix가 다음 all_boundaries location보다 클 때\n",
    "                if len(all_boundaries) > 1 and token_end_ix > all_boundaries[1][0]:\n",
    "                    if token_start_ix >= all_boundaries[1][0]:\n",
    "                        assert text[all_boundaries[0][0] - 1] == '¨'\n",
    "                    all_boundaries.pop(0)\n",
    "                    current_target = token_maps[all_boundaries[0][1]]\n",
    "                    targets[token_ix] = current_target\n",
    "                    current_target += 1\n",
    "\n",
    "                else:\n",
    "                    if token_start_ix >= all_boundaries[0][0]:\n",
    "                        current_target = 0\n",
    "                    targets[token_ix] = current_target\n",
    "                    current_target = 0\n",
    "            all_boundaries.pop(0)\n",
    "            if not all_boundaries:\n",
    "                break\n",
    "        else:\n",
    "            # 변화가 없는 위치 -> current_target\n",
    "            targets[token_ix] = current_target\n",
    "            \n",
    "    # target -> NER targets\n",
    "    class_index, bi, bio, bies, bieso = make_more_targets(targets)\n",
    "    combined_bi, combined_bio, combined_bies, combined_bieso = combine_labels(class_index, bi, bio, bies, bieso)\n",
    "    assert (combined_bio[1:-1] == targets[1:-1]).all()\n",
    "    num_tokens = len(targets)\n",
    "    \n",
    "    # 남은것들은 padding -> 0\n",
    "    tokens_dataset[filename_ix, :num_tokens] = tokenizer_outs['input_ids']\n",
    "    tokens_dataset[filename_ix, num_tokens:] = 0\n",
    "    attention_masks_dataset[filename_ix, :num_tokens] = tokenizer_outs['attention_mask']\n",
    "    attention_masks_dataset[filename_ix, num_tokens:] = 0\n",
    "    token_offsets_dataset[filename_ix, :num_tokens] = token_positions\n",
    "    token_offsets_dataset[filename_ix, num_tokens:] = 0\n",
    "    class_labels_dataset[filename_ix, :num_tokens] = make_one_hot(class_index, 8)\n",
    "    class_labels_dataset[filename_ix, num_tokens:] = 0\n",
    "    num_tokens_dataset[filename_ix] = num_tokens\n",
    "    bi_labels_dataset[filename_ix, :num_tokens] = bi\n",
    "    bi_labels_dataset[filename_ix, num_tokens:] = 0\n",
    "    bio_labels_dataset[filename_ix, :num_tokens] = bio\n",
    "    bio_labels_dataset[filename_ix, num_tokens:] = 0\n",
    "    bies_labels_dataset[filename_ix, :num_tokens] = bies\n",
    "    bies_labels_dataset[filename_ix, num_tokens:] = 0\n",
    "    bieso_labels_dataset[filename_ix, :num_tokens] = bieso\n",
    "    bieso_labels_dataset[filename_ix, num_tokens:] = 0\n",
    "    cbi_labels_dataset[filename_ix, :num_tokens] = make_one_hot(combined_bi, 16)\n",
    "    cbi_labels_dataset[filename_ix, num_tokens:] = 0\n",
    "    cbio_labels_dataset[filename_ix, :num_tokens] = make_one_hot(combined_bio, 15)\n",
    "    cbio_labels_dataset[filename_ix, num_tokens:] = 0\n",
    "    cbies_labels_dataset[filename_ix, :num_tokens] = make_one_hot(combined_bies, 32)\n",
    "    cbies_labels_dataset[filename_ix, num_tokens:] = 0\n",
    "    cbieso_labels_dataset[filename_ix, :num_tokens] = make_one_hot(combined_bieso, 29)\n",
    "    cbieso_labels_dataset[filename_ix, num_tokens:] = 0"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  1%|          | 176/15594 [00:57<1:20:20,  3.20it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cut entity  176 40331978F344\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  2%|▏         | 256/15594 [01:23<1:23:51,  3.05it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3817288/4081799568.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mtoken_offsets_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mclass_labels_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mnum_tokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mclass_labels_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mnum_tokens_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename_ix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mbi_labels_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mnum_tokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new/lib/python3.7/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, args, val)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Perform the write, with broadcasting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfspace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new/lib/python3.7/site-packages/h5py/_hl/selections.py\u001b[0m in \u001b[0;36mbroadcast\u001b[0;34m(self, source_shape)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0msid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_hyperslab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0msid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0msid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munravel_index\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_file.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('new': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "c622ab5b55136b32939278ccb9296a6b680e41a2a5270ba7b0ccd6c71683d8f4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}