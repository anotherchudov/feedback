{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee95c59-a8ab-46b0-82c3-31feb1486379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T06:29:11.178926Z",
     "start_time": "2022-02-24T06:29:11.172177Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/home/feedback/working/feedback/models_training/longformer/sumbission/codes/new_transformers_branch/transformers/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601c1bc7-15d0-446b-913c-3c7eeead066d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T06:29:13.469459Z",
     "start_time": "2022-02-24T06:29:11.180260Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "import dill as pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import torch\n",
    "from transformers import DebertaV2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8676079f-5c03-4035-8d58-6959605c8aa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T06:29:20.897809Z",
     "start_time": "2022-02-24T06:29:13.470526Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('microsoft/deberta-v3-large')\n",
    "tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4067a3d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T06:29:21.394133Z",
     "start_time": "2022-02-24T06:29:20.898881Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../../feedback-prize-2021/train.csv')\n",
    "data.loc[data.discourse_id==1623258656795.0, 'discourse_text'] =  data.loc[data.discourse_id==1623258656795.0, \n",
    "                                                                           'discourse_text'].map(lambda x: x.replace('florida', 'LOCATION_NAME')).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9596e2b8-eeab-4bdc-888f-d9aca60467cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T06:29:21.397811Z",
     "start_time": "2022-02-24T06:29:21.395698Z"
    }
   },
   "outputs": [],
   "source": [
    "label_names = ['None', 'Lead', 'Position', 'Evidence', 'Claim',\n",
    "               'Concluding Statement', 'Counterclaim', 'Rebuttal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9fc64a6-21cd-4999-8e10-0c79c9a70a4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T06:29:21.409598Z",
     "start_time": "2022-02-24T06:29:21.398608Z"
    }
   },
   "outputs": [],
   "source": [
    "colors = {\n",
    "            'Lead': '#8000ff',\n",
    "            'Position': '#2b7ff6',\n",
    "            'Evidence': '#2adddd',\n",
    "            'Claim': '#80ffb4',\n",
    "            'Concluding Statement': 'd4dd80',\n",
    "            'Counterclaim': '#ff8042',\n",
    "            'Rebuttal': '#ff0000'\n",
    "         }\n",
    "options = {\"ents\": list(colors.keys()), \"colors\": colors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db29c97-5606-4e72-9e16-bddb3a22fc36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T06:29:21.419813Z",
     "start_time": "2022-02-24T06:29:21.410361Z"
    }
   },
   "outputs": [],
   "source": [
    "token_maps = dict(zip(colors, range(1, 2 * len(colors), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dfe8093-d0b5-4a97-90d4-54105d39a7fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T06:29:21.432136Z",
     "start_time": "2022-02-24T06:29:21.420623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lead': 1,\n",
       " 'Position': 3,\n",
       " 'Evidence': 5,\n",
       " 'Claim': 7,\n",
       " 'Concluding Statement': 9,\n",
       " 'Counterclaim': 11,\n",
       " 'Rebuttal': 13}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d1df82e-4072-4632-8882-c35b3bfe4612",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T06:29:21.443032Z",
     "start_time": "2022-02-24T06:29:21.432922Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_more_targets(targets):\n",
    "    linkage = np.zeros((len(targets), 2), 'f4')\n",
    "    class_index = np.zeros((len(targets),), 'f4')\n",
    "    linkage_mask = np.ones((len(targets),), 'f4')\n",
    "    \n",
    "    current_target = -2\n",
    "    for ix in range(1, len(targets) - 1):\n",
    "        if ((current_target % 2 == 0 and current_target == targets[ix]) \n",
    "              or (targets[ix] == current_target + 1 and current_target % 2 == 1)):\n",
    "            linkage[ix - 1, 1] = 1\n",
    "            linkage[ix, 0] = 1\n",
    "        \n",
    "        current_target = targets[ix]\n",
    "        class_index[:] = [x // 2 for x in targets + 1]\n",
    "    \n",
    "    link_sums = (linkage * np.array([2, 1])).sum(-1).astype('i4')\n",
    "    \n",
    "    bi =  np.zeros((len(targets), 2), 'f4')\n",
    "    bi[link_sums < 2, 0] = 1\n",
    "    bi[link_sums >= 2, 1] = 1\n",
    "    \n",
    "    bio = np.array(bi)\n",
    "    bio[targets == 0] = 0\n",
    "    \n",
    "    bies =  np.zeros((len(targets), 4), 'f4')\n",
    "    bies[:, :2] = bi\n",
    "    bies[link_sums == 0] = (0, 0, 0, 1)\n",
    "    bies[link_sums == 2] = (0, 0, 1, 0)\n",
    "    \n",
    "    bieso = np.array(bies)\n",
    "    bieso[targets==0] = 0\n",
    "    \n",
    "    return class_index, bi, bio, bies, bieso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1480d2c6-2c72-4f89-ad56-3a5ad1e80ff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T06:29:21.462557Z",
     "start_time": "2022-02-24T06:29:21.443822Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_labels(class_index, bi, bio, bies, bieso):\n",
    "    \n",
    "    combined_bi = class_index * 2 + bi[:, 0]\n",
    "    combined_bies = class_index * 4 + bies @ np.array([0, 1, 2, 3])\n",
    "    \n",
    "    non_o_index = np.where(class_index != 0)[0]\n",
    "    \n",
    "    combined_bieso = np.array(class_index)\n",
    "    combined_bieso[non_o_index] = (class_index[non_o_index] - 1) * 4 + bieso[non_o_index] @ np.array([1, 2, 3, 4])\n",
    "    \n",
    "    combined_bio = np.array(class_index)\n",
    "    combined_bio[non_o_index] = (class_index[non_o_index] - 1) * 2 + bio[non_o_index] @ np.array([1, 2])\n",
    "    \n",
    "    return combined_bi, combined_bio, combined_bies, combined_bieso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8caa473-8da6-4df2-8465-89963371da1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T06:29:21.499422Z",
     "start_time": "2022-02-24T06:29:21.463471Z"
    }
   },
   "outputs": [],
   "source": [
    "# add +1 num text to fill the last text with 0, reason? I don't know why\n",
    "num_texts = len(glob('../../../feedback-prize-2021/train/*.txt')) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "199be078-08bb-41c9-8f2e-60dda9020d15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T06:29:21.502455Z",
     "start_time": "2022-02-24T06:29:21.500311Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_one_hot(indices, num_labels):\n",
    "    array = np.zeros((len(indices), num_labels))\n",
    "    array[np.arange(len(indices)), indices.astype('i4')] = 1\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e7084",
   "metadata": {},
   "source": [
    "## create dataset\n",
    "\n",
    "- [ðŸ”¥Transformer: tokenize a text [for beginner]ðŸŒ±](tokenizer_outshttps://www.kaggle.com/sytuannguyen/transformer-tokenize-a-text-for-beginner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71a9067c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T06:44:05.651511Z",
     "start_time": "2022-02-24T06:44:05.648123Z"
    }
   },
   "outputs": [],
   "source": [
    "# deverta3 newline tokens is gone?\n",
    "fix_text = lambda x: x.replace('\\n', 'â€½')\n",
    "\n",
    "regexp = re.compile('[0-9a-zA-z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f182d989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T11:09:53.891764Z",
     "start_time": "2022-02-24T11:09:53.887356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0540b3c3",
   "metadata": {},
   "source": [
    "### Token labeling version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1ab15b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T11:16:22.772825Z",
     "start_time": "2022-02-24T11:16:20.590284Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 2/15594 [00:00<18:14, 14.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287\n",
      "527\n",
      "282\n",
      "376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 8/15594 [00:00<13:13, 19.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n",
      "178\n",
      "308\n",
      "374\n",
      "577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 12/15594 [00:00<18:15, 14.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532\n",
      "408\n",
      "346\n",
      "876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 17/15594 [00:01<23:43, 10.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637\n",
      "290\n",
      "289\n",
      "303\n",
      "266\n",
      "1192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 19/15594 [00:02<28:59,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1197082/1265041432.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mclass_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbieso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_more_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0mcombined_bi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_bio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_bies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_bieso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbieso\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcombined_bio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1197082/2223305506.py\u001b[0m in \u001b[0;36mmake_more_targets\u001b[0;34m(targets)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcurrent_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mclass_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlink_sums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlinkage\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "id_to_ix_map = {}\n",
    "broken_indices = []\n",
    "\n",
    "for filename_ix, filename in tqdm(enumerate(glob('../../../feedback-prize-2021/train/*.txt')), total=num_texts - 1):\n",
    "    \n",
    "    # read the textfile by ID\n",
    "    ID = filename.split('/')[-1].split('.')[0]\n",
    "    with open(filename) as f:\n",
    "        text = fix_text(f.read().strip())\n",
    "    \n",
    "    # convert text to token\n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    # return_offsets_mapping: (optional) Set to True to return (char_start, char_end) for each token (default False).\n",
    "    #                         If using Python's tokenizer, this method will raise NotImplementedError.\n",
    "    #                         This one is only available on\n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    tokenizer_outs = tokenizer(text, return_offsets_mapping=True)\n",
    "    tokenizer_outs['input_ids'] = [input_id if input_id != 126861 else 128000 for input_id in tokenizer_outs['input_ids']]\n",
    "    \n",
    "    # get text meta information from specific ID\n",
    "    id_df = data.loc[data.id == ID].sort_values('discourse_start')\n",
    "\n",
    "    ent_boundaries = []\n",
    "    pointer = 0\n",
    "    \n",
    "    total_pred_n = 0\n",
    "    for row_id, row in id_df.iterrows():\n",
    "        total_pred_n += len(row.predictionstring.split(' '))\n",
    "        \n",
    "        # fix_text = lambda x: x.replace('\\n', 'â€½')\n",
    "        entity_text = fix_text(row.discourse_text.strip())\n",
    "\n",
    "        # regex to find text start with alphanumeric (a-zA-Z0-9)\n",
    "        entity_text = entity_text[next(regexp.finditer(entity_text)).start():]\n",
    "        \n",
    "        # if the first character length is 1, then check the previous text chunk\n",
    "        if len(entity_text.split()[0]) == 1 and pointer != 0:\n",
    "            entity_start_ix = text[pointer:].index(entity_text)\n",
    "            prev_text = text[:pointer + entity_start_ix]\n",
    "            \n",
    "            # current text is not the beginning and the previous text last char is alphanumeric\n",
    "            if pointer + entity_start_ix > 0 and prev_text[-1].isalpha():\n",
    "                broken_indices.append((filename_ix, ID))\n",
    "                print('cut entity ', filename_ix, ID)\n",
    "                cut_word_chunk_size = len(prev_text.split()[-1])\n",
    "                \n",
    "                # if the previous text last word length is not 1\n",
    "                if cut_word_chunk_size > 1:\n",
    "                    # TODO: add `:` at the end\n",
    "                    # -------------------------------------------------------\n",
    "                    # s it can tell me if I enjoy the club or activity that I signed up\n",
    "                    # o if you would dear principal please consider policy number 1 over 2.\n",
    "                    # t they could all come to the same conclusion.\n",
    "\n",
    "                    # i\n",
    "                    # i\n",
    "                    # t\n",
    "                    entity_text = entity_text[next(regexp.finditer(entity_text[1:])).start() + 1:]\n",
    "\n",
    "        # TODO: what is the meaning of this?\n",
    "        if row.discourse_id in (1620147556527.0, 1622983056026.0):\n",
    "            pointer += 10\n",
    "\n",
    "        offset = text[pointer:].index(entity_text)\n",
    "        starts_at = offset + pointer\n",
    "        ent_boundaries.append((starts_at, starts_at + len(entity_text), row.discourse_type))\n",
    "        pointer = starts_at + len(entity_text)\n",
    "        \n",
    "    # [(0, 'Lead', 'start'), (174, 'Lead', 'end'), (176, 'Position', 'start'), (271, 'Position', 'end')]\n",
    "    all_boundaries = deque([])\n",
    "    for ent_boundary in ent_boundaries:\n",
    "        for position, boundary_type in zip(ent_boundary[:2], ('start', 'end')):\n",
    "            discourse_type = ent_boundary[-1]\n",
    "            all_boundaries.append((position, discourse_type, boundary_type))\n",
    "            \n",
    "    current_target = 0\n",
    "    targets = np.zeros(len(tokenizer_outs['input_ids']), 'i8')\n",
    "    token_positions = np.vstack(tokenizer_outs['offset_mapping']).astype('i4')\n",
    "    \n",
    "#     print('token ids', tokenizer_outs['input_ids'])\n",
    "#     print(f\"token len {len(tokenizer_outs['input_ids'])}, boundary len {len(all_boundaries)}\")\n",
    "    \n",
    "    for token_ix in range(len(tokenizer_outs['input_ids'])):\n",
    "        token_start_ix, token_end_ix = tokenizer_outs['offset_mapping'][token_ix]\n",
    "        \n",
    "        cur_pos, cur_dis_type, cur_bound_type = all_boundaries[0]\n",
    "\n",
    "        if token_end_ix != 0 \\\n",
    "           and (cur_bound_type == 'end' and token_end_ix >= cur_pos) \\\n",
    "           or (cur_bound_type == 'start' and token_end_ix > cur_pos):\n",
    "            \n",
    "            if len(all_boundaries) > 1:\n",
    "                next_pos, next_dis_type, next_bound_type = all_boundaries[1]\n",
    "            if cur_bound_type == 'start':\n",
    "                # token map {'Lead': 1, 'Position': 3, ..., 'Rebuttal': 13}\n",
    "                current_target = token_maps[cur_dis_type]\n",
    "                targets[token_ix] = current_target\n",
    "                \n",
    "                if token_end_ix == next_pos:\n",
    "                    current_target = 0\n",
    "                    all_boundaries.popleft()\n",
    "                else:\n",
    "                    current_target += 1\n",
    "            else:\n",
    "                # If there is more entity left to consider and current is already on the next pos\n",
    "                if len(all_boundaries) > 1 and token_end_ix > next_pos:\n",
    "                    \n",
    "                    # can this actually happen?\n",
    "                    if token_start_ix >= next_pos:\n",
    "                        assert text[cur_pos - 1] == 'Â¨'\n",
    "\n",
    "                    all_boundaries.popleft()\n",
    "                    current_target = token_maps[cur_dis_type]\n",
    "                    targets[token_ix] = current_target\n",
    "                    current_target += 1\n",
    "                else:\n",
    "                    if token_start_ix >= cur_pos:\n",
    "                        current_target = 0\n",
    "\n",
    "                    targets[token_ix] = current_target\n",
    "                    current_target = 0\n",
    "\n",
    "            all_boundaries.popleft()\n",
    "            if not all_boundaries:\n",
    "                break\n",
    "        else:\n",
    "            targets[token_ix] = current_target\n",
    "\n",
    "    print((targets > 0).sum())\n",
    "    class_index, bi, bio, bies, bieso = make_more_targets(targets)\n",
    "    combined_bi, combined_bio, combined_bies, combined_bieso = combine_labels(class_index, bi, bio, bies, bieso)\n",
    "    assert (combined_bio[1:-1] == targets[1:-1]).all()\n",
    "    num_tokens = len(targets)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79cb924",
   "metadata": {},
   "source": [
    "### Token labeling version 2\n",
    "\n",
    "- [TensorFlow - LongFormer - NER - [CV 0.633]](https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-633)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4efcac46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T11:48:24.493053Z",
     "start_time": "2022-02-24T11:48:24.485625Z"
    }
   },
   "outputs": [],
   "source": [
    "def token_labeling(tokenizer_outs, ent_boundaries, token_maps):\n",
    "\n",
    "    targets = np.zeros(len(tokenizer_outs['input_ids']), 'i8')\n",
    "\n",
    "    offsets = tokenizer_outs['offset_mapping']\n",
    "    offset_index = 0    \n",
    "    for a, b, discourse_type in ent_boundaries:\n",
    "        if offset_index > len(offsets) - 1:\n",
    "            break\n",
    "\n",
    "        c = offsets[offset_index][0]\n",
    "        d = offsets[offset_index][1]\n",
    "        beginning = True\n",
    "        while b > c:\n",
    "            if (c >= a) & (b >= d):\n",
    "                if beginning:\n",
    "                    targets[offset_index] = token_maps[discourse_type]\n",
    "                    beginning = False\n",
    "                else:\n",
    "                    targets[offset_index] = token_maps[discourse_type] + 1\n",
    "\n",
    "            offset_index += 1\n",
    "            if offset_index > len(offsets) - 1:\n",
    "                break\n",
    "\n",
    "            c = offsets[offset_index][0]\n",
    "            d = offsets[offset_index][1]\n",
    "            \n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e5766e14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:50:46.030040Z",
     "start_time": "2022-02-24T12:50:45.939687Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/15594 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sergei version 287\n",
      "chris version 285\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1197082/2190321097.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mclass_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbieso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_more_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mcombined_bi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_bio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_bies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_bieso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbieso\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcombined_bio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0mnum_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "id_to_ix_map = {}\n",
    "broken_indices = []\n",
    "\n",
    "for filename_ix, filename in tqdm(enumerate(glob('../../../feedback-prize-2021/train/*.txt')), total=num_texts - 1):\n",
    "    \n",
    "    # read the textfile by ID\n",
    "    ID = filename.split('/')[-1].split('.')[0]\n",
    "    with open(filename) as f:\n",
    "        text = fix_text(f.read().strip())\n",
    "    \n",
    "    # convert text to token\n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    # return_offsets_mapping: (optional) Set to True to return (char_start, char_end) for each token (default False).\n",
    "    #                         If using Python's tokenizer, this method will raise NotImplementedError.\n",
    "    #                         This one is only available on\n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    tokenizer_outs = tokenizer(text, return_offsets_mapping=True)\n",
    "    tokenizer_outs['input_ids'] = [input_id if input_id != 126861 else 128000 for input_id in tokenizer_outs['input_ids']]\n",
    "    \n",
    "    # get text meta information from specific ID\n",
    "    id_df = data.loc[data.id == ID].sort_values('discourse_start')\n",
    "\n",
    "    ent_boundaries = []\n",
    "    pointer = 0\n",
    "    \n",
    "    total_pred_n = 0\n",
    "    for row_id, row in id_df.iterrows():\n",
    "        total_pred_n += len(row.predictionstring.split(' '))\n",
    "        \n",
    "        # fix_text = lambda x: x.replace('\\n', 'â€½')\n",
    "        entity_text = fix_text(row.discourse_text.strip())\n",
    "\n",
    "        # regex to find text start with alphanumeric (a-zA-Z0-9)\n",
    "        entity_text = entity_text[next(regexp.finditer(entity_text)).start():]\n",
    "        \n",
    "        # if the first character length is 1, then check the previous text chunk\n",
    "        if len(entity_text.split()[0]) == 1 and pointer != 0:\n",
    "            entity_start_ix = text[pointer:].index(entity_text)\n",
    "            prev_text = text[:pointer + entity_start_ix]\n",
    "            \n",
    "            # current text is not the beginning and the previous text last char is alphanumeric\n",
    "            if pointer + entity_start_ix > 0 and prev_text[-1].isalpha():\n",
    "                broken_indices.append((filename_ix, ID))\n",
    "                print('cut entity ', filename_ix, ID)\n",
    "                cut_word_chunk_size = len(prev_text.split()[-1])\n",
    "                \n",
    "                # if the previous text last word length is not 1\n",
    "                if cut_word_chunk_size > 1:\n",
    "                    entity_text = entity_text[next(regexp.finditer(entity_text[1:])).start() + 1:]\n",
    "\n",
    "        # TODO: what is the meaning of this?\n",
    "        if row.discourse_id in (1620147556527.0, 1622983056026.0):\n",
    "            pointer += 10\n",
    "\n",
    "        offset = text[pointer:].index(entity_text)\n",
    "        starts_at = offset + pointer\n",
    "        ent_boundaries.append((starts_at, starts_at + len(entity_text), row.discourse_type))\n",
    "        pointer = starts_at + len(entity_text)\n",
    "        \n",
    "    # [(0, 'Lead', 'start'), (174, 'Lead', 'end'), (176, 'Position', 'start'), (271, 'Position', 'end')]\n",
    "    all_boundaries = deque([])\n",
    "    for ent_boundary in ent_boundaries:\n",
    "        for position, boundary_type in zip(ent_boundary[:2], ('start', 'end')):\n",
    "            discourse_type = ent_boundary[-1]\n",
    "            all_boundaries.append((position, discourse_type, boundary_type))\n",
    "            \n",
    "    current_target = 0\n",
    "    targets = np.zeros(len(tokenizer_outs['input_ids']), 'i8')\n",
    "    token_positions = np.vstack(tokenizer_outs['offset_mapping']).astype('i4')\n",
    "    \n",
    "#     print('token ids', tokenizer_outs['input_ids'])\n",
    "#     print(f\"token len {len(tokenizer_outs['input_ids'])}, boundary len {len(all_boundaries)}\")\n",
    "    \n",
    "    for token_ix in range(len(tokenizer_outs['input_ids'])):\n",
    "        token_start_ix, token_end_ix = tokenizer_outs['offset_mapping'][token_ix]\n",
    "        \n",
    "        cur_pos, cur_dis_type, cur_bound_type = all_boundaries[0]\n",
    "\n",
    "        if token_end_ix != 0 \\\n",
    "           and (cur_bound_type == 'end' and token_end_ix >= cur_pos) \\\n",
    "           or (cur_bound_type == 'start' and token_end_ix > cur_pos):\n",
    "            \n",
    "            if len(all_boundaries) > 1:\n",
    "                next_pos, next_dis_type, next_bound_type = all_boundaries[1]\n",
    "            if cur_bound_type == 'start':\n",
    "                # token map {'Lead': 1, 'Position': 3, ..., 'Rebuttal': 13}\n",
    "                current_target = token_maps[cur_dis_type]\n",
    "                targets[token_ix] = current_target\n",
    "                \n",
    "                if token_end_ix == next_pos:\n",
    "                    current_target = 0\n",
    "                    all_boundaries.popleft()\n",
    "                else:\n",
    "                    current_target += 1\n",
    "            else:\n",
    "                # If there is more entity left to consider and current is already on the next pos\n",
    "                if len(all_boundaries) > 1 and token_end_ix > next_pos:\n",
    "                    \n",
    "                    # can this actually happen?\n",
    "                    if token_start_ix >= next_pos:\n",
    "                        assert text[cur_pos - 1] == 'Â¨'\n",
    "\n",
    "                    all_boundaries.popleft()\n",
    "                    current_target = token_maps[cur_dis_type]\n",
    "                    targets[token_ix] = current_target\n",
    "                    current_target += 1\n",
    "                else:\n",
    "                    if token_start_ix >= cur_pos:\n",
    "                        current_target = 0\n",
    "\n",
    "                    targets[token_ix] = current_target\n",
    "                    current_target = 0\n",
    "\n",
    "            all_boundaries.popleft()\n",
    "            if not all_boundaries:\n",
    "                break\n",
    "        else:\n",
    "            targets[token_ix] = current_target\n",
    "\n",
    "    print('sergei version', (targets > 0).sum())\n",
    "#     print('sergei version', targets)\n",
    "            \n",
    "    targets = token_labeling(tokenizer_outs, ent_boundaries, token_maps)\n",
    "    print('chris version', (targets > 0).sum())\n",
    "#     print('chris version', targets)\n",
    "    print('-' * 30)\n",
    "    \n",
    "    class_index, bi, bio, bies, bieso = make_more_targets(targets)\n",
    "    combined_bi, combined_bio, combined_bies, combined_bieso = combine_labels(class_index, bi, bio, bies, bieso)\n",
    "    assert (combined_bio[1:-1] == targets[1:-1]).all()\n",
    "    num_tokens = len(targets)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "567d18de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:50:48.102435Z",
     "start_time": "2022-02-24T12:50:48.095454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        0.,  0.,  3.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "        4.,  4.,  4.,  0.,  7.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "        8.,  8.,  8.,  8.,  0.,  5.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  0.,  0.,  9., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10.], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_bio[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "543696a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:50:48.536126Z",
     "start_time": "2022-02-24T12:50:48.531446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  0,  0,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "        4,  4,  4,  4,  0,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  0,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  0,  0,\n",
       "        9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "38f8edd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:07:34.553173Z",
     "start_time": "2022-02-24T12:07:34.549581Z"
    }
   },
   "outputs": [],
   "source": [
    "linkage = np.zeros((len(targets), 2), 'f4')\n",
    "class_index = np.zeros((len(targets),), 'f4')\n",
    "linkage_mask = np.ones((len(targets),), 'f4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c9196c97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:17:15.287626Z",
     "start_time": "2022-02-24T12:17:15.283724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2 % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ab593ca2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:41:52.553726Z",
     "start_time": "2022-02-24T12:41:52.549313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  0,\n",
       "        5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  0,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  0,  5,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  0,  0,  9, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10,  0])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e6b25c7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:42:01.114615Z",
     "start_time": "2022-02-24T12:42:01.108781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 0., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
       "       4., 4., 4., 4., 4., 4., 4., 4., 4., 0., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 0., 0., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
       "       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
       "       5., 5., 5., 0.], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b30bc51b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:23:02.403545Z",
     "start_time": "2022-02-24T12:23:02.380792Z"
    }
   },
   "outputs": [],
   "source": [
    "current_target = -2\n",
    "for ix in range(1, len(targets) - 1):\n",
    "    # 1. its the token included inside entity but not the first\n",
    "    # 2. token right after the first token inside the entity \n",
    "    if ((current_target % 2 == 0 and current_target == targets[ix])\n",
    "          or (targets[ix] == current_target + 1 and current_target % 2 == 1)):\n",
    "        linkage[ix - 1, 1] = 1\n",
    "        linkage[ix, 0] = 1\n",
    "\n",
    "    current_target = targets[ix]\n",
    "    \n",
    "    # literally class?\n",
    "    class_index[:] = [x // 2 for x in targets + 1]\n",
    "\n",
    "link_sums = (linkage * np.array([2, 1])).sum(-1).astype('i4')\n",
    "\n",
    "bi = np.zeros((len(targets), 2), 'f4')\n",
    "bi[link_sums < 2, 0] = 1\n",
    "bi[link_sums >= 2, 1] = 1\n",
    "\n",
    "bio = np.array(bi)\n",
    "bio[targets == 0] = 0\n",
    "\n",
    "bies =  np.zeros((len(targets), 4), 'f4')\n",
    "bies[:, :2] = bi\n",
    "bies[link_sums == 0] = (0, 0, 0, 1)\n",
    "bies[link_sums == 2] = (0, 0, 1, 0)\n",
    "\n",
    "bieso = np.array(bies)\n",
    "bieso[targets==0] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "158eb19c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:34:20.040826Z",
     "start_time": "2022-02-24T12:34:20.036358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  0,\n",
       "        5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  0,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  0,  5,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  0,  0,  9, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10,  0])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5f46c3ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:46:36.849699Z",
     "start_time": "2022-02-24T12:46:36.845278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 0, 1,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 0, 1, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 2, 1, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 0], dtype=int32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cb1be1b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:23:07.979098Z",
     "start_time": "2022-02-24T12:23:07.974734Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 0, 1,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 0, 1, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 2, 1, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 0], dtype=int32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b73c806",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Original Sergei Code vs Chris\n",
    "\n",
    "- [TensorFlow - LongFormer - NER - [CV 0.633]](https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-633)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8b4e2e32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:05:44.318267Z",
     "start_time": "2022-02-24T12:05:43.341467Z"
    },
    "hidden": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 11/15594 [00:00<02:28, 104.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sergei version 287\n",
      "chris version 285\n",
      "------------------------------\n",
      "sergei version 527\n",
      "chris version 521\n",
      "------------------------------\n",
      "sergei version 282\n",
      "chris version 276\n",
      "------------------------------\n",
      "sergei version 376\n",
      "chris version 370\n",
      "------------------------------\n",
      "sergei version 276\n",
      "chris version 271\n",
      "------------------------------\n",
      "sergei version 178\n",
      "chris version 171\n",
      "------------------------------\n",
      "sergei version 308\n",
      "chris version 304\n",
      "------------------------------\n",
      "sergei version 374\n",
      "chris version 367\n",
      "------------------------------\n",
      "sergei version 577\n",
      "chris version 569\n",
      "------------------------------\n",
      "sergei version 532\n",
      "chris version 529\n",
      "------------------------------\n",
      "sergei version 408\n",
      "chris version 404\n",
      "------------------------------\n",
      "sergei version 346\n",
      "chris version 338\n",
      "------------------------------\n",
      "sergei version 876\n",
      "chris version 867\n",
      "------------------------------\n",
      "sergei version 637\n",
      "chris version 631\n",
      "------------------------------\n",
      "sergei version 290\n",
      "chris version 283\n",
      "------------------------------\n",
      "sergei version 289\n",
      "chris version 287\n",
      "------------------------------\n",
      "sergei version 303\n",
      "chris version 298\n",
      "------------------------------\n",
      "sergei version 266\n",
      "chris version 268\n",
      "------------------------------\n",
      "sergei version 1192\n",
      "chris version 1184\n",
      "------------------------------\n",
      "sergei version 844\n",
      "chris version 838\n",
      "------------------------------\n",
      "sergei version 543\n",
      "chris version 538\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 32/15594 [00:00<02:36, 99.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sergei version 419\n",
      "chris version 417\n",
      "------------------------------\n",
      "sergei version 823\n",
      "chris version 816\n",
      "------------------------------\n",
      "sergei version 463\n",
      "chris version 457\n",
      "------------------------------\n",
      "sergei version 405\n",
      "chris version 403\n",
      "------------------------------\n",
      "sergei version 170\n",
      "chris version 167\n",
      "------------------------------\n",
      "sergei version 609\n",
      "chris version 600\n",
      "------------------------------\n",
      "sergei version 748\n",
      "chris version 741\n",
      "------------------------------\n",
      "sergei version 533\n",
      "chris version 532\n",
      "------------------------------\n",
      "sergei version 384\n",
      "chris version 377\n",
      "------------------------------\n",
      "sergei version 531\n",
      "chris version 523\n",
      "------------------------------\n",
      "sergei version 196\n",
      "chris version 194\n",
      "------------------------------\n",
      "sergei version 221\n",
      "chris version 214\n",
      "------------------------------\n",
      "sergei version 930\n",
      "chris version 926\n",
      "------------------------------\n",
      "sergei version 621\n",
      "chris version 615\n",
      "------------------------------\n",
      "sergei version 638\n",
      "chris version 633\n",
      "------------------------------\n",
      "sergei version 691\n",
      "chris version 683\n",
      "------------------------------\n",
      "sergei version 712\n",
      "chris version 707\n",
      "------------------------------\n",
      "sergei version 522\n",
      "chris version 517\n",
      "------------------------------\n",
      "sergei version 186\n",
      "chris version 181\n",
      "------------------------------\n",
      "sergei version 310\n",
      "chris version 306\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 53/15594 [00:00<02:34, 100.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sergei version 393\n",
      "chris version 389\n",
      "------------------------------\n",
      "sergei version 905\n",
      "chris version 901\n",
      "------------------------------\n",
      "sergei version 372\n",
      "chris version 367\n",
      "------------------------------\n",
      "sergei version 168\n",
      "chris version 166\n",
      "------------------------------\n",
      "sergei version 450\n",
      "chris version 448\n",
      "------------------------------\n",
      "sergei version 461\n",
      "chris version 451\n",
      "------------------------------\n",
      "sergei version 309\n",
      "chris version 307\n",
      "------------------------------\n",
      "sergei version 733\n",
      "chris version 722\n",
      "------------------------------\n",
      "sergei version 238\n",
      "chris version 234\n",
      "------------------------------\n",
      "sergei version 212\n",
      "chris version 205\n",
      "------------------------------\n",
      "sergei version 330\n",
      "chris version 329\n",
      "------------------------------\n",
      "sergei version 551\n",
      "chris version 544\n",
      "------------------------------\n",
      "sergei version 701\n",
      "chris version 695\n",
      "------------------------------\n",
      "sergei version 454\n",
      "chris version 447\n",
      "------------------------------\n",
      "sergei version 374\n",
      "chris version 368\n",
      "------------------------------\n",
      "sergei version 777\n",
      "chris version 772\n",
      "------------------------------\n",
      "sergei version 622\n",
      "chris version 613\n",
      "------------------------------\n",
      "sergei version 460\n",
      "chris version 457\n",
      "------------------------------\n",
      "sergei version 436\n",
      "chris version 431\n",
      "------------------------------\n",
      "sergei version 438\n",
      "chris version 434\n",
      "------------------------------\n",
      "sergei version 500\n",
      "chris version 493\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 74/15594 [00:00<02:37, 98.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sergei version 545\n",
      "chris version 539\n",
      "------------------------------\n",
      "sergei version 1059\n",
      "chris version 1049\n",
      "------------------------------\n",
      "sergei version 278\n",
      "chris version 272\n",
      "------------------------------\n",
      "sergei version 580\n",
      "chris version 577\n",
      "------------------------------\n",
      "sergei version 313\n",
      "chris version 312\n",
      "------------------------------\n",
      "sergei version 615\n",
      "chris version 607\n",
      "------------------------------\n",
      "sergei version 679\n",
      "chris version 673\n",
      "------------------------------\n",
      "sergei version 525\n",
      "chris version 519\n",
      "------------------------------\n",
      "sergei version 458\n",
      "chris version 453\n",
      "------------------------------\n",
      "sergei version 216\n",
      "chris version 209\n",
      "------------------------------\n",
      "sergei version 441\n",
      "chris version 435\n",
      "------------------------------\n",
      "sergei version 824\n",
      "chris version 818\n",
      "------------------------------\n",
      "sergei version 674\n",
      "chris version 667\n",
      "------------------------------\n",
      "sergei version 530\n",
      "chris version 524\n",
      "------------------------------\n",
      "sergei version 442\n",
      "chris version 438\n",
      "------------------------------\n",
      "sergei version 784\n",
      "chris version 777\n",
      "------------------------------\n",
      "sergei version 609\n",
      "chris version 602\n",
      "------------------------------\n",
      "sergei version 187\n",
      "chris version 186\n",
      "------------------------------\n",
      "sergei version 1140\n",
      "chris version 1136\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% 90/15594 [00:00<02:38, 97.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sergei version 716\n",
      "chris version 714\n",
      "------------------------------\n",
      "sergei version 333\n",
      "chris version 326\n",
      "------------------------------\n",
      "sergei version 357\n",
      "chris version 353\n",
      "------------------------------\n",
      "sergei version 659\n",
      "chris version 657\n",
      "------------------------------\n",
      "sergei version 697\n",
      "chris version 695\n",
      "------------------------------\n",
      "sergei version 387\n",
      "chris version 381\n",
      "------------------------------\n",
      "sergei version 206\n",
      "chris version 203\n",
      "------------------------------\n",
      "sergei version 672\n",
      "chris version 665\n",
      "------------------------------\n",
      "sergei version 188\n",
      "chris version 185\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1197082/621281355.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# get text meta information from specific ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'discourse_start'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0ment_boundaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5501\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5502\u001b[0;31m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "id_to_ix_map = {}\n",
    "broken_indices = []\n",
    "\n",
    "for filename_ix, filename in tqdm(enumerate(glob('../../../feedback-prize-2021/train/*.txt')), total=num_texts - 1):\n",
    "    \n",
    "    # read the textfile by ID\n",
    "    ID = filename.split('/')[-1].split('.')[0]\n",
    "    with open(filename) as f:\n",
    "        text = fix_text(f.read().strip())\n",
    "    \n",
    "    # convert text to token\n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    # return_offsets_mapping: (optional) Set to True to return (char_start, char_end) for each token (default False).\n",
    "    #                         If using Python's tokenizer, this method will raise NotImplementedError.\n",
    "    #                         This one is only available on\n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    tokenizer_outs = tokenizer(text, return_offsets_mapping=True)\n",
    "    tokenizer_outs['input_ids'] = [input_id if input_id != 126861 else 128000 for input_id in tokenizer_outs['input_ids']]\n",
    "    \n",
    "    # get text meta information from specific ID\n",
    "    id_df = data.loc[data.id == ID].sort_values('discourse_start')\n",
    "\n",
    "    ent_boundaries = []\n",
    "    pointer = 0\n",
    "    \n",
    "    total_pred_n = 0\n",
    "    for row_id, row in id_df.iterrows():\n",
    "        total_pred_n += len(row.predictionstring.split(' '))\n",
    "        \n",
    "        # fix_text = lambda x: x.replace('\\n', 'â€½')\n",
    "        entity_text = fix_text(row.discourse_text.strip())\n",
    "\n",
    "        # regex to find text start with alphanumeric (a-zA-Z0-9)\n",
    "        entity_text = entity_text[next(regexp.finditer(entity_text)).start():]\n",
    "        \n",
    "        # if the first character length is 1, then check the previous text chunk\n",
    "        if len(entity_text.split()[0]) == 1 and pointer != 0:\n",
    "            entity_start_ix = text[pointer:].index(entity_text)\n",
    "            prev_text = text[:pointer + entity_start_ix]\n",
    "            \n",
    "            # current text is not the beginning and the previous text last char is alphanumeric\n",
    "            if pointer + entity_start_ix > 0 and prev_text[-1].isalpha():\n",
    "                broken_indices.append((filename_ix, ID))\n",
    "                print('cut entity ', filename_ix, ID)\n",
    "                cut_word_chunk_size = len(prev_text.split()[-1])\n",
    "                \n",
    "                # if the previous text last word length is not 1\n",
    "                if cut_word_chunk_size > 1:\n",
    "                    entity_text = entity_text[next(regexp.finditer(entity_text[1:])).start() + 1:]\n",
    "\n",
    "        # TODO: what is the meaning of this?\n",
    "        if row.discourse_id in (1620147556527.0, 1622983056026.0):\n",
    "            pointer += 10\n",
    "\n",
    "        offset = text[pointer:].index(entity_text)\n",
    "        starts_at = offset + pointer\n",
    "        ent_boundaries.append((starts_at, starts_at + len(entity_text), row.discourse_type))\n",
    "        pointer = starts_at + len(entity_text)\n",
    "        \n",
    "    # [(0, 'Lead', 'start'), (174, 'Lead', 'end'), (176, 'Position', 'start'), (271, 'Position', 'end')]\n",
    "    all_boundaries = [(z, x[-1], t) for x in ent_boundaries for z, t in zip(x[:2], ('start', 'end'))]\n",
    "\n",
    "    current_target = 0\n",
    "    targets = np.zeros(len(tokenizer_outs['input_ids']), 'i8')\n",
    "    token_positions = np.vstack(tokenizer_outs['offset_mapping']).astype('i4')\n",
    "    \n",
    "#     print('token ids', tokenizer_outs['input_ids'])\n",
    "#     print(f\"token len {len(tokenizer_outs['input_ids'])}, boundary len {len(all_boundaries)}\")\n",
    "    \n",
    "    for token_ix in range(len(tokenizer_outs['input_ids'])):\n",
    "        token_start_ix, token_end_ix = tokenizer_outs['offset_mapping'][token_ix]\n",
    "        \n",
    "        \n",
    "        if token_end_ix != 0 and (all_boundaries[0][2] == 'end' and token_end_ix >= all_boundaries[0][0])\\\n",
    "                            or (all_boundaries[0][2] == 'start' and token_end_ix > all_boundaries[0][0]):\n",
    "            if all_boundaries[0][2] == 'start':\n",
    "                current_target = token_maps[all_boundaries[0][1]]\n",
    "                targets[token_ix] = current_target\n",
    "                if token_end_ix == all_boundaries[1][0]:\n",
    "                    current_target = 0\n",
    "                    all_boundaries.pop(0)\n",
    "                else:\n",
    "                    current_target += 1\n",
    "            else:\n",
    "                if len(all_boundaries) > 1 and token_end_ix > all_boundaries[1][0]:\n",
    "                    if token_start_ix >= all_boundaries[1][0]:\n",
    "                        assert text[all_boundaries[0][0] - 1] == 'Â¨'\n",
    "                    all_boundaries.pop(0)\n",
    "                    current_target = token_maps[all_boundaries[0][1]]\n",
    "                    targets[token_ix] = current_target\n",
    "                    current_target += 1\n",
    "                else:\n",
    "                    if token_start_ix >= all_boundaries[0][0]:\n",
    "                        current_target = 0\n",
    "                    targets[token_ix] = current_target\n",
    "                    current_target = 0\n",
    "\n",
    "            all_boundaries.pop(0)\n",
    "            if not all_boundaries:\n",
    "                break\n",
    "        else:\n",
    "            targets[token_ix] = current_target\n",
    "\n",
    "    print('sergei version', (targets > 0).sum())\n",
    "#     print('sergei version', targets)\n",
    "            \n",
    "    targets = token_labeling(tokenizer_outs, ent_boundaries, token_maps)\n",
    "    print('chris version', (targets > 0).sum())\n",
    "#     print('chris version', targets)\n",
    "    print('-' * 30)    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.823px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
