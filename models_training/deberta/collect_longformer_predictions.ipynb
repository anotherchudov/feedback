{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16869ad8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 2.541992,
     "end_time": "2022-02-15T12:56:11.597181",
     "exception": false,
     "start_time": "2022-02-15T12:56:09.055189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "import sys\n",
    "sys.path.insert(0, '/home/qwe/projects/feedback/models_training/longformer/sumbission/codes')\n",
    "import torch as t\n",
    "t.autograd.set_grad_enabled(False)\n",
    "from longformer.longformer import Longformer, LongformerConfig\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import RobertaTokenizerFast\n",
    "import dill as pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ec28167",
   "metadata": {
    "papermill": {
     "duration": 17.538995,
     "end_time": "2022-02-15T12:56:29.165998",
     "exception": false,
     "start_time": "2022-02-15T12:56:11.627003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/qwe/projects/feedback/models_training/longformer/sumbission_large/pretrained_checkpoints/longformer-large-4096/ were not used when initializing Longformer: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing Longformer from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Longformer from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class TvmLongformer(t.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        config = LongformerConfig.from_pretrained(\n",
    "            '/home/qwe/projects/feedback/models_training/longformer/sumbission_large/pretrained_checkpoints/longformer-large-4096/') \n",
    "        config.attention_mode = 'sliding_chunks'\n",
    "        self.feats = Longformer.from_pretrained(\n",
    "            '/home/qwe/projects/feedback/models_training/longformer/sumbission_large/pretrained_checkpoints/longformer-large-4096/', config=config)\n",
    "        self.feats.pooler = None\n",
    "        self.class_projector = t.nn.Sequential(\n",
    "            t.nn.LayerNorm(1024),\n",
    "            t.nn.Linear(1024, 15)\n",
    "        )\n",
    "    def forward(self, tokens, mask):\n",
    "        return self.class_projector(self.feats(tokens, mask, return_dict=False)[0])\n",
    "    \n",
    "model = TvmLongformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b170f20",
   "metadata": {
    "papermill": {
     "duration": 0.146747,
     "end_time": "2022-02-15T12:56:29.321019",
     "exception": false,
     "start_time": "2022-02-15T12:56:29.174272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(t.utils.data.Dataset):\n",
    "    def __init__(self, files):\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained('/home/qwe/projects/feedback/models_training/longformer/sumbission_large/tokenizer')\n",
    "        tokenizer.model_max_length = 4096\n",
    "        self.tokenizer = tokenizer\n",
    "        files = set(files)\n",
    "        self.texts = {}\n",
    "        for file_id in files:\n",
    "            with open(f'../../train/{file_id}.txt') as f:\n",
    "                self.texts[file_id] = f.read().strip()\n",
    "        self.keys = sorted(self.texts.keys())\n",
    "        self.space_regex = re.compile('[\\s\\n]')\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    def __getitem__(self, ix):\n",
    "        tokens_array = np.zeros(4096, 'i8')\n",
    "        mask_array = np.zeros(4096, 'f4')\n",
    "        offsets_array = np.zeros((4096, 2), 'i4')\n",
    "        \n",
    "        text = self.texts[self.keys[ix]]\n",
    "        key = self.keys[ix]\n",
    "        tokenizer_outs = self.tokenizer(text, return_offsets_mapping=True)\n",
    "        tokens = np.array(tokenizer_outs['input_ids'], 'i8')\n",
    "        mask = np.array(tokenizer_outs['attention_mask'], 'f4')\n",
    "        mask[0] = 2\n",
    "        mask[-1] = 2\n",
    "        mask[tokens==4] = 2\n",
    "        mask[tokens==116] = 2\n",
    "        mask[tokens==328] = 2\n",
    "        \n",
    "        offsets = np.vstack(tokenizer_outs['offset_mapping']).astype('i4')\n",
    "        \n",
    "        tokens_array[:len(tokens)] = tokens\n",
    "        mask_array[:len(tokens)] = mask\n",
    "        offsets_array[:len(tokens)] = offsets\n",
    "        \n",
    "        index_map = []\n",
    "        current_word = 0\n",
    "        blank = False\n",
    "        for char_ix in range(text.index(text.strip()[0]), len(text)):\n",
    "            if self.space_regex.match(text[char_ix]) is not None:\n",
    "                blank = True\n",
    "            elif blank:\n",
    "                current_word += 1\n",
    "                blank = False\n",
    "            index_map.append(current_word)\n",
    "            \n",
    "        return tokens_array, mask_array, offsets_array, index_map, key, len(tokens)\n",
    "    \n",
    "first_batch = True\n",
    "def collate_fn(ins):\n",
    "    global first_batch\n",
    "    if first_batch:\n",
    "        max_len = 2048\n",
    "        first_batch = False\n",
    "    else:\n",
    "        max_len = (max(x[-1] for x in ins) + 511) // 512 * 512\n",
    "    return tuple(t.from_numpy(np.concatenate([ins[z][x][None, :max_len]\n",
    "                                              for z in range(len(ins))]))\n",
    "                 for x in range(len(ins[0]) - 3)) \\\n",
    "                 + ([x[-3] for x in ins], [x[-2] for x in ins], np.array([x[-1] for x in ins]),)    \n",
    "\n",
    "\n",
    "label_names = ['None', 'Lead', 'Position', 'Evidence', 'Claim',\n",
    "               'Concluding Statement', 'Counterclaim', 'Rebuttal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f89e30",
   "metadata": {
    "papermill": {
     "duration": 3.139145,
     "end_time": "2022-02-15T12:56:32.493780",
     "exception": false,
     "start_time": "2022-02-15T12:56:29.354635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoints = glob('../longformer/sumbission_large/weights/fold*attn2')\n",
    "model.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "333dfe40-ad59-40fc-973b-19b547eeffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../id_to_ix_map.pickle', 'rb') as f:\n",
    "    id_to_ix_map = {x.split('/')[-1].split('.')[0]: y for x, y in pickle.load(f).items()}\n",
    "with open('../../data_splits.pickle', 'rb') as f:\n",
    "    data_splits = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e586c",
   "metadata": {
    "papermill": {
     "duration": 71.461684,
     "end_time": "2022-02-15T12:57:43.968110",
     "exception": false,
     "start_time": "2022-02-15T12:56:32.506426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3140/3140 [03:59<00:00, 13.12it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3121/3121 [03:59<00:00, 13.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3139/3139 [03:57<00:00, 13.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 3086/3096 [03:51<00:00, 13.60it/s]"
     ]
    }
   ],
   "source": [
    "all_outs = np.zeros((len(glob('../../train/*.txt')), 2048, 15), 'f4')\n",
    "all_bounds = np.zeros((len(glob('../../train/*.txt')), 2048, 2), 'i4')\n",
    "all_token_nums = np.zeros((len(glob('../../train/*.txt')),), 'i4')\n",
    "all_word_indices = []\n",
    "all_sample_ids = []\n",
    "ix = 0\n",
    "for fold_ix, checkpoint in enumerate(sorted(checkpoints, key=lambda x: int(x.split('/')[-1].split('_')[0][-1]))):\n",
    "    val_files = data_splits[0][250]['normed'][fold_ix]\n",
    "    dataset = t.utils.data.DataLoader(Dataset(val_files), collate_fn=collate_fn,\n",
    "                                  batch_size=1, num_workers=2)\n",
    "    model.load_state_dict(t.load(checkpoint));\n",
    "    for batch in tqdm(dataset):\n",
    "        tokens, mask, bounds, word_indices, sample_ids, num_tokens = batch\n",
    "        batch_size, batch_len = tokens.shape[:2]\n",
    "        outs = t.log_softmax(model(tokens.cuda(), mask.cuda()), -1)\n",
    "        all_outs[ix: ix + batch_size, :batch_len] = outs.cpu().numpy()\n",
    "        all_bounds[ix: ix + batch_size, :batch_len] = bounds\n",
    "        all_token_nums[ix: ix + batch_size] = num_tokens\n",
    "        all_word_indices.extend(word_indices)\n",
    "        all_sample_ids.extend(sample_ids)\n",
    "        ix += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647db38-c9f6-49b4-a16d-24f20e2aef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (array, \n",
    "    array_name) in zip((all_outs, all_bounds, all_token_nums, all_word_indices, all_sample_ids),\n",
    "                        'all_outs, all_bounds, all_token_nums, all_word_indices, all_sample_ids'.split(', ')):\n",
    "    np.save(f'../oof_ps/longformer/{array_name}.npy', array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc6b71b-f1a5-4945-8e54-1937b15cfa39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 106.396967,
   "end_time": "2022-02-15T12:57:46.771934",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-15T12:56:00.374967",
   "version": "2.3.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
