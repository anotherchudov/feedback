{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce244a3-4fc3-47a8-b8ad-4fb053c50517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df2d560-8c05-4e95-82f5-dd33a10dd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = h5py.File('../../deberta_spm_data_v2.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c53da6d6-ca95-409e-92ed-89b793be7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fbc34e6-b122-4c16-bd0d-8a77a799dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = {}\n",
    "for f in glob('../../train/*.txt'):\n",
    "    with open(f) as x:\n",
    "        all_texts[f.split('/')[-1].split('.')[0]] = x.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66f1582a-ef91-404c-8e0f-384afe41d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67626ee0-0c73-4f33-b918-30e98e0a537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c56c1089-76fa-4289-88dd-cc9cf2b279fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa61defe-2d7f-490d-926a-ede93685acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "603446dc-b2b8-4156-b11a-bcc494a27abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['None', 'Lead', 'Position', 'Evidence', 'Claim',\n",
    "               'Concluding Statement', 'Counterclaim', 'Rebuttal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35ae1cfd-cc1f-4128-a101-05bb64d1da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6a85a2f-ee90-4b55-a853-cd251a1d3440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23849756-faef-4f1f-be8b-70c7c9f4d162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv('../../train.csv')\n",
    "space_regex = re.compile('[\\s\\n]')\n",
    "with open('../../id_to_ix_map.pickle', 'rb') as f:\n",
    "    id_to_ix_map = {x.split('/')[-1].split('.')[0]: y for x, y in pickle.load(f).items()}\n",
    "with open('../../data_splits.pickle', 'rb') as f:\n",
    "    data_splits = pickle.load(f)\n",
    "\n",
    "file_ids = [id_to_ix_map[x] for fold in range(5) for x in data_splits[seed][250]['normed'][fold]]\n",
    "files = list(id_to_ix_map.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f434ae3e-40a6-4f1d-96a0-8d47eb98920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ix = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f7e44e-7fdf-4f8b-92a6-9937f28e270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(ps, n):\n",
    "    cat_ps = ps.argmax(-1)\n",
    "    all_entities = {}\n",
    "    current_cat = None\n",
    "    current_start = None\n",
    "    seq_len = ps.shape[0]\n",
    "    for ix in range(seq_len - n, seq_len - 2):\n",
    "    # for ix in range(1, n - 1):\n",
    "        if cat_ps[ix] % 2 == 1:\n",
    "            if current_cat is not None:\n",
    "                if current_cat not in all_entities:\n",
    "                    all_entities[current_cat] = []\n",
    "                all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = (cat_ps[ix] + 1) // 2\n",
    "            current_start = ix\n",
    "        elif cat_ps[ix] == 0:\n",
    "            if current_cat is not None:\n",
    "                if current_cat not in all_entities:\n",
    "                    all_entities[current_cat] = []\n",
    "                all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = None\n",
    "            \n",
    "    if current_cat is not None:\n",
    "        if current_cat not in all_entities:\n",
    "            all_entities[current_cat] = []\n",
    "        all_entities[current_cat].append((current_start, ix))\n",
    "    return all_entities\n",
    "\n",
    "def process_sample(raw_ps, index_map, bounds, gt_spans, num_tokens, match_stats, min_len=0):\n",
    "    predicted_spans = {x: [map_span_to_word_indices(span, index_map, bounds) for span in y] \n",
    "                       for x, y in extract_entities(raw_ps, num_tokens).items()}\n",
    "    predicted_spans = {x: [z for z in y if z[1] - z[0] >= min_len] for x, y in predicted_spans.items()}\n",
    "    \n",
    "    for cat_ix in range(1, 8):\n",
    "        \n",
    "        pspans = predicted_spans.get(cat_ix, [])\n",
    "        gspans = gt_spans.get(cat_ix, [])\n",
    "        if not len(pspans) or not len(gspans):\n",
    "            match_stats[cat_ix]['fn'] += len(gspans)\n",
    "            match_stats[cat_ix]['fp'] += len(pspans)\n",
    "        else:\n",
    "            all_overlaps = np.zeros((len(pspans), len(gspans)))\n",
    "            for x1 in range(len(pspans)):\n",
    "                pspan = pspans[x1]\n",
    "                for x2 in range(len(gspans)):\n",
    "                    gspan = gspans[x2]\n",
    "                    start_ix = max(pspan[0], gspan[0])\n",
    "                    end_ix = min(pspan[1], gspan[1])\n",
    "                    overlap = max(0, end_ix - start_ix + 1)\n",
    "                    if overlap > 0:\n",
    "                        o1 = overlap / (pspan[1] - pspan[0] + 1)\n",
    "                        o2 = overlap / (gspan[1] - gspan[0] + 1)\n",
    "                        if min(o1, o2) >= .5:\n",
    "                            all_overlaps[x1, x2] = max(o1, o2)\n",
    "            unused_p_ix = set(range(len(pspans)))\n",
    "            unused_g_ix = set(range(len(gspans)))\n",
    "            col_size = len(pspans)\n",
    "            row_size = len(gspans)\n",
    "            for ix in np.argsort(all_overlaps.ravel())[::-1]:\n",
    "                if not len(unused_g_ix) or not len(unused_p_ix) or all_overlaps.ravel()[ix] == 0:\n",
    "                    match_stats[cat_ix]['fp'] += len(unused_p_ix)\n",
    "                    match_stats[cat_ix]['fn'] += len(unused_g_ix)\n",
    "                    break\n",
    "                p_ix = ix // row_size\n",
    "                g_ix = ix % row_size\n",
    "                if p_ix not in unused_p_ix or g_ix not in unused_g_ix:\n",
    "                    continue\n",
    "                match_stats[cat_ix]['tp'] += 1\n",
    "                unused_g_ix.remove(g_ix)\n",
    "                unused_p_ix.remove(p_ix)\n",
    "    return match_stats\n",
    "\n",
    "def map_span_to_word_indices(span, index_map, bounds):\n",
    "    return (index_map[bounds[span[0], 0]], index_map[bounds[span[1], 1] - 1])\n",
    "\n",
    "def split_predstring(x):\n",
    "    vals = x.split()\n",
    "    return int(vals[0]), int(vals[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aa96295-70e7-48bd-8c2b-8f13cdda7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aef46f52-8aac-4754-91ed-c242876a1d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 15594/15594 [02:59<00:00, 86.83it/s]\n"
     ]
    }
   ],
   "source": [
    "match_stats = {ix:  {'fp': 0, 'fn': 0, 'tp': 0} for ix in range(1, 8)}\n",
    "for sample_ix in tqdm(range(len(glob('../../train/*.txt')))):\n",
    "    text = all_texts[files[sample_ix]].strip()\n",
    "    gt_dict = {}\n",
    "    sample_df = csv.loc[csv.id==files[sample_ix]]\n",
    "    for cat_ix in range(1, 8):\n",
    "        cat_name = label_names[cat_ix]\n",
    "        cat_entities = sample_df.loc[sample_df.discourse_type==cat_name]\n",
    "        if len(cat_entities):\n",
    "            gt_dict[cat_ix] = [(x[0], x[1]) for x in cat_entities.predictionstring.map(split_predstring)]\n",
    "    \n",
    "    index_map = []\n",
    "    current_word = 0\n",
    "    blank = False\n",
    "    for char_ix in range(text.index(text.strip()[0]), len(text)):\n",
    "        if space_regex.match(text[char_ix]) is not None:\n",
    "            blank = True\n",
    "        elif blank:\n",
    "            current_word += 1\n",
    "            blank = False\n",
    "        index_map.append(current_word)\n",
    "        \n",
    "    raw_ps = data[f'cbio_labels'][sample_ix]\n",
    "    num_tokens = data['num_tokens'][sample_ix, 0]\n",
    "    bounds = data['token_offsets'][sample_ix]\n",
    "    \n",
    "    \n",
    "    match_stats = process_sample(raw_ps, index_map, bounds, gt_dict, num_tokens, match_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d0c7c-fe5d-4ed9-a3c9-42c65dd1b74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a66052ec-13c7-405e-b1fb-d021e2a40c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = np.zeros(8)\n",
    "rec = np.zeros(7)\n",
    "prec = np.zeros(7)\n",
    "\n",
    "for ix in range(1, 8):\n",
    "    f1s[ix] = match_stats[ix]['tp'] / (1e-7 + match_stats[ix]['tp'] + .5 * (match_stats[ix]['fp'] + match_stats[ix]['fn']))\n",
    "    rec[ix - 1] = match_stats[ix]['tp'] / (1e-7 + match_stats[ix]['tp'] + match_stats[ix]['fn'])\n",
    "    prec[ix - 1] = match_stats[ix]['tp'] / (1e-7 + match_stats[ix]['tp'] + match_stats[ix]['fp'])\n",
    "f1s[0] = np.mean(f1s[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386090a-e4ce-4244-8ab1-6ff235b74bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a02fa612-15ac-422c-8da1-0873ce3fb744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.        , 1.        , 1.        , 0.99934273, 0.99992595,\n",
       "        1.        , 1.        ]),\n",
       " array([1.        , 1.        , 1.        , 0.99934273, 0.99992595,\n",
       "        1.        , 1.        ]),\n",
       " array([0.99989553, 1.        , 1.        , 1.        , 0.99934273,\n",
       "        0.99992595, 1.        , 1.        ]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec, rec, f1s #deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e040447-3a42-4bf9-afaa-fc96c0ede4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.99989253, 0.99993514, 1.        , 0.99964148, 1.        ,\n",
       "        1.        , 0.99976943]),\n",
       " array([0.99989253, 0.99993514, 1.        , 0.99960166, 1.        ,\n",
       "        1.        , 0.99976943]),\n",
       " array([0.99988838, 0.99989253, 0.99993514, 1.        , 0.99962157,\n",
       "        1.        , 1.        , 0.99976943]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec, rec, f1s #roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cd5d58f-cd21-416f-bb18-00c737eba333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.        , 1.        , 1.        , 0.99964149, 0.99992595,\n",
       "        1.        , 1.        ]),\n",
       " array([1.        , 1.        , 1.        , 0.99964149, 0.99992595,\n",
       "        1.        , 1.        ]),\n",
       " array([0.99993821, 1.        , 1.        , 1.        , 0.99964149,\n",
       "        0.99992595, 1.        , 1.        ]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec, rec, f1s #xlnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da3501e-22a1-4b97-8554-b68623015919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1270586-9545-4685-8d77-cfca63cbc515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620fd3a1-ab6e-4e61-b5c3-35b2909b9d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f87842c-aa75-4403-92fd-109983c5ffd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe4c514-e51b-4439-87ab-33540e8c8b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230b9edd-4601-4233-942a-c43702792bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498fa26-a421-4832-8273-2c0e386c9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/qwe/projects/feedback/models_training/longformer/sumbission/codes')\n",
    "gpun = sys.argv[1]\n",
    "seed = 0\n",
    "min_len=0\n",
    "wd = 1e-2\n",
    "weights_pow = 0.1\n",
    "use_groupped_weights = False\n",
    "global_attn = 0\n",
    "label_smoothing = 0.1\n",
    "extra_dense = False\n",
    "epochs = 9\n",
    "batch_size = 8\n",
    "grad_acc_steps = batch_size\n",
    "grad_checkpt = True\n",
    "data_prefix = ''\n",
    "max_grad_norm = 25 * batch_size\n",
    "use_mixup = False\n",
    "mixup_beta = 1.\n",
    "start_eval_at = 3000\n",
    "lr = 32e-6\n",
    "min_lr = 32e-6\n",
    "dataset_version = 1\n",
    "warmup_steps = 500\n",
    "d1, d2, d3 = (0,0,0)\n",
    "if gpun == '0':\n",
    "    val_fold = 1\n",
    "    d1 = .05\n",
    "elif gpun == '1':\n",
    "    val_fold = 0\n",
    "if gpun == '2':\n",
    "    val_fold = 0\n",
    "    max_grad_norm = 1.\n",
    "    use_mixup = True\n",
    "elif gpun == '3':\n",
    "    use_mixup = True\n",
    "    val_fold = 0\n",
    "    mixup_beta = .5\n",
    "\n",
    "decay_bias = False\n",
    "eval_interval = 200\n",
    "    \n",
    "sys.path.append('longformer/tvm/python/')\n",
    "sys.path.append('longformer/')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpun\n",
    "import torch as t\n",
    "# t.use_deterministic_algorithms(True)\n",
    "# from longformer.longformer import Longformer, LongformerConfig, RobertaModel\n",
    "# from longformer.sliding_chunks import pad_to_window_size\n",
    "from transformers import XLNetTokenizerFast, XLNetModel\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import re\n",
    "\n",
    "with open('../../token_counts.pickle', 'rb') as f:\n",
    "    groupped_token_counts, ungroupped_token_counts = pickle.load(f)\n",
    "    \n",
    "if use_groupped_weights:\n",
    "    counts = groupped_token_counts\n",
    "else:\n",
    "    counts = ungroupped_token_counts\n",
    "\n",
    "token_weights = (counts.mean() / counts) ** weights_pow\n",
    "\n",
    "\n",
    "\n",
    "run = wandb.init(entity='schudov', project='feedback_xlnet_large')\n",
    "run.name = f'fold{val_fold}_minlr{min_lr}_maxlr{lr}_wd{wd}_warmup{warmup_steps}_gradnorm{max_grad_norm}_biasdecay{decay_bias}_ls{label_smoothing}_wp{weights_pow}_data{dataset_version}_mixup{use_mixup}_beta{mixup_beta}_d1{d1}_d2{d2}_d3{d3}'\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "    np.random.seed(seed)\n",
    "    t.manual_seed(seed)\n",
    "    t.cuda.manual_seed_all(seed)\n",
    "    t.backends.cudnn.deterministic = True\n",
    "    t.backends.cudnn.benchmark = False\n",
    "    \n",
    "    \n",
    "label_names = ['None', 'Lead', 'Position', 'Evidence', 'Claim',\n",
    "               'Concluding Statement', 'Counterclaim', 'Rebuttal']\n",
    "\n",
    "all_texts = {}\n",
    "for f in glob('../../train/*.txt'):\n",
    "    with open(f) as x:\n",
    "        all_texts[f.split('/')[-1].split('.')[0]] = x.read()\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "class TrainDataset(t.utils.data.Dataset):\n",
    "    def __init__(self, ids):\n",
    "        self.ids = ids\n",
    "        \n",
    "        self.data = h5py.File(f'../../xlnet_data_v{dataset_version}.h5py')\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def __getitem__(self, ix):\n",
    "        x = self.ids[ix]\n",
    "        tokens = self.data['tokens'][x]\n",
    "        attention_mask = self.data['attention_masks'][x]\n",
    "        num_tokens = self.data['num_tokens'][x, 0]\n",
    "        cbio_labels = self.data[f'{data_prefix}cbio_labels'][x]\n",
    "        cbio_labels *= (1 - label_smoothing)\n",
    "        cbio_labels += label_smoothing / 15\n",
    "        label_mask = np.zeros_like(attention_mask)\n",
    "        argmax_labels = cbio_labels.argmax(-1)\n",
    "        for ix in range(1, 15):\n",
    "            label_mask[argmax_labels==ix] = token_weights[ix]\n",
    "        zero_label_mask = argmax_labels==0\n",
    "        zero_label_mask[-2:] = False\n",
    "        zero_label_mask[:-num_tokens] = False\n",
    "        label_mask[zero_label_mask] = token_weights[0]\n",
    "        return tokens, attention_mask, cbio_labels, label_mask, num_tokens\n",
    "    \n",
    "    \n",
    "class ValDataset(t.utils.data.Dataset):\n",
    "    def __init__(self, ids):\n",
    "        self.ids = ids\n",
    "        self.data = h5py.File('../../xlnet_data_v1.h5py')\n",
    "        self.csv = pd.read_csv('../../train.csv')\n",
    "        self.space_regex = re.compile('[\\s\\n]')\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def __getitem__(self, ix):\n",
    "        x = self.ids[ix]\n",
    "        text = all_texts[val_files[ix]]\n",
    "        gt_dict = {}\n",
    "        sample_df = self.csv.loc[self.csv.id==val_files[ix]]\n",
    "        for cat_ix in range(1, 8):\n",
    "            cat_name = label_names[cat_ix]\n",
    "            cat_entities = sample_df.loc[sample_df.discourse_type==cat_name]\n",
    "            if len(cat_entities):\n",
    "                gt_dict[cat_ix] = [(x[0], x[1]) for x in cat_entities.predictionstring.map(split_predstring)]\n",
    "        \n",
    "        tokens = self.data['tokens'][x]\n",
    "        attention_mask = self.data['attention_masks'][x]\n",
    "        num_tokens = self.data['num_tokens'][x, 0]\n",
    "        token_bounds = self.data['token_offsets'][x]\n",
    "\n",
    "        cbio_labels = self.data['cbio_labels'][x]\n",
    "        \n",
    "        label_mask = np.zeros_like(attention_mask)\n",
    "        argmax_labels = cbio_labels.argmax(-1)\n",
    "        for cat_ix in range(1, 15):\n",
    "            label_mask[argmax_labels==cat_ix] = token_weights[cat_ix]\n",
    "        zero_label_mask = argmax_labels==0\n",
    "        zero_label_mask[-2:] = False\n",
    "        zero_label_mask[:-num_tokens] = False\n",
    "        label_mask[zero_label_mask] = token_weights[0]\n",
    "        index_map = []\n",
    "        current_word = 0\n",
    "        blank = False\n",
    "        for char_ix in range(text.index(text.strip()[0]), len(text)):\n",
    "            if self.space_regex.match(text[char_ix]) is not None:\n",
    "                blank = True\n",
    "            elif blank:\n",
    "                current_word += 1\n",
    "                blank = False\n",
    "            index_map.append(current_word)\n",
    "        \n",
    "        return tokens, attention_mask, cbio_labels, label_mask, token_bounds, gt_dict, index_map, num_tokens\n",
    "\n",
    "    \n",
    "first_batch = True\n",
    "def train_collate_fn(ins):\n",
    "    global first_batch\n",
    "    if first_batch:\n",
    "        max_len = 2048\n",
    "        first_batch = False\n",
    "    else:\n",
    "        max_len = (max(x[-1] for x in ins) + 7) // 8 * 8\n",
    "    return tuple(t.from_numpy(np.concatenate([ins[z][x][None, -max_len:] for z in range(len(ins))])) \n",
    "                 for x in range(len(ins[0]) - 1))\n",
    "\n",
    "def val_collate_fn(ins):\n",
    "    max_len = (max(x[-1] for x in ins) + 7) // 8 * 8\n",
    "    return tuple(t.from_numpy(np.concatenate([ins[z][x][None, -max_len:] for z in range(len(ins))])) \n",
    "                 for x in range(len(ins[0]) - 3)) \\\n",
    "                 + ([x[-3] for x in ins], [x[-2] for x in ins], np.array([x[-1] for x in ins]),)\n",
    "\n",
    "def extract_entities(ps, n):\n",
    "    cat_ps = ps.argmax(-1).cpu().numpy()\n",
    "    all_entities = {}\n",
    "    current_cat = None\n",
    "    current_start = None\n",
    "    seq_len = ps.shape[1]\n",
    "    for ix in range(seq_len - n, seq_len - 2):\n",
    "        if cat_ps[ix] % 2 == 1:\n",
    "            if current_cat is not None:\n",
    "                if current_cat not in all_entities:\n",
    "                    all_entities[current_cat] = []\n",
    "                all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = (cat_ps[ix] + 1) // 2\n",
    "            current_start = ix\n",
    "    if current_cat is not None:\n",
    "        if current_cat not in all_entities:\n",
    "            all_entities[current_cat] = []\n",
    "        all_entities[current_cat].append((current_start, ix))\n",
    "    return all_entities\n",
    "\n",
    "def process_sample(raw_ps, raw_gts, index_map, bounds, gt_spans, num_tokens, match_stats, min_len=0):\n",
    "    predicted_spans = {x: [map_span_to_word_indices(span, index_map, bounds) for span in y] \n",
    "                       for x, y in extract_entities(raw_ps, num_tokens).items()}\n",
    "    predicted_spans = {x: [z for z in y if z[1] - z[0] >= min_len] for x, y in predicted_spans.items()}\n",
    "    \n",
    "    for cat_ix in range(1, 8):\n",
    "        \n",
    "        pspans = predicted_spans.get(cat_ix, [])\n",
    "        gspans = gt_spans.get(cat_ix, [])\n",
    "        if not len(pspans) or not len(gspans):\n",
    "            match_stats[cat_ix]['fn'] += len(gspans)\n",
    "            match_stats[cat_ix]['fp'] += len(pspans)\n",
    "        else:\n",
    "            all_overlaps = np.zeros((len(pspans), len(gspans)))\n",
    "            for x1 in range(len(pspans)):\n",
    "                pspan = pspans[x1]\n",
    "                for x2 in range(len(gspans)):\n",
    "                    gspan = gspans[x2]\n",
    "                    start_ix = max(pspan[0], gspan[0])\n",
    "                    end_ix = min(pspan[1], gspan[1])\n",
    "                    overlap = max(0, end_ix - start_ix + 1)\n",
    "                    if overlap > 0:\n",
    "                        o1 = overlap / (pspan[1] - pspan[0] + 1)\n",
    "                        o2 = overlap / (gspan[1] - gspan[0] + 1)\n",
    "                        if min(o1, o2) >= .5:\n",
    "                            all_overlaps[x1, x2] = max(o1, o2)\n",
    "            unused_p_ix = set(range(len(pspans)))\n",
    "            unused_g_ix = set(range(len(gspans)))\n",
    "            col_size = len(pspans)\n",
    "            row_size = len(gspans)\n",
    "            for ix in np.argsort(all_overlaps.ravel())[::-1]:\n",
    "                if not len(unused_g_ix) or not len(unused_p_ix) or all_overlaps.ravel()[ix] == 0:\n",
    "                    match_stats[cat_ix]['fp'] += len(unused_p_ix)\n",
    "                    match_stats[cat_ix]['fn'] += len(unused_g_ix)\n",
    "                    break\n",
    "                p_ix = ix // row_size\n",
    "                g_ix = ix % row_size\n",
    "                if p_ix not in unused_p_ix or g_ix not in unused_g_ix:\n",
    "                    continue\n",
    "                match_stats[cat_ix]['tp'] += 1\n",
    "                unused_g_ix.remove(g_ix)\n",
    "                unused_p_ix.remove(p_ix)\n",
    "    return match_stats\n",
    "\n",
    "def map_span_to_word_indices(span, index_map, bounds):\n",
    "    return (index_map[bounds[span[0], 0]], index_map[bounds[span[1], 1] - 1])\n",
    "\n",
    "def split_predstring(x):\n",
    "    vals = x.split()\n",
    "    return int(vals[0]), int(vals[-1])\n",
    "    \n",
    "\n",
    "with open('../../id_to_ix_map.pickle', 'rb') as f:\n",
    "    id_to_ix_map = {x.split('/')[-1].split('.')[0]: y for x, y in pickle.load(f).items()}\n",
    "with open('../../data_splits.pickle', 'rb') as f:\n",
    "    data_splits = pickle.load(f)\n",
    "\n",
    "train_ids = [id_to_ix_map[x] for fold in range(5) if fold != val_fold for x in data_splits[seed][250]['normed'][fold]]\n",
    "val_files = data_splits[seed][250]['normed'][val_fold]\n",
    "val_ids = [id_to_ix_map[x] for x in val_files]\n",
    "\n",
    "if use_mixup:\n",
    "    epochs *= 2\n",
    "\n",
    "all_train_ids = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_ids = [x for x in train_ids]\n",
    "    for _ in range(3):\n",
    "        shuffle(epoch_train_ids)\n",
    "    all_train_ids.extend(epoch_train_ids)\n",
    "# if shuffle_data:\n",
    "#     shuffle(all_train_ids)\n",
    "    \n",
    "class TvmLongformer(t.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feats = XLNetModel.from_pretrained('xlnet-large-cased')\n",
    "        self.feats.pooler = None\n",
    "        if grad_checkpt:\n",
    "            self.feats.use_grad_checkpoint = True\n",
    "        self.feats.train();\n",
    "        if extra_dense:\n",
    "            self.class_projector = t.nn.Sequential(\n",
    "                t.nn.LayerNorm(1024),\n",
    "                t.nn.Linear(1024, 256),\n",
    "                t.nn.GELU(),\n",
    "                t.nn.Linear(256, 15)\n",
    "            )\n",
    "        else:\n",
    "            self.class_projector = t.nn.Sequential(\n",
    "                t.nn.LayerNorm(1024),\n",
    "                t.nn.Linear(1024, 15)\n",
    "            )\n",
    "    def forward(self, embeddings, mask):\n",
    "        return t.log_softmax(self.class_projector(self.feats(inputs_embeds=embeddings, attention_mask=mask, return_dict=False)[0]), -1)\n",
    "    \n",
    "train_bs = batch_size//grad_acc_steps\n",
    "if use_mixup:\n",
    "    train_bs *= 2\n",
    "train_dataset = t.utils.data.DataLoader(TrainDataset(all_train_ids), collate_fn=train_collate_fn, \n",
    "                                        batch_size=train_bs,\n",
    "                                        num_workers=8)\n",
    "val_dataset = t.utils.data.DataLoader(ValDataset(val_ids), collate_fn=val_collate_fn, batch_size=4, num_workers=8,\n",
    "                                      persistent_workers=True)\n",
    "\n",
    "model = TvmLongformer().cuda()\n",
    "for m in model.modules():\n",
    "    if isinstance(m, t.nn.Dropout):\n",
    "        m.p = 0\n",
    "# for l in model.feats.encoder.layer:\n",
    "#     l.attention.self.dropout.p = d1\n",
    "#     l.attention.output.dropout.p = d2\n",
    "#     l.output.dropout.p = d3\n",
    "        \n",
    "weights = []\n",
    "biases = []\n",
    "for n, p in model.named_parameters():\n",
    "    if n.startswith('feats.word_embedding') or 'LayerNorm' in n or n.endswith('bias'):\n",
    "        biases.append(p)\n",
    "    else:\n",
    "        weights.append(p)\n",
    "\n",
    "opt = t.optim.AdamW([{'params': weights, 'weight_decay': wd, 'lr': 0},\n",
    "                     {'params': biases, 'weight_decay': 0 if not decay_bias else wd, 'lr': 0}])\n",
    "\n",
    "def validate(model, dataset):\n",
    "    ls = []\n",
    "    model.eval();\n",
    "    val_matches = t.zeros(16)\n",
    "    val_labels = t.zeros(16)\n",
    "    match_stats = {ix:  {'fp': 0, 'fn': 0, 'tp': 0} for ix in range(1, 8)}\n",
    "    f1s = np.zeros(8)\n",
    "    rec = np.zeros(7)\n",
    "    prec = np.zeros(7)\n",
    "    for tokens, mask, labels, labels_mask, bounds, gt_dicts, index_map, num_tokens in val_dataset:\n",
    "        with t.no_grad():\n",
    "            tokens, mask, label, label_mask = (x.cuda() for x in (tokens, mask, labels, labels_mask))\n",
    "            with t.cuda.amp.autocast():\n",
    "                outs = model(model.feats.word_embedding(tokens), mask)\n",
    "            ls.append(-(((outs.float() * label).sum(-1) * label_mask).sum(-1) / label_mask.sum(-1)).mean())\n",
    "            match_updates = calc_acc(outs, label, label_mask)\n",
    "            val_matches += match_updates[0]\n",
    "            val_labels += match_updates[1]\n",
    "            for sample_ix, num in enumerate(num_tokens):\n",
    "                match_stats = process_sample(outs[sample_ix], labels[sample_ix], \n",
    "                                                   index_map[sample_ix], bounds[sample_ix],\n",
    "                                                   gt_dicts[sample_ix],\n",
    "                                                   num, match_stats, min_len=min_len)\n",
    "    for ix in range(1, 8):\n",
    "        f1s[ix] = match_stats[ix]['tp'] / (1e-7 + match_stats[ix]['tp'] + .5 * (match_stats[ix]['fp'] + match_stats[ix]['fn']))\n",
    "        rec[ix - 1] = match_stats[ix]['tp'] / (1e-7 + match_stats[ix]['tp'] + match_stats[ix]['fn'])\n",
    "        prec[ix - 1] = match_stats[ix]['tp'] / (1e-7 + match_stats[ix]['tp'] + match_stats[ix]['fp'])\n",
    "    f1s[0] = np.mean(f1s[1:])\n",
    "\n",
    "    model.train()\n",
    "    val_accs = (val_matches / val_labels).cpu().numpy()\n",
    "    val_labels = val_labels.cpu().numpy()\n",
    "    return t.stack(ls).mean().item(), val_accs, val_labels, f1s, rec, prec\n",
    "\n",
    "def calc_acc(raw_ps, raw_labels, valid_mask):\n",
    "    valid_mask = (valid_mask > 0).float()\n",
    "    all_matches = t.zeros(16)\n",
    "    all_labels = t.zeros(16)\n",
    "    ps = raw_ps.argmax(-1)\n",
    "    labels = raw_labels.argmax(-1) - (1 - valid_mask)\n",
    "    matched = (ps == labels)\n",
    "    for x in range(15):\n",
    "        class_mask = labels==x\n",
    "        class_labels = (class_mask).sum()\n",
    "        class_matches = (matched[class_mask].sum())\n",
    "        all_matches[x] = class_matches\n",
    "        all_labels[x] = class_labels\n",
    "    all_matches[-1] = matched.sum()\n",
    "    all_labels[-1] = valid_mask.sum()\n",
    "    return all_matches, all_labels\n",
    "\n",
    "def make_match_dict(ce, accs, labels, prefix, extras=None):\n",
    "    log_dict = {}\n",
    "    log_dict.update({f'{prefix}_ce': ce})\n",
    "    log_dict.update({f'{prefix}_ACC#' + label_names[(x + 1)  // 2] + ('_B' if x % 2 == 1 else '_I'): accs[x]\n",
    "                    for x in range(1, 15)})\n",
    "    log_dict.update({f'{prefix}_ACC#' + 'None': accs[0], f'{prefix}_ACC#' + 'ALL': accs[-1]})\n",
    "    log_dict.update({f'{prefix}_A_' + 'B': accs[1:-1:2].mean(), f'{prefix}_A_' + 'I': accs[:-1:2].mean(),\n",
    "                    f'{prefix}_A_' + 'MEAN': accs[:-1].mean()})\n",
    "    if extras is not None:\n",
    "        f1s, rec, prec = extras\n",
    "        log_dict.update({f'{prefix}_F1_{label_names[(ix + 1) // 2]}': f1s[ix]\n",
    "                         for ix in range(1, 8)})\n",
    "        log_dict.update({f'{prefix}_MacroF1': f1s[0]})\n",
    "        log_dict.update({f'{prefix}_Rec_{label_names[(ix + 1) // 2]}': rec[ix - 1]\n",
    "                         for ix in range(1, 8)})\n",
    "        log_dict.update({f'{prefix}_Prec_{label_names[(ix + 1) // 2]}': prec[ix - 1]\n",
    "                         for ix in range(1, 8)})\n",
    "    return log_dict\n",
    "\n",
    "lr_schedule = np.r_[np.linspace(0, lr, warmup_steps),\n",
    "                    (np.cos(np.linspace(0, np.pi, len(train_dataset) - warmup_steps)) * .5 + .5) * (lr - min_lr) + min_lr]\n",
    "\n",
    "zzz = []\n",
    "ls = []\n",
    "norms = []\n",
    "train_matches = t.zeros(16)\n",
    "train_labels = t.zeros(16)\n",
    "log_dict = {}\n",
    "best_val_score = 0\n",
    "\n",
    "past_averaged_params = []\n",
    "current_averaged_params = {x: y.clone().double() for x, y in model.state_dict().items()}\n",
    "from tqdm import tqdm\n",
    "step = 0\n",
    "pbar = tqdm(total=(len(train_dataset) + grad_acc_steps - 1)//grad_acc_steps)\n",
    "model_params = [p for n, p in model.named_parameters() if n != 'feats.mask_emb' and not \n",
    "                (n.endswith('.r_s_bias') or n.endswith('.seg_embed'))]\n",
    "param_names = [n for n, p in model.named_parameters() if n != 'feats.mask_emb' and not \n",
    "                (n.endswith('.r_s_bias') or n.endswith('.seg_embed'))]\n",
    "    \n",
    "scaler = t.cuda.amp.GradScaler(init_scale=65536.0/grad_acc_steps)\n",
    "for step_, batch in enumerate(train_dataset):\n",
    "    if step_ % grad_acc_steps == 0:\n",
    "        for ix in range(len(opt.param_groups)):\n",
    "            opt.param_groups[ix]['lr'] = lr_schedule[step]\n",
    "        opt.zero_grad()\n",
    "    tokens, mask, label, label_mask = (x.cuda() for x in batch)\n",
    "    if use_mixup:\n",
    "        rand = t.tensor(np.random.beta(mixup_beta, mixup_beta), dtype=t.float, device='cuda:0').expand(1,1,1)\n",
    "        emb_a, emb_b = model.feats.word_embedding(tokens).chunk(2, 0)\n",
    "        lbl_a, lbl_b = label.chunk(2, 0)\n",
    "        lm_a, lm_b = label_mask.chunk(2, 0)\n",
    "        mask = (mask.sum(0, keepdim=True) > 0).float()\n",
    "        embeddings = emb_a * rand.expand(1,1,1) + emb_b * (1 - rand)\n",
    "        label = lbl_a * rand + lbl_b * (1 - rand)\n",
    "        label_mask = lm_a * rand + lm_b * (1 - rand)\n",
    "    else:\n",
    "        embeddings = model.feats.word_embedding(tokens)\n",
    "    with t.cuda.amp.autocast():\n",
    "        outs = model(embeddings, mask)\n",
    "    l = -(((outs.float() * label).sum(-1) * label_mask).sum(-1) / label_mask.sum(-1)).mean()\n",
    "    scaler.scale(l).backward()\n",
    "    ls.append(l.detach())\n",
    "    if step_ % grad_acc_steps == grad_acc_steps - 1:\n",
    "        if max_grad_norm is not None:\n",
    "            scaler.unscale_(opt)\n",
    "            norm = t.norm(t.stack([t.norm(p.grad.detach()) for p in model_params]))\n",
    "            # norms.append(norm)\n",
    "            if t.isfinite(norm):\n",
    "                grad_mult = (max_grad_norm / (norm + 1e-6)).clamp(max=1.)\n",
    "                for p in model_params:\n",
    "                    p.grad.detach().mul_(grad_mult)\n",
    "        scaler.step(opt)#.step()\n",
    "        scaler.update()\n",
    "        with t.no_grad():\n",
    "            current_averaged_params = {x: y.clone().double() + current_averaged_params[x] for x, y in model.state_dict().items()}\n",
    "            match_updates = calc_acc(outs, label, label_mask)\n",
    "            train_matches += match_updates[0]\n",
    "            train_labels += match_updates[1]\n",
    "        if step % eval_interval == (eval_interval - 1):\n",
    "            # norms = t.stack(norms)\n",
    "            log_dict = {}\n",
    "            # 'norm_mean': t.mean(norms).item(),\n",
    "            #             'norm_max': t.max(norms).item(),\n",
    "            #             'norm_.9': t.quantile(norms, .9).item(),\n",
    "            #             'norm_.95': t.quantile(norms, .95).item(),}\n",
    "            \n",
    "            norms = []\n",
    "            t.autograd.set_grad_enabled(False)\n",
    "            params_backup = {x: y.cpu().clone() for x, y in model.state_dict().items()}\n",
    "\n",
    "            train_accs = (train_matches / train_labels).cpu().numpy()\n",
    "            train_labels = train_labels.cpu().numpy()\n",
    "            log_dict.update(make_match_dict(t.stack(ls).mean().item(), train_accs, train_labels, 'Train'))\n",
    "            ls = []\n",
    "            train_matches = t.zeros(16)\n",
    "            train_labels = t.zeros(16)\n",
    "\n",
    "            past_averaged_params.append({x: (y * (1 / eval_interval)).cpu().half() for x, y in current_averaged_params.items()})\n",
    "            if len(past_averaged_params) > 15:\n",
    "                past_averaged_params = past_averaged_params[-15:]\n",
    "            if step > start_eval_at:\n",
    "                evaluation_params = None\n",
    "                for params_ix in range(1, len(past_averaged_params) + 1):\n",
    "                    if evaluation_params is None:\n",
    "                        evaluation_params = {x: y.double() for x, y in past_averaged_params[-params_ix].items()}\n",
    "                    else:\n",
    "                        evaluation_params = {x: y.double() + evaluation_params[x] \n",
    "                                             for x, y in past_averaged_params[-params_ix].items()}\n",
    "                    if params_ix >= 10 and params_ix % 5 == 0:\n",
    "                        model.cpu()\n",
    "                        model.load_state_dict({x: (y / params_ix).float() for x, y in evaluation_params.items()})\n",
    "                        model.cuda()\n",
    "                        val_ce, val_accs, val_labels, f1s, rec, prec = validate(model, val_dataset)\n",
    "                        val_score = f1s[0]\n",
    "                        if val_score >= best_val_score:\n",
    "                            best_val_score = val_score\n",
    "                            t.save(model.state_dict(), f'checkpoints/{run.name}')\n",
    "                        log_dict.update(make_match_dict(val_ce, val_accs, val_labels, f'ValSWA{params_ix*eval_interval}', (f1s, rec, prec)))\n",
    "\n",
    "            \n",
    "            model.cpu()\n",
    "            model.load_state_dict(params_backup)\n",
    "            model.cuda()\n",
    "            current_averaged_params = {x: y.clone() for x, y in model.state_dict().items()}\n",
    "    \n",
    "            t.autograd.set_grad_enabled(True)\n",
    "            wandb.log(log_dict)        \n",
    "        step += 1\n",
    "        pbar.update(1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
