{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601c1bc7-15d0-446b-913c-3c7eeead066d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from transformers import XLNetTokenizerFast\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8676079f-5c03-4035-8d58-6959605c8aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizerFast\n",
    "tokenizer = XLNetTokenizerFast.from_pretrained('xlnet-large-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d608f708-056e-477e-b786-8764da927da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(['^'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c13444e9-6de8-4239-9a2e-67a530f47162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.get_vocab()['<eop>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f798fe-7125-4154-b23d-2db4d8221c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.get_vocab()['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ae7ca8-b50d-4bf4-b24b-d7f5a8195d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../train.csv')\n",
    "data.loc[data.discourse_id==1623258656795.0, 'discourse_text'] =  data.loc[data.discourse_id==1623258656795.0, \n",
    "                                                                           'discourse_text'].map(lambda x: x.replace('florida', 'LOCATION_NAME')).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9596e2b8-eeab-4bdc-888f-d9aca60467cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['None', 'Lead', 'Position', 'Evidence', 'Claim',\n",
    "               'Concluding Statement', 'Counterclaim', 'Rebuttal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9fc64a6-21cd-4999-8e10-0c79c9a70a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "            'Lead': '#8000ff',\n",
    "            'Position': '#2b7ff6',\n",
    "            'Evidence': '#2adddd',\n",
    "            'Claim': '#80ffb4',\n",
    "            'Concluding Statement': 'd4dd80',\n",
    "            'Counterclaim': '#ff8042',\n",
    "            'Rebuttal': '#ff0000'\n",
    "         }\n",
    "options = {\"ents\": list(colors.keys()), \"colors\": colors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6db29c97-5606-4e72-9e16-bddb3a22fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_maps = dict(zip(colors, range(1, 2 * len(colors), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dfe8093-d0b5-4a97-90d4-54105d39a7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lead': 1,\n",
       " 'Position': 3,\n",
       " 'Evidence': 5,\n",
       " 'Claim': 7,\n",
       " 'Concluding Statement': 9,\n",
       " 'Counterclaim': 11,\n",
       " 'Rebuttal': 13}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86dbfeb2-73bc-4914-bc83-ac50e007276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(ID, df=None):\n",
    "    with open(f'../../train/{ID}.txt') as f:\n",
    "        org_text = f.read().strip()\n",
    "    if df is None:\n",
    "        dsc = data.loc[data.id==ID].sort_values('discourse_start')\n",
    "    else:\n",
    "        dsc = df\n",
    "    ents = []\n",
    "    pointer = 0\n",
    "    for _, row in dsc.iterrows():\n",
    "        entity = row.discourse_text.strip()\n",
    "        starts_at = org_text[pointer:].index(entity) + pointer\n",
    "        end = 0\n",
    "        start = 0\n",
    "        # if starts_at + len(entity) < len(org_text) and  punkt_re.match(org_text[starts_at + len(entity)]) is not None:\n",
    "        #     end = 1\n",
    "        # if punkt_re.match(org_text[starts_at]) is not None:\n",
    "        #     start = 1\n",
    "        ents.append({'start': starts_at + start, 'end': starts_at + len(entity) + end, 'label': row.discourse_type})\n",
    "        pointer = starts_at + len(entity)\n",
    "    displacy.render({'text': org_text, 'ents': ents, 'title': ID}, style=\"ent\",\n",
    "                    options=options, manual=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fac03bb6-d42a-4b4a-baea-fe447bf16f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = re.compile('[0-9a-zA-z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d1df82e-4072-4632-8882-c35b3bfe4612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_more_targets(targets):\n",
    "    linkage = np.zeros((len(targets), 2), 'f4')\n",
    "    class_index = np.zeros((len(targets),), 'f4')\n",
    "    linkage_mask = np.ones((len(targets),), 'f4')\n",
    "    current_target = -2\n",
    "    for ix in range(0, len(targets) -2):\n",
    "        if ((current_target % 2 == 0 and current_target == targets[ix]) \n",
    "              or (targets[ix] == current_target + 1 and current_target %2 == 1)):\n",
    "            linkage[ix - 1, 1] = 1\n",
    "            linkage[ix, 0] = 1\n",
    "        current_target = targets[ix]\n",
    "        class_index[:] = [x // 2 for x in targets + 1]\n",
    "    link_sums = (linkage * np.array([2, 1])).sum(-1).astype('i4')\n",
    "    bi =  np.zeros((len(targets), 2), 'f4')\n",
    "    bi[link_sums < 2, 0] = 1\n",
    "    bi[link_sums >= 2, 1] = 1\n",
    "    bio = np.array(bi)\n",
    "    bio[targets==0] = 0\n",
    "    bies =  np.zeros((len(targets), 4), 'f4')\n",
    "    bies[:, :2] = bi\n",
    "    bies[link_sums == 0] = (0, 0, 0, 1)\n",
    "    bies[link_sums == 2] = (0, 0, 1, 0)\n",
    "    bieso = np.array(bies)\n",
    "    bieso[targets==0] = 0\n",
    "    return class_index, bi, bio, bies, bieso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1480d2c6-2c72-4f89-ad56-3a5ad1e80ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_labels(class_index, bi, bio, bies, bieso):\n",
    "    combined_bi = class_index * 2 + bi[:, 0]\n",
    "    combined_bies = class_index * 4 + bies @ np.array([0, 1, 2, 3])\n",
    "    non_o_index = np.where(class_index != 0)[0]\n",
    "    combined_bieso = np.array(class_index)\n",
    "    combined_bieso[non_o_index] = (class_index[non_o_index] - 1) * 4 + bieso[non_o_index] @ np.array([1, 2, 3, 4])\n",
    "    combined_bio = np.array(class_index)\n",
    "    combined_bio[non_o_index] = (class_index[non_o_index] - 1) * 2 + bio[non_o_index] @ np.array([1, 2])\n",
    "    return combined_bi, combined_bio, combined_bies, combined_bieso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb2be59a-4984-4430-b17c-876efb691c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9de84cde-4841-4331-9153-bd069512e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = h5py.File('../../xlnet_data_v3.h5py', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8caa473-8da6-4df2-8465-89963371da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_texts = len(glob('../../train/*.txt')) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4c1eebb-a629-45f1-86b2-35b240291816",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dataset = data_file.create_dataset('tokens', (num_texts, 2048), 'i8')\n",
    "attention_masks_dataset = data_file.create_dataset('attention_masks', (num_texts, 2048), 'f4')\n",
    "token_offsets_dataset = data_file.create_dataset('token_offsets', (num_texts, 2048, 2), 'i4')\n",
    "class_labels_dataset = data_file.create_dataset('class_labels', (num_texts, 2048, 8), 'f4')\n",
    "num_tokens_dataset = data_file.create_dataset('num_tokens', (num_texts, 2048), 'i4')\n",
    "\n",
    "bi_labels_dataset = data_file.create_dataset('bi_labels', (num_texts, 2048, 2), 'f4')\n",
    "bio_labels_dataset = data_file.create_dataset('bio_labels', (num_texts, 2048, 2), 'f4')\n",
    "bies_labels_dataset = data_file.create_dataset('bies_labels', (num_texts, 2048, 4), 'f4')\n",
    "bieso_labels_dataset = data_file.create_dataset('bieso_labels', (num_texts, 2048, 4), 'f4')\n",
    "\n",
    "cbi_labels_dataset = data_file.create_dataset('cbi_labels', (num_texts, 2048, 16), 'f4')\n",
    "cbio_labels_dataset = data_file.create_dataset('cbio_labels', (num_texts, 2048, 15), 'f4')\n",
    "cbies_labels_dataset = data_file.create_dataset('cbies_labels', (num_texts, 2048, 32), 'f4')\n",
    "cbieso_labels_dataset = data_file.create_dataset('cbieso_labels', (num_texts, 2048, 29), 'f4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40787e7b-e8f1-476c-b41b-9f00b6425f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 40.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for dataset in tqdm((tokens_dataset, \n",
    "               attention_masks_dataset,\n",
    "               token_offsets_dataset,\n",
    "               class_labels_dataset,\n",
    "               num_tokens_dataset,\n",
    "               bi_labels_dataset,\n",
    "               bio_labels_dataset,\n",
    "               bies_labels_dataset,\n",
    "               bieso_labels_dataset,\n",
    "               cbi_labels_dataset,\n",
    "               cbio_labels_dataset,\n",
    "               cbies_labels_dataset,\n",
    "               cbieso_labels_dataset,)):\n",
    "    dataset[-1] = 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "199be078-08bb-41c9-8f2e-60dda9020d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(indices, num_labels):\n",
    "    array = np.zeros((len(indices), num_labels))\n",
    "    array[np.arange(len(indices)), indices.astype('i4')] = 1\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cd49631-e16e-4f43-8b15-0f56598b7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_ix_map = {filename: ix for ix, filename in enumerate(glob('../../train/*.txt'))}\n",
    "with open('../../id_to_ix_map.pickle', 'wb') as f:\n",
    "    import dill as pickle\n",
    "    pickle.dump(id_to_ix_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd5047b6-a060-472a-86b0-0d743c46d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_text = lambda x: x.replace('\\n', '^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "415ca6af-5f85-4d56-b8d2-7f15df14f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24d3a44f-56be-4330-abe4-a78e9c4fb467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(ps, n):\n",
    "    cat_ps = ps\n",
    "    all_entities = {}\n",
    "    current_cat = None\n",
    "    current_start = None\n",
    "    for ix in range(n - 2):\n",
    "        if cat_ps[ix] % 2 == 1:\n",
    "            if current_cat is not None:\n",
    "                if current_cat not in all_entities:\n",
    "                    all_entities[current_cat] = []\n",
    "                all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = (cat_ps[ix] + 1) // 2\n",
    "            current_start = ix\n",
    "    if current_cat is not None:\n",
    "        if current_cat not in all_entities:\n",
    "            all_entities[current_cat] = []\n",
    "        all_entities[current_cat].append((current_start, ix))\n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "raw",
   "id": "612d3647-9c81-4861-8d26-15fdb43252d3",
   "metadata": {},
   "source": [
    "token_ents = sorted((x for v in extract_entities(targets, num_tokens).values() for x in v), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0359d869-8e87-4f60-a14f-4e67b551d94c",
   "metadata": {},
   "source": [
    "for a, b in zip(ent_boundaries, token_ents):\n",
    "    print(a[-1])\n",
    "    print('#'*10)\n",
    "    print(new_text[a[0]:a[1]])\n",
    "    print('+'*10)\n",
    "    print(tokenizer.decode(tokenizer_outs['input_ids'][b[0]:b[1]]).replace('<unk>', '\\x92'))\n",
    "    print('z'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d33572a4-7673-4229-b620-a5a4e2359f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_newlines_and_tokenize(org_text):\n",
    "    prev_unk_positions = np.where(np.array(tokenizer(org_text)['input_ids']) == 0)[0]\n",
    "    text = org_text.replace('\\n', '\\x92 ')\n",
    "    tokenizer_outs = tokenizer(text, return_offsets_mapping=True)\n",
    "    new_unk_positions = np.where(np.array(tokenizer_outs['input_ids'])\n",
    "                                    == 0)[0]\n",
    "    for ix1 in range(len(new_unk_positions)):\n",
    "        for ix2 in range(len(prev_unk_positions)):\n",
    "            if new_unk_positions[ix1] == prev_unk_positions[ix2]:\n",
    "                break\n",
    "            elif new_unk_positions[ix1] < prev_unk_positions[ix2]:\n",
    "                prev_unk_positions[ix2:] += 1\n",
    "                break\n",
    "    assert len(set(prev_unk_positions).difference(new_unk_positions)) == 0\n",
    "    \n",
    "    new_text = tokenizer.decode(tokenizer_outs['input_ids'])[:-10].replace('<unk>', '\\x92')\n",
    "    tokenizer_outs = tokenizer(new_text, return_offsets_mapping=True)\n",
    "    num_new_tokens = len(tokenizer_outs['input_ids'])\n",
    "    for newline_pos in set(new_unk_positions).difference(prev_unk_positions):\n",
    "        if newline_pos >= num_new_tokens:\n",
    "            print(num_new_tokens, newline_pos)\n",
    "            break\n",
    "        tokenizer_outs['input_ids'][newline_pos] = 8\n",
    "    return tokenizer_outs, new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be249aaa-40d5-4211-a647-098f48e071af",
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a29de6-79d0-4d03-b756-a629f75e7bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████████████▍                                                                            | 4337/15594 [23:38<57:51,  3.24it/s]"
     ]
    }
   ],
   "source": [
    "id_to_ix_map = {}\n",
    "for filename_ix, filename in tqdm(enumerate(glob('../../train/*.txt')), total = num_texts-1):\n",
    "    ID = filename.split('/')[-1].split('.')[0]\n",
    "    with open(filename) as f:\n",
    "        text = fix_text(f.read().strip())\n",
    "    tokenizer_outs = tokenizer(text, return_offsets_mapping=True)\n",
    "    tokenizer_outs['input_ids'] = [x if x != 32000 else 8 for x in tokenizer_outs['input_ids']]\n",
    "    text_data = data.loc[data.id==ID].sort_values('discourse_start')\n",
    "    ent_boundaries = []\n",
    "    pointer = 0\n",
    "    for row_id, row in text_data.iterrows():\n",
    "        entity_text = fix_text(row.discourse_text.strip())\n",
    "        entity_text = entity_text[next(regexp.finditer(entity_text)).start():]\n",
    "        if len(entity_text.split()[0]) == 1 and pointer != 0:\n",
    "            entity_start_ix = text[pointer:].index(entity_text)\n",
    "            prev_text = text[:pointer + entity_start_ix]\n",
    "            if pointer + entity_start_ix > 0 and prev_text[-1].isalpha():\n",
    "                broken_indices.append((filename_ix, ID))\n",
    "                print('cut entity ', filename_ix, ID)\n",
    "                cut_word_chunk_size = len(prev_text.split()[-1])\n",
    "                if cut_word_chunk_size > 1:\n",
    "                    entity_text = entity_text[next(regexp.finditer(entity_text[1:])).start() + 1]\n",
    "        if row.discourse_id in (1620147556527.0, 1622983056026.0):\n",
    "            pointer += 10\n",
    "        offset = text[pointer:].index(entity_text)\n",
    "        starts_at = offset + pointer\n",
    "        ent_boundaries.append((starts_at, starts_at + len(entity_text), row.discourse_type))\n",
    "        pointer = starts_at + len(entity_text)\n",
    "    all_boundaries = [(z, x[-1], t) for x in ent_boundaries for z, t in zip(x[:2], ('start', 'end'))]\n",
    "    current_target = 0\n",
    "    targets = np.zeros(len(tokenizer_outs['input_ids']), 'i8')\n",
    "    token_positions = np.vstack(tokenizer_outs['offset_mapping']).astype('i4')\n",
    "    for token_ix in range(len(tokenizer_outs['input_ids'])):\n",
    "        token_start_ix, token_end_ix = tokenizer_outs['offset_mapping'][token_ix]\n",
    "        if token_end_ix != 0 and (all_boundaries[0][2] == 'end' and token_end_ix >= all_boundaries[0][0])\\\n",
    "                            or (all_boundaries[0][2] == 'start' and token_end_ix > all_boundaries[0][0]):\n",
    "            if all_boundaries[0][2] == 'start':\n",
    "                current_target = token_maps[all_boundaries[0][1]]\n",
    "                targets[token_ix] = current_target\n",
    "                if token_end_ix == all_boundaries[1][0]:\n",
    "                    current_target = 0\n",
    "                    all_boundaries.pop(0)\n",
    "                else:\n",
    "                    current_target += 1\n",
    "            else:\n",
    "                if len(all_boundaries) > 1 and token_end_ix > all_boundaries[1][0]:\n",
    "                    if token_start_ix >= all_boundaries[1][0]:\n",
    "                        assert text[all_boundaries[0][0] - 1] == '¨'\n",
    "                    all_boundaries.pop(0)\n",
    "                    current_target = token_maps[all_boundaries[0][1]]\n",
    "                    targets[token_ix] = current_target\n",
    "                    current_target += 1\n",
    "                else:\n",
    "                    if token_start_ix >= all_boundaries[0][0]:\n",
    "                        current_target = 0\n",
    "                    targets[token_ix] = current_target\n",
    "                    current_target = 0\n",
    "            all_boundaries.pop(0)\n",
    "            if not all_boundaries:\n",
    "                break\n",
    "        else:\n",
    "            targets[token_ix] = current_target\n",
    "    class_index, bi, bio, bies, bieso = make_more_targets(targets)\n",
    "    combined_bi, combined_bio, combined_bies, combined_bieso = combine_labels(class_index, bi, bio, bies, bieso)\n",
    "    assert (combined_bio[:-2] == targets[:-2]).all()\n",
    "    num_tokens = len(targets)\n",
    "    tokens_range = np.arange(num_tokens)\n",
    "    tokens_dataset[filename_ix, -num_tokens:] = tokenizer_outs['input_ids']\n",
    "    tokens_dataset[filename_ix, :-num_tokens] = 5\n",
    "    attention_masks_dataset[filename_ix, -num_tokens:] = tokenizer_outs['attention_mask']\n",
    "    attention_masks_dataset[filename_ix, :-num_tokens] = 0\n",
    "    token_offsets_dataset[filename_ix, -num_tokens:] = token_positions\n",
    "    token_offsets_dataset[filename_ix, :-num_tokens] = 0\n",
    "    class_labels_dataset[filename_ix, -num_tokens:] = make_one_hot(class_index, 8)\n",
    "    class_labels_dataset[filename_ix, :-num_tokens] = 0\n",
    "    num_tokens_dataset[filename_ix] = num_tokens\n",
    "    bi_labels_dataset[filename_ix, -num_tokens:] = bi\n",
    "    bi_labels_dataset[filename_ix, :-num_tokens] = 0\n",
    "    bio_labels_dataset[filename_ix, -num_tokens:] = bio\n",
    "    bio_labels_dataset[filename_ix, :-num_tokens] = 0\n",
    "    bies_labels_dataset[filename_ix, -num_tokens:] = bies\n",
    "    bies_labels_dataset[filename_ix, :-num_tokens] = 0\n",
    "    bieso_labels_dataset[filename_ix, -num_tokens:] = bieso\n",
    "    bieso_labels_dataset[filename_ix, :-num_tokens] = 0\n",
    "    cbi_labels_dataset[filename_ix, -num_tokens:] = make_one_hot(combined_bi, 16)\n",
    "    cbi_labels_dataset[filename_ix, :-num_tokens] = 0\n",
    "    cbio_labels_dataset[filename_ix, -num_tokens:] = make_one_hot(combined_bio, 15)\n",
    "    cbio_labels_dataset[filename_ix, :-num_tokens] = 0\n",
    "    cbies_labels_dataset[filename_ix, -num_tokens:] = make_one_hot(combined_bies, 32)\n",
    "    cbies_labels_dataset[filename_ix, :-num_tokens] = 0\n",
    "    cbieso_labels_dataset[filename_ix, -num_tokens:] = make_one_hot(combined_bieso, 29)\n",
    "    cbieso_labels_dataset[filename_ix, :-num_tokens] = 0\n",
    "                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8606bca8-6416-4baa-8d75-f951906d8c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e126777b-9525-46e6-a8dc-8e7895af6db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686f0ec-b56a-412c-8851-29c58c19a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "123"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbac403b-70cf-40b2-b9a7-080373dc05d1",
   "metadata": {},
   "source": [
    "nonchar_re = re.compile('[^A-Za-z0-9]')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4b88d8c-0df5-4e4c-8266-ddb4f3a7ede2",
   "metadata": {},
   "source": [
    "def fix_entities(ID, left=True):\n",
    "    text_data = data.loc[data.id==ID].copy()\n",
    "    with open(f'../../train/{ID}.txt') as f:\n",
    "        text = f.read().strip()\n",
    "    pointer = 0\n",
    "    for row_id, row in text_data.iterrows():\n",
    "        entity_text = row.discourse_text.strip()\n",
    "        entity_text = entity_text[next(regexp.finditer(entity_text)).start():]\n",
    "        if len(entity_text.split()[0]) == 1 and pointer != 0:\n",
    "            entity_start_ix = text[pointer:].index(entity_text)\n",
    "            prev_text = text[:pointer + entity_start_ix]\n",
    "            if pointer + entity_start_ix > 0 and prev_text[-1].isalpha():\n",
    "                if left:\n",
    "                    start_offset = - next(nonchar_re.finditer(prev_text[::-1])).start()\n",
    "                    entity_text = prev_text[start_offset:] + entity_text\n",
    "                else:\n",
    "                    start_offset = next(nonchar_re.finditer(entity_text)).start()\n",
    "                    entity_text = entity_text[start_offset:]\n",
    "                text_data.loc[text_data.discourse_id==row.discourse_id, 'discourse_text'] = entity_text\n",
    "                pointer += start_offset\n",
    "        if row.discourse_id in (1620147556527.0, 1622983056026.0):\n",
    "            pointer += 10\n",
    "        offset = text[pointer:].index(entity_text)\n",
    "        starts_at = offset + pointer\n",
    "        ent_boundaries.append((starts_at, starts_at + len(entity_text), row.discourse_type))\n",
    "        pointer = starts_at + len(entity_text)\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d46f0af-654f-453e-a3b3-17784aa1f9ee",
   "metadata": {},
   "source": [
    "ix = -1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2fe3dfb5-a28b-4c09-a886-7fe7686eb0d6",
   "metadata": {},
   "source": [
    "ix = 1\n",
    "ID = broken_indices[ix][1]\n",
    "show_sample(ID)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e7bfa32-b9dc-4dce-8ccf-b4b22e7e5b4b",
   "metadata": {},
   "source": [
    "fixed_df = fix_entities(ID)\n",
    "show_sample(ID, fixed_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df566fe2-04f3-40a1-b476-b37224a6e580",
   "metadata": {},
   "source": [
    "data.loc[data.id==ID] = fixed_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb155c2c-c227-4d37-9eb3-3cbca6453767",
   "metadata": {},
   "source": [
    "show_sample(ID)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a84758e-ac38-4682-8d1d-b194ca3c2a6d",
   "metadata": {},
   "source": [
    "data.to_csv('../../train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
