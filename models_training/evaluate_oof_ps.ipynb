{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40509f0c-3656-4ed9-90c2-f4c58cb6d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532957ef-56cd-4914-8d70-fdcab5eaf7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data_splits.pickle', 'rb') as f:\n",
    "    data_splits = pickle.load(f)\n",
    "val_bounds = np.cumsum([0] + [len(data_splits[0][250]['normed'][fold_ix]) for fold_ix in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5b6d37b-9b32-4a19-82fa-0782b75a4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(ps, n):\n",
    "    cat_ps = ps.argmax(-1)\n",
    "    all_entities = {}\n",
    "    current_cat = None\n",
    "    current_start = None\n",
    "    for ix in range(1, n - 1):\n",
    "        if cat_ps[ix] % 2 == 1:\n",
    "            if current_cat is not None:\n",
    "                if current_cat not in all_entities:\n",
    "                    all_entities[current_cat] = []\n",
    "                all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = (cat_ps[ix] + 1) // 2\n",
    "            current_start = ix\n",
    "        elif cat_ps[ix] == 0:\n",
    "            if current_cat is not None:\n",
    "                if current_cat not in all_entities:\n",
    "                    all_entities[current_cat] = []\n",
    "                all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = None\n",
    "    if current_cat is not None:\n",
    "        if current_cat not in all_entities:\n",
    "            all_entities[current_cat] = []\n",
    "        all_entities[current_cat].append((current_start, ix))\n",
    "    \n",
    "    for cat_ix, min_len in zip(range(1, 8), (2, 2, 5, 2, 4, 3, 2)):\n",
    "        if cat_ix in all_entities:\n",
    "            all_entities[cat_ix] = [x for x in all_entities[cat_ix] if x[1] - x[0] + 1 >= min_len]\n",
    "        \n",
    "    return all_entities\n",
    "\n",
    "def process_sample(raw_ps, index_map, bounds, gt_spans, num_tokens, match_stats, min_len=0):\n",
    "    \n",
    "    #bounds[num_tokens - 2, 1] = min(len(index_map) - 1, bounds[num_tokens - 2, 1])\n",
    "    predicted_spans = {x: [map_span_to_word_indices(span, index_map, bounds) for span in y] \n",
    "                       for x, y in extract_entities(raw_ps, num_tokens).items()}\n",
    "    predicted_spans = {x: [z for z in y if z[1] - z[0] >= min_len] for x, y in predicted_spans.items()}\n",
    "    \n",
    "    for cat_ix in range(1, 8):\n",
    "        \n",
    "        pspans = predicted_spans.get(cat_ix, [])\n",
    "        gspans = gt_spans.get(cat_ix, [])\n",
    "        if not len(pspans) or not len(gspans):\n",
    "            match_stats[cat_ix]['fn'] += len(gspans)\n",
    "            match_stats[cat_ix]['fp'] += len(pspans)\n",
    "        else:\n",
    "            all_overlaps = np.zeros((len(pspans), len(gspans)))\n",
    "            for x1 in range(len(pspans)):\n",
    "                pspan = pspans[x1]\n",
    "                for x2 in range(len(gspans)):\n",
    "                    gspan = gspans[x2]\n",
    "                    start_ix = max(pspan[0], gspan[0])\n",
    "                    end_ix = min(pspan[1], gspan[1])\n",
    "                    overlap = max(0, end_ix - start_ix + 1)\n",
    "                    if overlap > 0:\n",
    "                        o1 = overlap / (pspan[1] - pspan[0] + 1)\n",
    "                        o2 = overlap / (gspan[1] - gspan[0] + 1)\n",
    "                        if min(o1, o2) >= .5:\n",
    "                            all_overlaps[x1, x2] = max(o1, o2)\n",
    "            unused_p_ix = set(range(len(pspans)))\n",
    "            unused_g_ix = set(range(len(gspans)))\n",
    "            col_size = len(pspans)\n",
    "            row_size = len(gspans)\n",
    "            for ix in np.argsort(all_overlaps.ravel())[::-1]:\n",
    "                if not len(unused_g_ix) or not len(unused_p_ix) or all_overlaps.ravel()[ix] == 0:\n",
    "                    match_stats[cat_ix]['fp'] += len(unused_p_ix)\n",
    "                    match_stats[cat_ix]['fn'] += len(unused_g_ix)\n",
    "                    break\n",
    "                p_ix = ix // row_size\n",
    "                g_ix = ix % row_size\n",
    "                if p_ix not in unused_p_ix or g_ix not in unused_g_ix:\n",
    "                    continue\n",
    "                match_stats[cat_ix]['tp'] += 1\n",
    "                unused_g_ix.remove(g_ix)\n",
    "                unused_p_ix.remove(p_ix)\n",
    "    return match_stats\n",
    "\n",
    "def map_span_to_word_indices(span, index_map, bounds):\n",
    "    return (index_map[bounds[span[0], 0]], index_map[bounds[span[1], 1] - 1])\n",
    "\n",
    "def split_predstring(x):\n",
    "    vals = x.split()\n",
    "    return int(vals[0]), int(vals[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f225dfd-53f5-4a1a-9c09-bc3c5647383d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc653881-1cf1-4d6a-8298-32854fbe185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entity_score(span, ps, c):\n",
    "    s, e = span\n",
    "    score = (ps[s, c * 2 - 1] + ps[s + 1: e + 1, c * 2].sum())/(e - s + 1)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10da6df5-b278-491e-8a23-a7239dafe019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(ps, n):\n",
    "    cat_ps = ps.argmax(-1)\n",
    "    all_entities = {}\n",
    "    current_cat = None\n",
    "    current_start = None\n",
    "    for ix in range(1, n - 1):\n",
    "        if cat_ps[ix] % 2 == 1:\n",
    "            if current_cat is not None:\n",
    "                if current_cat not in all_entities:\n",
    "                    all_entities[current_cat] = []\n",
    "                all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = (cat_ps[ix] + 1) // 2\n",
    "            current_start = ix        \n",
    "        elif cat_ps[ix] == 0:\n",
    "            if current_cat is not None:\n",
    "                if current_cat not in all_entities:\n",
    "                    all_entities[current_cat] = []\n",
    "                all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = None\n",
    "        elif current_cat is not None and cat_ps[ix] != current_cat * 2:\n",
    "            if current_cat not in all_entities:\n",
    "                all_entities[current_cat] = []\n",
    "            all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = None\n",
    "    if current_cat is not None:\n",
    "        if current_cat not in all_entities:\n",
    "            all_entities[current_cat] = []\n",
    "        all_entities[current_cat].append((current_start, ix))\n",
    "\n",
    "    # score_thresholds = [None, -.9, -.56, -.55, -.65, -.56, -.76, -.68]\n",
    "    score_thresholds = [None, -5, -3.4, -2.065, -2.5, -6, -5.75, -5.5]\n",
    "    \n",
    "    for cat_ix, min_len in zip(range(1, 8), (2, 2, 5, 2, 4, 3, 2)):\n",
    "        if cat_ix in all_entities:\n",
    "            all_entities[cat_ix] = [x for x in all_entities[cat_ix] if x[1] - x[0] + 1 >= min_len and calc_entity_score(x, ps, cat_ix) > score_thresholds[cat_ix]]\n",
    "            \n",
    "        \n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d19b5869-4002-4c51-9907-189d44bba799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gt_dict(df):\n",
    "    gt_dict = {}\n",
    "    for cat_ix in range(1, 8):\n",
    "        cat_name = label_names[cat_ix]\n",
    "        cat_entities = df.loc[df.discourse_type==cat_name]\n",
    "        if len(cat_entities):\n",
    "            gt_dict[cat_ix] = [(x[0], x[1]) for x in cat_entities.predictionstring.map(split_predstring)]\n",
    "    return gt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f8297b4-1951-4269-be56-243945e38065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1s(stats):\n",
    "    f1s = np.zeros(8)\n",
    "    rec = np.zeros(7)\n",
    "    prec = np.zeros(7)\n",
    "    for ix in range(1, 8):\n",
    "        f1s[ix] = stats[ix]['tp'] / (1e-7 + stats[ix]['tp'] + .5 * (stats[ix]['fp'] + stats[ix]['fn']))\n",
    "        rec[ix - 1] = stats[ix]['tp'] / (1e-7 + stats[ix]['tp'] + stats[ix]['fn'])\n",
    "        prec[ix - 1] = stats[ix]['tp'] / (1e-7 + stats[ix]['tp'] + stats[ix]['fp'])\n",
    "    f1s[0] = np.mean(f1s[1:])\n",
    "    return f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e38e2eb-65b6-4cd3-b9cf-a3ee132d9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('../train.csv')\n",
    "label_names = ['None', 'Lead', 'Position', 'Evidence', 'Claim',\n",
    "               'Concluding Statement', 'Counterclaim', 'Rebuttal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bb6dd64-fb30-4944-850d-acd2148dd202",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../gt_dicts.pickle', 'rb') as f:\n",
    "    gt_dicts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f59b5214-81d7-445a-a825-ee63135bdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be755e2c-548e-459a-8e14-3ff2ce7f2931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(list_of_dir_names):\n",
    "    print(f'evaluating {list_of_dir_names}')\n",
    "    arrays = []\n",
    "    oof_dir_name = list_of_dir_names[0]\n",
    "    \n",
    "    for array_name in 'all_outs, all_bounds, all_token_nums, all_word_indices, all_sample_ids'.split(', '):\n",
    "        arrays.append(np.load(f'oof_ps/{oof_dir_name}/{array_name}.npy', allow_pickle=True))\n",
    "    all_outs, all_bounds, all_token_nums, all_word_indices, all_sample_ids = arrays\n",
    "    \n",
    "    for oof_dir_name in list_of_dir_names[1:]:\n",
    "        all_outs += np.load(f'oof_ps/{oof_dir_name}/all_outs.npy', allow_pickle=True)\n",
    "\n",
    "    for fold_ix in range(5):\n",
    "        start_ix = val_bounds[fold_ix]\n",
    "        end_ix = val_bounds[fold_ix + 1]\n",
    "        fold_stats = {ix:  {'fp': 0, 'fn': 0, 'tp': 0} for ix in range(1, 8)}\n",
    "        fold_ps = all_outs[start_ix:end_ix]\n",
    "        fold_bounds = all_bounds[start_ix:end_ix]\n",
    "        fold_token_nums = all_token_nums[start_ix:end_ix]\n",
    "        fold_word_indices = all_word_indices[start_ix:end_ix]\n",
    "        fold_sample_ids = all_sample_ids[start_ix:end_ix]\n",
    "        for sample_ix in range(len(fold_ps)):\n",
    "            fold_stats = process_sample(fold_ps[sample_ix], fold_word_indices[sample_ix], \n",
    "                                        fold_bounds[sample_ix],\n",
    "                                        gt_dicts[fold_sample_ids[sample_ix]],\n",
    "                                        fold_token_nums[sample_ix],\n",
    "                                       fold_stats)\n",
    "        print(f'fold {fold_ix} f1s: {f1s(fold_stats)}')\n",
    "    \n",
    "    for sample_ix in range(val_bounds[-1]):\n",
    "        fold_stats = process_sample(all_outs[sample_ix], all_word_indices[sample_ix], \n",
    "                                    all_bounds[sample_ix],\n",
    "                                    gt_dicts[all_sample_ids[sample_ix]],\n",
    "                                    all_token_nums[sample_ix],\n",
    "                                   fold_stats)\n",
    "    print(f'CV f1s: {f1s(fold_stats)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ebe9a-be60-43e9-a71c-ed07b46e7621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4702a84f-88d8-42d6-ad92-456e99d079ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ['debertav1_rce01']\n",
      "fold 0 f1s: [0.68948203 0.83719705 0.70455643 0.7406348  0.65539329 0.85863686\n",
      " 0.5621446  0.46781116]\n",
      "fold 1 f1s: [0.68838468 0.82464455 0.71379815 0.73202322 0.64304235 0.85083075\n",
      " 0.57084856 0.48350515]\n",
      "fold 2 f1s: [0.69156138 0.82933263 0.71834625 0.73452787 0.65588681 0.84813127\n",
      " 0.57349398 0.48121086]\n",
      "fold 3 f1s: [0.68134482 0.82512643 0.70401829 0.738637   0.65273914 0.85127453\n",
      " 0.5434606  0.45415778]\n",
      "fold 4 f1s: [0.69342359 0.83226837 0.72140762 0.73610489 0.6591209  0.85425101\n",
      " 0.56492411 0.48588821]\n",
      "CV f1s: [0.6895975  0.83014037 0.7139146  0.73634442 0.6542189  0.85290444\n",
      " 0.56329936 0.47636039]\n"
     ]
    }
   ],
   "source": [
    "evaluate(['debertav1_rce01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70165019-5731-4337-980d-ddf1fbe8c61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating ['debertav1_rce01', 'debertav1_norce_mnli', 'longformer']\n",
      "fold 0 f1s: [0.69677957 0.83508403 0.71330724 0.75843333 0.66666667 0.85514019\n",
      " 0.56665238 0.48217317]\n",
      "fold 1 f1s: [0.6976502  0.82411796 0.72124113 0.75004301 0.65961901 0.8537607\n",
      " 0.58078053 0.49398907]\n",
      "fold 2 f1s: [0.70187565 0.83753943 0.72438863 0.75313241 0.67021777 0.85516739\n",
      " 0.5864094  0.48627451]\n",
      "fold 3 f1s: [0.69346296 0.83466667 0.71780175 0.7560636  0.66850917 0.85866961\n",
      " 0.55795848 0.46057143]\n",
      "fold 4 f1s: [0.70347084 0.84249867 0.73143996 0.75455756 0.67549738 0.85252525\n",
      " 0.56514085 0.5026362 ]\n",
      "CV f1s: [0.69945575 0.83604097 0.72325151 0.75446629 0.66933601 0.85463064\n",
      " 0.57050731 0.48795752]\n"
     ]
    }
   ],
   "source": [
    "evaluate(['debertav1_rce01', 'debertav1_norce_mnli', 'longformer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246888b7-699e-4b05-86c8-7ba3650ecb75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4bb66c-5fa5-48b9-b842-00b104b96b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
