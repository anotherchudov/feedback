{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40509f0c-3656-4ed9-90c2-f4c58cb6d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532957ef-56cd-4914-8d70-fdcab5eaf7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data_splits.pickle', 'rb') as f:\n",
    "    data_splits = pickle.load(f)\n",
    "val_bounds = np.cumsum([0] + [len(data_splits[0][250]['normed'][fold_ix]) for fold_ix in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b6d37b-9b32-4a19-82fa-0782b75a4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(raw_ps, index_map, bounds, gt_spans, num_tokens, match_stats, ):\n",
    "    \n",
    "    predicted_spans = {x: [map_span_to_word_indices(span, index_map, bounds) for span in y] \n",
    "                       for x, y in extract_entities(raw_ps, num_tokens).items()}\n",
    "    num_entities = np.zeros(7)\n",
    "    for cat_ix in range(1, 8):\n",
    "        \n",
    "        pspans = predicted_spans.get(cat_ix, [])\n",
    "        gspans = gt_spans.get(cat_ix, [])\n",
    "        num_entities[cat_ix - 1] = len(pspans)\n",
    "        if not len(pspans) or not len(gspans):\n",
    "            match_stats[cat_ix]['fn'] += len(gspans)\n",
    "            match_stats[cat_ix]['fp'] += len(pspans)\n",
    "        else:\n",
    "            all_overlaps = np.zeros((len(pspans), len(gspans)))\n",
    "            for x1 in range(len(pspans)):\n",
    "                pspan = pspans[x1]\n",
    "                for x2 in range(len(gspans)):\n",
    "                    gspan = gspans[x2]\n",
    "                    start_ix = max(pspan[0], gspan[0])\n",
    "                    end_ix = min(pspan[1], gspan[1])\n",
    "                    overlap = max(0, end_ix - start_ix + 1)\n",
    "                    if overlap > 0:\n",
    "                        o1 = overlap / (pspan[1] - pspan[0] + 1)\n",
    "                        o2 = overlap / (gspan[1] - gspan[0] + 1)\n",
    "                        if min(o1, o2) >= .5:\n",
    "                            all_overlaps[x1, x2] = max(o1, o2)\n",
    "            unused_p_ix = set(range(len(pspans)))\n",
    "            unused_g_ix = set(range(len(gspans)))\n",
    "            col_size = len(pspans)\n",
    "            row_size = len(gspans)\n",
    "            for ix in np.argsort(all_overlaps.ravel())[::-1]:\n",
    "                if not len(unused_g_ix) or not len(unused_p_ix) or all_overlaps.ravel()[ix] == 0:\n",
    "                    match_stats[cat_ix]['fp'] += len(unused_p_ix)\n",
    "                    match_stats[cat_ix]['fn'] += len(unused_g_ix)\n",
    "                    break\n",
    "                p_ix = ix // row_size\n",
    "                g_ix = ix % row_size\n",
    "                if p_ix not in unused_p_ix or g_ix not in unused_g_ix:\n",
    "                    continue\n",
    "                match_stats[cat_ix]['tp'] += 1\n",
    "                unused_g_ix.remove(g_ix)\n",
    "                unused_p_ix.remove(p_ix)\n",
    "    return match_stats, num_entities\n",
    "\n",
    "def map_span_to_word_indices(span, index_map, bounds):\n",
    "    return (index_map[bounds[span[0], 0]], index_map[bounds[span[1], 1] - 1])\n",
    "\n",
    "def split_predstring(x):\n",
    "    vals = x.split()\n",
    "    return int(vals[0]), int(vals[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc653881-1cf1-4d6a-8298-32854fbe185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entity_score(span, ps, c):\n",
    "    s, e = span\n",
    "    score = (ps[s, c * 2 - 1] + ps[s + 1: e + 1, c * 2].sum())/(e - s + 1)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "10da6df5-b278-491e-8a23-a7239dafe019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(ps, n):\n",
    "    cat_ps = ps.argmax(-1)\n",
    "    all_entities = {}\n",
    "    current_cat = None\n",
    "    current_start = None\n",
    "    seq_len = ps.shape[0]\n",
    "    for ix in range(n):\n",
    "    # for ix in range(seq_len - n, seq_len - 2):\n",
    "        if cat_ps[ix] % 2 == 1:\n",
    "            if current_cat is not None:\n",
    "                if current_cat not in all_entities:\n",
    "                    all_entities[current_cat] = []\n",
    "                all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = (cat_ps[ix] + 1) // 2\n",
    "            current_start = ix        \n",
    "        elif cat_ps[ix] == 0:\n",
    "            if current_cat is not None:\n",
    "                if current_cat not in all_entities:\n",
    "                    all_entities[current_cat] = []\n",
    "                all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = None\n",
    "        elif current_cat is not None and cat_ps[ix] != current_cat * 2:\n",
    "            if current_cat not in all_entities:\n",
    "                all_entities[current_cat] = []\n",
    "            all_entities[current_cat].append((current_start, ix - 1))\n",
    "            current_cat = None\n",
    "    if current_cat is not None:\n",
    "        if current_cat not in all_entities:\n",
    "            all_entities[current_cat] = []\n",
    "        all_entities[current_cat].append((current_start, ix))\n",
    "\n",
    "    score_thresholds = [None, -.8, -.48, -.49, -.65, -.47, -.7, -.68]\n",
    "    \n",
    "    score_thresholds = [None, -.8, -.5, -.515, -.635, -.535, -.82, -.88]\n",
    "    score_thresholds = [None, -2.1, -1.15, -1.2, -1.5, -1.36, -3, -3]\n",
    "    \n",
    "    # length_thresholds = (6, 3, 16, 2, 8, 7, 5)\n",
    "    # score_thresholds = [None, -5, -3.4, -2.065, -2.5, -6, -5.75, -5.5]\n",
    "    xlnet_score_thresholds = [None, -.9, -.547, -.537, -.685, -.53, -.8, -.77]\n",
    "    xlnet_length_thresholds = (5, 2, 17, 2, 5, 7, 5)\n",
    "    debertav1new_score_thresholds = [None, -1., -.565, -.545, -.615, -.56, -1.05, -1.1]\n",
    "    debertav1new_length_thresholds = (6, 3, 18, 2, 7, 6, 7)\n",
    "    debertav1x2_score_thresholds = [None, -1.71, -1.3, -1.15, -1.35, -1.5, -3.5, -3.5]\n",
    "    debertav1x2_len_thresholds = (3, 3, 17, 2, 6, 7, 7)\n",
    "    debertav1old_score_thresholds = [None, -.8, -.5, -.515, -.635, -.535, -.82, -.88]\n",
    "    debertav1old_len_thresholds = (6, 3, 16, 2, 8, 7, 5)\n",
    "    \n",
    "    \n",
    "    length_thresholds = (6, 5, 17, 2, 7, 7, 5)\n",
    "    for cat_ix, min_len in zip(range(1, 8), debertav1old_len_thresholds):\n",
    "        if cat_ix in all_entities:\n",
    "            possible_entities = [x for x in all_entities[cat_ix] if x[1] - x[0] + 1 >= min_len\n",
    "                                      and calc_entity_score(x, ps, cat_ix) > debertav1old_score_thresholds[cat_ix]]\n",
    "            if cat_ix in (1, 2,  5):\n",
    "                if len(possible_entities) > 1:\n",
    "                    max_score = -9999\n",
    "                    for x in possible_entities:\n",
    "                        entity_score = calc_entity_score(x, ps, cat_ix)\n",
    "                        if entity_score > max_score:\n",
    "                            max_score = entity_score\n",
    "                            biggest_entity = x\n",
    "                    possible_entities = [biggest_entity]\n",
    "            all_entities[cat_ix] = possible_entities\n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d19b5869-4002-4c51-9907-189d44bba799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gt_dict(df):\n",
    "    gt_dict = {}\n",
    "    for cat_ix in range(1, 8):\n",
    "        cat_name = label_names[cat_ix]\n",
    "        cat_entities = df.loc[df.discourse_type==cat_name]\n",
    "        if len(cat_entities):\n",
    "            gt_dict[cat_ix] = [(x[0], x[1]) for x in cat_entities.predictionstring.map(split_predstring)]\n",
    "    return gt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f8297b4-1951-4269-be56-243945e38065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1s(stats):\n",
    "    f1s = np.zeros(8)\n",
    "    rec = np.zeros(7)\n",
    "    prec = np.zeros(7)\n",
    "    for ix in range(1, 8):\n",
    "        f1s[ix] = stats[ix]['tp'] / (1e-7 + stats[ix]['tp'] + .5 * (stats[ix]['fp'] + stats[ix]['fn']))\n",
    "        rec[ix - 1] = stats[ix]['tp'] / (1e-7 + stats[ix]['tp'] + stats[ix]['fn'])\n",
    "        prec[ix - 1] = stats[ix]['tp'] / (1e-7 + stats[ix]['tp'] + stats[ix]['fp'])\n",
    "    f1s[0] = np.mean(f1s[1:])\n",
    "    return f1s, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5e38e2eb-65b6-4cd3-b9cf-a3ee132d9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('../train.csv')\n",
    "label_names = ['None', 'Lead', 'Position', 'Evidence', 'Claim',\n",
    "               'Concluding Statement', 'Counterclaim', 'Rebuttal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9bb6dd64-fb30-4944-850d-acd2148dd202",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../gt_dicts.pickle', 'rb') as f:\n",
    "    gt_dicts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbf7d87-2004-4b25-87ca-ffc8fbdb2656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2636e-594c-48d1-ba7d-53986f77c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ps(logits_a, logits_b, bounds_a, bounds_b):\n",
    "    a = bounds_a\n",
    "    b = bounds_b\n",
    "    mapping_a = []\n",
    "    mapping_b = []\n",
    "    new_bounds = []\n",
    "    apos = 0\n",
    "    bpos = 0\n",
    "    a_s, a_e = a[apos]\n",
    "    b_s, b_e = b[bpos]\n",
    "    current_start = 0\n",
    "    while True:\n",
    "        if a_e == b_e:\n",
    "            if apos == len(a) - 1 or bpos == len(b) - 1:\n",
    "                break\n",
    "            new_bounds.append((current_start, b_e))\n",
    "            mapping_a.append(apos)\n",
    "            mapping_b.append(bpos)\n",
    "            next_a_s, next_b_s = a[apos + 1][0], b[bpos + 1][0]\n",
    "            if next_a_s < next_b_s:\n",
    "                apos += 1\n",
    "                a_s, a_e = a[apos]\n",
    "                current_start = a_s\n",
    "            elif next_b_s < next_a_s:\n",
    "                bpos += 1\n",
    "                b_s, b_e = b[bpos]\n",
    "                current_start = b_s\n",
    "            else:\n",
    "                apos += 1\n",
    "                bpos += 1\n",
    "                a_s, a_e = a[apos]\n",
    "                b_s, b_e = b[bpos]\n",
    "                current_start = a_s\n",
    "        elif a_e < b_e:\n",
    "            new_bounds.append((current_start, a_e))\n",
    "            mapping_a.append(apos)\n",
    "            mapping_b.append(bpos)\n",
    "            apos += 1\n",
    "            a_s, a_e = a[apos]\n",
    "            current_start = a_s\n",
    "        else:\n",
    "            new_bounds.append((current_start, b_e))\n",
    "            mapping_a.append(apos)\n",
    "            mapping_b.append(bpos)\n",
    "            bpos += 1\n",
    "            b_s, b_e = b[bpos]\n",
    "            current_start = b_s\n",
    "    \n",
    "    return logits_a[mapping_a] + logits_b[mapping_b], np.array(new_bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be755e2c-548e-459a-8e14-3ff2ce7f2931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(list_of_dir_names):\n",
    "    print(f'evaluating {list_of_dir_names}')\n",
    "    arrays = []\n",
    "    oof_dir_name = list_of_dir_names[0]\n",
    "    \n",
    "    for array_name in 'all_outs, all_bounds, all_token_nums, all_word_indices, all_sample_ids'.split(', '):\n",
    "        arrays.append(np.load(f'oof_ps/{oof_dir_name}/{array_name}.npy', allow_pickle=True))\n",
    "    all_outs, all_bounds, all_token_nums, all_word_indices, all_sample_ids = arrays\n",
    "    # all_outs += np.load(f'oof_ps/{list_of_dir_names[1]}/all_outs.npy', allow_pickle=True)\n",
    "    \n",
    "#     for oof_dir_name in list_of_dir_names[1:]:\n",
    "#         arrays = []\n",
    "#         for array_name in 'all_outs, all_bounds, all_token_nums'.split(', '):\n",
    "#             arrays.append(np.load(f'oof_ps/{oof_dir_name}/{array_name}.npy', allow_pickle=True))\n",
    "#         new_outs, new_bounds, new_token_nums = arrays\n",
    "#         merged_outs = []\n",
    "#         merged_bounds = []\n",
    "#         merged_token_nums = []\n",
    "#         for sample_ix in range(len(new_outs)):\n",
    "#             logits, bounds = merge_ps(all_outs[sample_ix][:all_token_nums[sample_ix]],\n",
    "#                                       new_outs[sample_ix][:new_token_nums[sample_ix]],\n",
    "#                                       all_bounds[sample_ix][:all_token_nums[sample_ix]],\n",
    "#                                       new_bounds[sample_ix][:new_token_nums[sample_ix]])\n",
    "#             merged_outs.append(logits)\n",
    "#             merged_bounds.append(bounds)\n",
    "#             merged_token_nums.append(len(logits))\n",
    "#         all_outs = merged_outs\n",
    "#         all_bounds = merged_bounds\n",
    "#         all_token_nums = merged_token_nums\n",
    "        \n",
    "    all_num_entities = np.zeros((len(all_outs), 7))\n",
    "    text_ix = 0\n",
    "    for fold_ix in range(5):\n",
    "        start_ix = val_bounds[fold_ix]\n",
    "        end_ix = val_bounds[fold_ix + 1]\n",
    "        fold_stats = {ix:  {'fp': 0, 'fn': 0, 'tp': 0} for ix in range(1, 8)}\n",
    "        fold_ps = all_outs[start_ix:end_ix]\n",
    "        fold_bounds = all_bounds[start_ix:end_ix]\n",
    "        fold_token_nums = all_token_nums[start_ix:end_ix]\n",
    "        fold_word_indices = all_word_indices[start_ix:end_ix]\n",
    "        fold_sample_ids = all_sample_ids[start_ix:end_ix]\n",
    "        for sample_ix in range(len(fold_ps)):\n",
    "            fold_stats, num_entities = process_sample(fold_ps[sample_ix], fold_word_indices[sample_ix], \n",
    "                                        fold_bounds[sample_ix],\n",
    "                                        gt_dicts[fold_sample_ids[sample_ix]],\n",
    "                                        fold_token_nums[sample_ix],\n",
    "                                       fold_stats)\n",
    "            all_num_entities[text_ix] = num_entities\n",
    "            text_ix += 1\n",
    "            \n",
    "        fs, prec, rec = f1s(fold_stats)\n",
    "        print(f'fold {fold_ix}\\nf1s: {fs}\\nprec: {prec}\\nrec: {rec}')\n",
    "        \n",
    "    fold_stats = {ix:  {'fp': 0, 'fn': 0, 'tp': 0} for ix in range(1, 8)}\n",
    "    for sample_ix in range(val_bounds[-1]):\n",
    "        fold_stats, _ = process_sample(all_outs[sample_ix], all_word_indices[sample_ix], \n",
    "                                    all_bounds[sample_ix],\n",
    "                                    gt_dicts[all_sample_ids[sample_ix]],\n",
    "                                    all_token_nums[sample_ix],\n",
    "                                   fold_stats)\n",
    "    fs, prec, rec = f1s(fold_stats)\n",
    "    print(f'Total\\nf1s: {fs}\\nprec: {prec}\\nrec: {rec}')\n",
    "    return all_num_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8637ba2d-3432-41b9-90b3-1e2ea9766431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205bd825-4dc8-4d52-8718-1b3bf9e4438a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4702a84f-88d8-42d6-ad92-456e99d079ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate(['debertav1_rce01']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b1d16f-898c-4c73-a3b9-0b6636218600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
