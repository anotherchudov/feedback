{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e8d8b3-c834-4635-933c-e4f3e78dc03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5315852-2729-482b-8c3b-f12ae5862e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_combination(list_of_models):\n",
    "\n",
    "    with open('data_splits.pickle', 'rb') as f:\n",
    "        data_splits = pickle.load(f)\n",
    "    val_bounds = np.cumsum([0] + [len(data_splits[0][250]['normed'][fold_ix]) for fold_ix in range(5)])\n",
    "    val_ids = [x for fold_ix in range(5) for x in data_splits[0][250]['normed'][fold_ix]]\n",
    "    val0_ids = val_ids[::2]\n",
    "    val1_ids = val_ids[1::2]\n",
    "\n",
    "    def extract_entities(ps, n):\n",
    "        START_WITH_I = True\n",
    "        LOOK_AHEAD = True\n",
    "        max_ps = ps.max(-1)\n",
    "\n",
    "        ps = ps.argsort(-1)[...,::-1]\n",
    "        # argmax\n",
    "        cat_ps = ps[:, 0]\n",
    "        # argmax2\n",
    "        cat_ps2 = ps[:, 1]\n",
    "\n",
    "        all_entities = {}\n",
    "        new_entity = True\n",
    "        current_cat = current_start = current_end = None\n",
    "\n",
    "        # except for special tokens\n",
    "        for ix in range(n):\n",
    "\n",
    "            # logic on new entity\n",
    "            if new_entity:\n",
    "                # Background - ignore\n",
    "                if cat_ps[ix] == 0:\n",
    "                    pass\n",
    "\n",
    "                # B-LABEL(1,3,5,7,...) - start entity\n",
    "                elif cat_ps[ix] % 2 == 1:\n",
    "                    current_cat = (cat_ps[ix] + 1) // 2\n",
    "                    current_start = current_end = ix\n",
    "                    new_entity = False\n",
    "\n",
    "                    if current_cat in [6, 7]:\n",
    "                        LOOK_AHEAD = False\n",
    "                    else:\n",
    "                        LOOK_AHEAD = True\n",
    "\n",
    "                # I-LABEL(2,4,6,8,...) - conditional start\n",
    "                elif cat_ps[ix] % 2 == 0:\n",
    "                    if START_WITH_I:\n",
    "                        # Condition: I-LABEL in argmax with B-LABEL in argmax2\n",
    "                        if cat_ps[ix] == (cat_ps2[ix]+1):\n",
    "                            current_cat = cat_ps[ix] // 2\n",
    "                            current_start = current_end = ix\n",
    "                            new_entity = False\n",
    "\n",
    "                            if current_cat in [6, 7]:\n",
    "                                LOOK_AHEAD = False\n",
    "                            else:\n",
    "                                LOOK_AHEAD = True\n",
    "\n",
    "            # logic on ongoing entity\n",
    "            else:\n",
    "                # Background - save current entity and init current\n",
    "                if cat_ps[ix] == 0:\n",
    "                    if LOOK_AHEAD:\n",
    "                        if ix < n - 1 and (cat_ps[ix+1] == current_cat*2) and (cat_ps2[ix] == current_cat*2):\n",
    "                            current_end = ix\n",
    "                        else:\n",
    "                            # update current\n",
    "                            if current_cat not in all_entities:\n",
    "                                all_entities[current_cat] = []\n",
    "                            all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "                            # init current for new start\n",
    "                            new_entity = True\n",
    "                            current_cat = current_start = current_end = None\n",
    "\n",
    "                    else:\n",
    "                        # update current\n",
    "                        if current_cat not in all_entities:\n",
    "                            all_entities[current_cat] = []\n",
    "                        all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "                        # init current for new start\n",
    "                        new_entity = True\n",
    "                        current_cat = current_start = current_end = None\n",
    "\n",
    "                # B-LABEL(1,3,5,7,...) - save current entity and start new\n",
    "                elif cat_ps[ix] % 2 == 1:\n",
    "                    if cat_ps[ix] == (current_cat*2-1):\n",
    "                        # update current\n",
    "                        if current_cat not in all_entities:\n",
    "                            all_entities[current_cat] = []\n",
    "                        all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "                        # start new current\n",
    "                        current_cat = (cat_ps[ix] + 1) // 2\n",
    "                        current_start = current_end = ix\n",
    "                        new_entity = False\n",
    "\n",
    "                        if current_cat in [6, 7]:\n",
    "                            LOOK_AHEAD = False\n",
    "                        else:\n",
    "                            LOOK_AHEAD = True\n",
    "\n",
    "                    else:\n",
    "                        if LOOK_AHEAD:\n",
    "                            if ix < n - 1 and (cat_ps[ix+1] == current_cat*2) and (cat_ps2[ix] == current_cat*2):\n",
    "                                current_end = ix\n",
    "                            else:\n",
    "                                # update current\n",
    "                                if current_cat not in all_entities:\n",
    "                                    all_entities[current_cat] = []\n",
    "                                all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "                                # start new current\n",
    "                                current_cat = (cat_ps[ix] + 1) // 2\n",
    "                                current_start = current_end = ix\n",
    "                                new_entity = False\n",
    "\n",
    "                                if current_cat in [6, 7]:\n",
    "                                    LOOK_AHEAD = False\n",
    "                                else:\n",
    "                                    LOOK_AHEAD = True\n",
    "\n",
    "                        else:\n",
    "                            # update current\n",
    "                            if current_cat not in all_entities:\n",
    "                                all_entities[current_cat] = []\n",
    "                            all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "                            # start new current\n",
    "                            current_cat = (cat_ps[ix] + 1) // 2\n",
    "                            current_start = current_end = ix\n",
    "                            new_entity = False\n",
    "\n",
    "                            if current_cat in [6, 7]:\n",
    "                                LOOK_AHEAD = False\n",
    "                            else:\n",
    "                                LOOK_AHEAD = True\n",
    "\n",
    "                # I-LABEL(2,4,6,8,...) - conditional continue\n",
    "                elif cat_ps[ix] % 2 == 0:\n",
    "                    # B-LABEL0, I-LABEL0 - continue\n",
    "                    if cat_ps[ix] == current_cat*2:\n",
    "                        current_end = ix\n",
    "                    # B-LBAEL0, I-LABEL1 - conditional finish current entity\n",
    "                    else:\n",
    "                        if LOOK_AHEAD:\n",
    "                            if ix < n - 1 and (cat_ps[ix+1] == current_cat*2) and (cat_ps2[ix] == current_cat*2):\n",
    "                                current_end = ix\n",
    "                            else:\n",
    "                                # update current\n",
    "                                if current_cat not in all_entities:\n",
    "                                    all_entities[current_cat] = []\n",
    "                                all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "                                # init current\n",
    "                                new_entity = True\n",
    "                                current_cat = current_start = current_end = None\n",
    "                        else:\n",
    "                            # update current\n",
    "                            if current_cat not in all_entities:\n",
    "                                all_entities[current_cat] = []\n",
    "                            all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "                            # init current\n",
    "                            new_entity = True\n",
    "                            current_cat = current_start = current_end = None\n",
    "\n",
    "        # last entity\n",
    "        if not new_entity:\n",
    "            # update current\n",
    "            if current_cat not in all_entities:\n",
    "                all_entities[current_cat] = []\n",
    "            all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "        return all_entities\n",
    "\n",
    "    def filter_ps(all_entities, ps, sample_index_map, sample_bounds):\n",
    "        nonlocal word_len_filters\n",
    "        nonlocal token_len_filters\n",
    "        nonlocal score_filters\n",
    "        nonlocal len_exp\n",
    "        word_len_filters = [0] * 8\n",
    "\n",
    "        new_entities = {}\n",
    "\n",
    "        for cat_ix, min_num_tokens, min_num_words, min_score in zip(range(1, 8), token_len_filters, word_len_filters, score_filters):\n",
    "\n",
    "            if cat_ix in all_entities:\n",
    "                possible_entities = [x for x \n",
    "                                     in all_entities[cat_ix] \n",
    "                                     if x[2] >= min_num_tokens\n",
    "                                     and x[1] > min_score]\n",
    "\n",
    "                if cat_ix in (1, 2, 5):\n",
    "                    if len(possible_entities) > 1:\n",
    "                        max_score = -9999\n",
    "                        for x in possible_entities:\n",
    "                            entity_score = x[1]\n",
    "                            if entity_score > max_score:\n",
    "                                max_score = entity_score\n",
    "                                biggest_entity = x\n",
    "                        possible_entities = [biggest_entity]\n",
    "\n",
    "                new_entities[cat_ix] = possible_entities\n",
    "\n",
    "        return new_entities\n",
    "\n",
    "\n",
    "    def first_token_merge(gather_a, gather_b, bounds_a, bounds_b, logits_a, logits_b, new_bounds):\n",
    "        new_gather_a = [0]\n",
    "        new_gather_b = [0]\n",
    "        prev_same_a = False\n",
    "        prev_same_b = False\n",
    "        for ix in range(1, len(gather_a)):\n",
    "            same_a = gather_a[ix] == gather_a[ix - 1]\n",
    "            same_b = gather_b[ix] == gather_b[ix - 1]\n",
    "            if same_a:\n",
    "                assert not same_b\n",
    "                if prev_same_b and not prev_same_a and new_gather_a[-1] != gather_a[ix]:\n",
    "                    new_gather_a.append(gather_a[ix])\n",
    "                    new_gather_b.append(gather_b[ix])\n",
    "                else:\n",
    "                    prev_same_a = same_a\n",
    "                    prev_same_b = same_b\n",
    "                    continue\n",
    "            elif same_b:\n",
    "                if prev_same_a and not prev_same_b and new_gather_b[-1] != gather_b[ix]:\n",
    "                    new_gather_a.append(gather_a[ix])\n",
    "                    new_gather_b.append(gather_b[ix])\n",
    "                else:\n",
    "                    prev_same_a = same_a\n",
    "                    prev_same_b = same_b\n",
    "                    continue\n",
    "            else:\n",
    "                new_gather_a.append(gather_a[ix])\n",
    "                new_gather_b.append(gather_b[ix])\n",
    "            prev_same_a = same_a\n",
    "            prev_same_b = same_b\n",
    "        new_bounds = []\n",
    "        for index_tokens_a, index_tokens_b in zip(new_gather_a, new_gather_b):\n",
    "            new_bounds.append((min(bounds_a[index_tokens_a, 0],\n",
    "                                   bounds_b[index_tokens_b, 0]), \n",
    "                               max(bounds_a[index_tokens_a, 1],\n",
    "                                   bounds_b[index_tokens_b, 1])))\n",
    "        new_logits = logits_a[new_gather_a] + logits_b[new_gather_b]\n",
    "        return new_logits, np.array(new_bounds)\n",
    "\n",
    "    def merge_ps(logits_a, logits_b, bounds_a, bounds_b):\n",
    "        a = bounds_a\n",
    "        b = bounds_b\n",
    "        mapping_a = []\n",
    "        mapping_b = []\n",
    "        new_bounds = []\n",
    "        apos = 0\n",
    "        bpos = 0\n",
    "        a_s, a_e = a[apos]\n",
    "        b_s, b_e = b[bpos]\n",
    "        current_start = 0\n",
    "        while True:\n",
    "            if a_e == b_e:\n",
    "                new_bounds.append((current_start, b_e))\n",
    "                mapping_a.append(apos)\n",
    "                mapping_b.append(bpos)\n",
    "                if apos == len(a) - 1 or bpos == len(b) - 1:\n",
    "                    break\n",
    "                next_a_s, next_b_s = a[apos + 1][0], b[bpos + 1][0]\n",
    "                if next_a_s < next_b_s:\n",
    "                    apos += 1\n",
    "                    a_s, a_e = a[apos]\n",
    "                    current_start = a_s\n",
    "                elif next_b_s < next_a_s:\n",
    "                    bpos += 1\n",
    "                    b_s, b_e = b[bpos]\n",
    "                    current_start = b_s\n",
    "                else:\n",
    "                    apos += 1\n",
    "                    bpos += 1\n",
    "                    a_s, a_e = a[apos]\n",
    "                    b_s, b_e = b[bpos]\n",
    "                    current_start = a_s\n",
    "            elif a_e < b_e:\n",
    "                new_bounds.append((current_start, a_e))\n",
    "                mapping_a.append(apos)\n",
    "                mapping_b.append(bpos)\n",
    "                apos += 1\n",
    "                a_s, a_e = a[apos]\n",
    "                current_start = a_s\n",
    "            else:\n",
    "                new_bounds.append((current_start, b_e))\n",
    "                mapping_a.append(apos)\n",
    "                mapping_b.append(bpos)\n",
    "                bpos += 1\n",
    "                b_s, b_e = b[bpos]\n",
    "                current_start = b_s\n",
    "\n",
    "        return first_token_merge(mapping_a, mapping_b, bounds_a, bounds_b, logits_a, logits_b, new_bounds)\n",
    "\n",
    "\n",
    "\n",
    "    def process_sample(raw_ps, unfiltered_entities, index_map, bounds, gt_spans, num_tokens, match_stats):\n",
    "\n",
    "        filtered_entities = filter_ps(unfiltered_entities, raw_ps, index_map, bounds)\n",
    "        predicted_spans = {x: [map_span_to_word_indices(extend_tokens(span, num_tokens), index_map, bounds) for span, score, span_len in y] \n",
    "                           for x, y in filtered_entities.items()}\n",
    "        token_spans = predicted_spans\n",
    "        # token_spans = {x: y[0] for x, y in filtered_entities}\n",
    "\n",
    "        false_positives = {x: [] for x in range(1, 8)}\n",
    "        true_positives = {x: [] for x in range(1, 8)}\n",
    "        num_entities = np.zeros(7)\n",
    "        for cat_ix in range(1, 8):\n",
    "\n",
    "            pspans = predicted_spans.get(cat_ix, [])\n",
    "            gspans = gt_spans.get(cat_ix, [])\n",
    "            num_entities[cat_ix - 1] = len(pspans)\n",
    "            if not len(pspans) or not len(gspans):\n",
    "                match_stats[cat_ix]['fn'] += len(gspans)\n",
    "                match_stats[cat_ix]['fp'] += len(pspans)\n",
    "            else:\n",
    "                all_overlaps = np.zeros((len(pspans), len(gspans)))\n",
    "                for x1 in range(len(pspans)):\n",
    "                    pspan = pspans[x1]\n",
    "                    for x2 in range(len(gspans)):\n",
    "                        gspan = gspans[x2]\n",
    "                        start_ix = max(pspan[0], gspan[0])\n",
    "                        end_ix = min(pspan[1], gspan[1])\n",
    "                        overlap = max(0, end_ix - start_ix + 1)\n",
    "                        if overlap > 0:\n",
    "                            o1 = overlap / (pspan[1] - pspan[0] + 1)\n",
    "                            o2 = overlap / (gspan[1] - gspan[0] + 1)\n",
    "                            if min(o1, o2) >= .5:\n",
    "                                all_overlaps[x1, x2] = max(o1, o2)\n",
    "                unused_p_ix = set(range(len(pspans)))\n",
    "                unused_g_ix = set(range(len(gspans)))\n",
    "                col_size = len(pspans)\n",
    "                row_size = len(gspans)\n",
    "                for ix in np.argsort(all_overlaps.ravel())[::-1]:\n",
    "                    if not len(unused_g_ix) or not len(unused_p_ix) or all_overlaps.ravel()[ix] == 0:\n",
    "                        match_stats[cat_ix]['fp'] += len(unused_p_ix)\n",
    "                        false_positives[cat_ix] = [(filtered_entities[cat_ix][p_ix][1:])\n",
    "                                                    for p_ix in unused_p_ix]\n",
    "                        match_stats[cat_ix]['fn'] += len(unused_g_ix)\n",
    "                        break\n",
    "                    p_ix = ix // row_size\n",
    "                    g_ix = ix % row_size\n",
    "                    if p_ix not in unused_p_ix or g_ix not in unused_g_ix:\n",
    "                        continue\n",
    "                    match_stats[cat_ix]['tp'] += 1\n",
    "                    true_positives[cat_ix].append((filtered_entities[cat_ix][p_ix][1:]))\n",
    "                    unused_g_ix.remove(g_ix)\n",
    "                    unused_p_ix.remove(p_ix)\n",
    "\n",
    "\n",
    "        return match_stats, num_entities, true_positives, false_positives\n",
    "\n",
    "\n",
    "    def map_span_to_word_indices(span, index_map, bounds):\n",
    "        return (index_map[bounds[span[0], 0]], index_map[bounds[span[1], 1] - 1])\n",
    "\n",
    "    def split_predstring(x):\n",
    "        vals = x.split()\n",
    "        return int(vals[0]), int(vals[-1])\n",
    "\n",
    "    def evaluate_score_thresholds(sorted_labels, tp, fp, fn):\n",
    "        base_f1 = (f1(tp, fp, fn))\n",
    "        scores = [base_f1]\n",
    "        for ix in range(len(sorted_labels) - 1):\n",
    "            if sorted_labels[ix] == 0:\n",
    "                fp -= 1\n",
    "            else:\n",
    "                tp -= 1\n",
    "                fn += 1\n",
    "            scores.append(f1(tp, fp, fn))\n",
    "            if scores[-1] < base_f1 - .01:\n",
    "                break\n",
    "        return scores\n",
    "\n",
    "    def f1(tp, fp, fn):\n",
    "        return tp / (tp + .5 * (fp + fn))\n",
    "\n",
    "    def make_gt_dict(df):\n",
    "        gt_dict = {}\n",
    "        for cat_ix in range(1, 8):\n",
    "            cat_name = label_names[cat_ix]\n",
    "            cat_entities = df.loc[df.discourse_type==cat_name]\n",
    "            if len(cat_entities):\n",
    "                gt_dict[cat_ix] = [(x[0], x[1]) for x in cat_entities.predictionstring.map(split_predstring)]\n",
    "        return gt_dict\n",
    "\n",
    "    def f1s(stats):\n",
    "        f1s = np.zeros(8)\n",
    "        f1s[1:] = stats[:, 0] / (1e-7 + stats[:, 0] + stats[:, 1:].mean(-1))\n",
    "        f1s[0] = f1s[1:].mean()\n",
    "        return f1s\n",
    "\n",
    "    def calc_entity_score(span, ps, c):\n",
    "        s, e = span\n",
    "        score = (ps[s, c * 2 - 1] + ps[s + 1: e + 1, c * 2].sum())/(e - s + 1)\n",
    "        return score\n",
    "\n",
    "    with open('oof_ps/gt_dicts.pickle', 'rb') as f:\n",
    "        gt_dicts = pickle.load(f)\n",
    "\n",
    "\n",
    "    list_of_dir_names = list_of_models\n",
    "\n",
    "    arrays = []\n",
    "\n",
    "    for array_name in 'all_outs, all_bounds, all_token_nums, all_word_indices, all_sample_ids'.split(', '):\n",
    "        if array_name == 'all_outs':\n",
    "            logps = np.load(f'oof_ps/{list_of_dir_names[0]}/{array_name}.npy', allow_pickle=True)\n",
    "            arrays.append(np.exp(logps))\n",
    "\n",
    "        else:\n",
    "            arrays.append(np.load(f'oof_ps/{list_of_dir_names[0]}/{array_name}.npy', allow_pickle=True))\n",
    "\n",
    "    all_outs, all_bounds, all_token_nums, all_word_indices, all_sample_ids = arrays\n",
    "    all_outs = all_outs\n",
    "    for ix in range(1, len(list_of_dir_names)):\n",
    "        arrays = []\n",
    "        for array_name in 'all_outs, all_bounds, all_token_nums'.split(', '):\n",
    "            if array_name == 'all_outs':\n",
    "                logps = np.load(f'oof_ps/{list_of_dir_names[ix]}/{array_name}.npy', allow_pickle=True)\n",
    "                arrays.append(np.exp(logps))\n",
    "            else:\n",
    "                arrays.append(np.load(f'oof_ps/{list_of_dir_names[ix]}/{array_name}.npy', allow_pickle=True))\n",
    "        new_outs, new_bounds, new_token_nums = arrays\n",
    "        new_outs = new_outs\n",
    "        merged_outs = []\n",
    "        merged_bounds = []\n",
    "        merged_token_nums = []\n",
    "        for sample_ix in range(len(new_outs)):\n",
    "            logits, bounds = merge_ps(all_outs[sample_ix][:all_token_nums[sample_ix]],\n",
    "                                      new_outs[sample_ix][:new_token_nums[sample_ix]],\n",
    "                                      all_bounds[sample_ix][:all_token_nums[sample_ix]],\n",
    "                                      new_bounds[sample_ix][:new_token_nums[sample_ix]])\n",
    "            merged_outs.append(logits)\n",
    "            merged_bounds.append(bounds)\n",
    "            merged_token_nums.append(len(logits))\n",
    "        all_outs = merged_outs\n",
    "        all_bounds = merged_bounds\n",
    "        all_token_nums = merged_token_nums\n",
    "    sample_id_to_ix = {x: ix for ix, x in enumerate(all_sample_ids)}\n",
    "\n",
    "\n",
    "    label_names = ['None', 'Lead', 'Position', 'Evidence', 'Claim',\n",
    "                   'Concluding Statement', 'Counterclaim', 'Rebuttal']\n",
    "\n",
    "    len_exp = .2\n",
    "\n",
    "    def extend_tokens(ent, n):\n",
    "        return ent\n",
    "\n",
    "    all_unfiltered_entities = [extract_entities(all_outs[sample_ix], all_token_nums[sample_ix])\n",
    "                               for sample_ix in range(len(all_outs))]\n",
    "    for sample_ix, sample_entities in enumerate(all_unfiltered_entities):\n",
    "        for cat_ix in sample_entities:\n",
    "            spans_with_scores_and_len = []\n",
    "            for span in sample_entities[cat_ix]:\n",
    "                spans_with_scores_and_len.append((span, calc_entity_score(span, all_outs[sample_ix], cat_ix) * (span[1] - span[0] + 1) ** len_exp, \n",
    "                                                 span[1] - span[0] + 1))\n",
    "            all_unfiltered_entities[sample_ix][cat_ix] = spans_with_scores_and_len\n",
    "    max_scores_hist = np.zeros((25, 8))\n",
    "    score_thresholds_hist = np.zeros((25, 8))\n",
    "    for min_num_tokens in range(25):\n",
    "        token_len_filters, word_len_filters = ([min_num_tokens] * 8, [min_num_tokens] * 8)\n",
    "        score_filters = [-100] * 8\n",
    "        all_fps = {ix: [] for ix in range(1, 8)}\n",
    "        all_tps = {ix: [] for ix in range(1, 8)}\n",
    "        all_stats = np.zeros((len(val0_ids), 7, 3))\n",
    "        for array_pos, sample_id in enumerate(val0_ids):\n",
    "            sample_ix = sample_id_to_ix[sample_id]\n",
    "            sample_stats, _, tp, fp  = process_sample(all_outs[sample_ix], all_unfiltered_entities[sample_ix],\n",
    "                                                      all_word_indices[sample_ix], \n",
    "                                        all_bounds[sample_ix],\n",
    "                                        gt_dicts[all_sample_ids[sample_ix]],\n",
    "                                        all_token_nums[sample_ix],\n",
    "                                        {ix:  {'fp': 0, 'fn': 0, 'tp': 0} for ix in range(1, 8)})\n",
    "            for cat_ix in range(7):\n",
    "                all_stats[array_pos, cat_ix] = [sample_stats[cat_ix + 1][x] for x in ('tp', 'fp', 'fn')]\n",
    "                all_fps[cat_ix + 1].extend(fp[cat_ix + 1])\n",
    "                all_tps[cat_ix + 1].extend(tp[cat_ix + 1])\n",
    "        max_scores = np.zeros(8)\n",
    "        thresholds = np.zeros(7)\n",
    "        for cat_ix in range(1, 8):\n",
    "            tp, fp, fn = all_stats[:, cat_ix - 1].sum(0)\n",
    "            all_scores = [x[0] for x in all_tps[cat_ix]]\n",
    "            all_scores.extend((x[0] for x in all_fps[cat_ix]))\n",
    "            all_num_tokens = np.array([x[1] for x in all_tps[cat_ix]] + [x[1] for x in all_fps[cat_ix]])\n",
    "            all_labels = [1] * len(all_tps[cat_ix]) + [0] * len(all_fps[cat_ix])\n",
    "            all_scores = np.array(all_scores)\n",
    "            all_labels = np.array(all_labels)\n",
    "            sorted_index = np.argsort(all_scores)\n",
    "            sorted_scores = all_scores[sorted_index]\n",
    "            sorted_labels = all_labels[sorted_index]\n",
    "            sorted_num_tokens = all_num_tokens[sorted_index]\n",
    "            all_thresholds = np.r_[[sorted_scores[0]], .5 * (sorted_scores[1:] + sorted_scores[:-1])]\n",
    "            evaluation_scores = evaluate_score_thresholds(sorted_labels, *all_stats.sum(0)[cat_ix - 1])\n",
    "            conv_filter = np.ones(max(int(len(evaluation_scores) * .05) // 2 * 2, 2))\n",
    "            half_len = len(conv_filter) // 2\n",
    "            conv_filter *= (1/len(conv_filter))\n",
    "            conved = np.convolve(evaluation_scores, conv_filter, 'same')\n",
    "            conved[:half_len] /= np.arange(half_len, len(conv_filter)) * (1/len(conv_filter))\n",
    "            conved[-half_len:] /= (np.arange(half_len + 1, len(conv_filter) + 1) * (1/len(conv_filter)))[::-1]\n",
    "            max_ix = np.argmax(conved)\n",
    "            max_scores[cat_ix] = conved[max_ix]\n",
    "            thresholds[cat_ix - 1] = all_thresholds[max_ix]\n",
    "\n",
    "            score_filtered_tp = sum(1 for x in all_tps[cat_ix] if x[0] <= thresholds[cat_ix - 1])\n",
    "            score_filtered_fp = sum(1 for x in all_fps[cat_ix] if x[0] <= thresholds[cat_ix - 1])\n",
    "            new_tp = tp - score_filtered_tp\n",
    "            new_fp = fp - score_filtered_fp\n",
    "            new_fn = fn + score_filtered_tp\n",
    "\n",
    "            score_filtered_mask = sorted_scores > thresholds[cat_ix - 1]\n",
    "            score_filtered_token_counts = sorted_num_tokens[score_filtered_mask]\n",
    "            score_filtered_word_counts = sorted_num_tokens[score_filtered_mask]\n",
    "            score_filtered_labels = sorted_labels[score_filtered_mask]\n",
    "\n",
    "            score_filtered_f1 = new_tp / (new_tp + .5 * (new_fp + new_fn))\n",
    "            max_scores_hist[min_num_tokens, cat_ix] = score_filtered_f1\n",
    "            score_thresholds_hist[min_num_tokens, cat_ix] = thresholds[cat_ix - 1]\n",
    "\n",
    "    token_len_filters = max_scores_hist.argmax(0)[1:]\n",
    "    score_filters = score_thresholds_hist[max_scores_hist.argmax(0), np.arange(8)][1:]\n",
    "\n",
    "    all_stats = np.zeros((len(val1_ids), 7, 3))\n",
    "    for array_pos, sample_id in enumerate(val1_ids):\n",
    "        sample_ix = sample_id_to_ix[sample_id]\n",
    "        sample_stats, _, tp, fp  = process_sample(all_outs[sample_ix], all_unfiltered_entities[sample_ix],\n",
    "                                                  all_word_indices[sample_ix], \n",
    "                                    all_bounds[sample_ix],\n",
    "                                    gt_dicts[all_sample_ids[sample_ix]],\n",
    "                                    all_token_nums[sample_ix],\n",
    "                                    {ix:  {'fp': 0, 'fn': 0, 'tp': 0} for ix in range(1, 8)})\n",
    "        for cat_ix in range(7):\n",
    "            all_stats[array_pos, cat_ix] = [sample_stats[cat_ix + 1][x] for x in ('tp', 'fp', 'fn')]\n",
    "\n",
    "\n",
    "    def extend_tokens(ent, n):\n",
    "        nonlocal extension_sizes\n",
    "        nonlocal extension_sizes2\n",
    "        ent_size = ent[1] - ent[0] + 1\n",
    "        for ix in range(len(extension_sizes2) - 1):\n",
    "            if ent_size <= ix + 2:\n",
    "                return [max(0, ent[0] + extension_sizes2[ix]) , min(n-1, ent[1] + extension_sizes[ix])]\n",
    "        return [max(0, ent[0] + extension_sizes2[-1]) , min(n-1, ent[1] + extension_sizes[-1])]\n",
    "\n",
    "    extension_sizes2 = [0] * 15\n",
    "    results = np.zeros((15, 10, 8))\n",
    "    for ext_ix in range(15):\n",
    "        for ext_num in range(10):\n",
    "            extension_sizes = [0] * 15\n",
    "            extension_sizes[ext_ix] = ext_num\n",
    "            all_stats = np.zeros((val_bounds[-1], 7, 3))\n",
    "            for array_pos, sample_id in enumerate(val0_ids):\n",
    "                sample_ix = sample_id_to_ix[sample_id]\n",
    "                sample_stats, _, tp, fp  = process_sample(all_outs[sample_ix], all_unfiltered_entities[sample_ix],\n",
    "                                                          all_word_indices[sample_ix], \n",
    "                                            all_bounds[sample_ix],\n",
    "                                            gt_dicts[all_sample_ids[sample_ix]],\n",
    "                                            all_token_nums[sample_ix],\n",
    "                                            {ix:  {'fp': 0, 'fn': 0, 'tp': 0} for ix in range(1, 8)})\n",
    "                for cat_ix in range(7):\n",
    "                    all_stats[sample_ix, cat_ix] = [sample_stats[cat_ix + 1][x] for x in ('tp', 'fp', 'fn')]\n",
    "            results[ext_ix, ext_num] = f1s(all_stats.sum(0))\n",
    "\n",
    "    extension_sizes = results[..., 0].argmax(1)\n",
    "\n",
    "    max_scores_hist = np.zeros((25, 8))\n",
    "    score_thresholds_hist = np.zeros((25, 8))\n",
    "    for min_num_tokens in range(25):\n",
    "        token_len_filters, word_len_filters = ([min_num_tokens] * 8, [min_num_tokens] * 8)\n",
    "        score_filters = [-100] * 8\n",
    "        all_fps = {ix: [] for ix in range(1, 8)}\n",
    "        all_tps = {ix: [] for ix in range(1, 8)}\n",
    "        all_stats = np.zeros((len(val0_ids), 7, 3))\n",
    "        for array_pos, sample_id in enumerate(val0_ids):\n",
    "            sample_ix = sample_id_to_ix[sample_id]\n",
    "            sample_stats, _, tp, fp  = process_sample(all_outs[sample_ix], all_unfiltered_entities[sample_ix],\n",
    "                                                      all_word_indices[sample_ix], \n",
    "                                        all_bounds[sample_ix],\n",
    "                                        gt_dicts[all_sample_ids[sample_ix]],\n",
    "                                        all_token_nums[sample_ix],\n",
    "                                        {ix:  {'fp': 0, 'fn': 0, 'tp': 0} for ix in range(1, 8)})\n",
    "            for cat_ix in range(7):\n",
    "                all_stats[array_pos, cat_ix] = [sample_stats[cat_ix + 1][x] for x in ('tp', 'fp', 'fn')]\n",
    "                all_fps[cat_ix + 1].extend(fp[cat_ix + 1])\n",
    "                all_tps[cat_ix + 1].extend(tp[cat_ix + 1])\n",
    "        max_scores = np.zeros(8)\n",
    "        thresholds = np.zeros(7)\n",
    "        for cat_ix in range(1, 8):\n",
    "            tp, fp, fn = all_stats[:, cat_ix - 1].sum(0)\n",
    "            all_scores = [x[0] for x in all_tps[cat_ix]]\n",
    "            all_scores.extend((x[0] for x in all_fps[cat_ix]))\n",
    "            all_num_tokens = np.array([x[1] for x in all_tps[cat_ix]] + [x[1] for x in all_fps[cat_ix]])\n",
    "            all_labels = [1] * len(all_tps[cat_ix]) + [0] * len(all_fps[cat_ix])\n",
    "            all_scores = np.array(all_scores)\n",
    "            all_labels = np.array(all_labels)\n",
    "            sorted_index = np.argsort(all_scores)\n",
    "            sorted_scores = all_scores[sorted_index]\n",
    "            sorted_labels = all_labels[sorted_index]\n",
    "            sorted_num_tokens = all_num_tokens[sorted_index]\n",
    "            all_thresholds = np.r_[[sorted_scores[0]], .5 * (sorted_scores[1:] + sorted_scores[:-1])]\n",
    "            evaluation_scores = evaluate_score_thresholds(sorted_labels, *all_stats.sum(0)[cat_ix - 1])\n",
    "            conv_filter = np.ones(max(int(len(evaluation_scores) * .05) // 2 * 2, 2))\n",
    "            half_len = len(conv_filter) // 2\n",
    "            conv_filter *= (1/len(conv_filter))\n",
    "            conved = np.convolve(evaluation_scores, conv_filter, 'same')\n",
    "            conved[:half_len] /= np.arange(half_len, len(conv_filter)) * (1/len(conv_filter))\n",
    "            conved[-half_len:] /= (np.arange(half_len + 1, len(conv_filter) + 1) * (1/len(conv_filter)))[::-1]\n",
    "            max_ix = np.argmax(conved)\n",
    "            max_scores[cat_ix] = conved[max_ix]\n",
    "            thresholds[cat_ix - 1] = all_thresholds[max_ix]\n",
    "\n",
    "            score_filtered_tp = sum(1 for x in all_tps[cat_ix] if x[0] <= thresholds[cat_ix - 1])\n",
    "            score_filtered_fp = sum(1 for x in all_fps[cat_ix] if x[0] <= thresholds[cat_ix - 1])\n",
    "            new_tp = tp - score_filtered_tp\n",
    "            new_fp = fp - score_filtered_fp\n",
    "            new_fn = fn + score_filtered_tp\n",
    "\n",
    "            score_filtered_mask = sorted_scores > thresholds[cat_ix - 1]\n",
    "            score_filtered_token_counts = sorted_num_tokens[score_filtered_mask]\n",
    "            score_filtered_word_counts = sorted_num_tokens[score_filtered_mask]\n",
    "            score_filtered_labels = sorted_labels[score_filtered_mask]\n",
    "\n",
    "            score_filtered_f1 = new_tp / (new_tp + .5 * (new_fp + new_fn))\n",
    "            max_scores_hist[min_num_tokens, cat_ix] = score_filtered_f1\n",
    "            score_thresholds_hist[min_num_tokens, cat_ix] = thresholds[cat_ix - 1]\n",
    "\n",
    "\n",
    "    token_len_filters = max_scores_hist.argmax(0)[1:]\n",
    "    score_filters = score_thresholds_hist[max_scores_hist.argmax(0), np.arange(8)][1:]\n",
    "\n",
    "    all_stats = np.zeros((len(val1_ids), 7, 3))\n",
    "    for array_pos, sample_id in enumerate(val1_ids):\n",
    "        sample_ix = sample_id_to_ix[sample_id]\n",
    "        sample_stats, _, tp, fp  = process_sample(all_outs[sample_ix], all_unfiltered_entities[sample_ix],\n",
    "                                                  all_word_indices[sample_ix], \n",
    "                                    all_bounds[sample_ix],\n",
    "                                    gt_dicts[all_sample_ids[sample_ix]],\n",
    "                                    all_token_nums[sample_ix],\n",
    "                                    {ix:  {'fp': 0, 'fn': 0, 'tp': 0} for ix in range(1, 8)})\n",
    "        for cat_ix in range(7):\n",
    "            all_stats[array_pos, cat_ix] = [sample_stats[cat_ix + 1][x] for x in ('tp', 'fp', 'fn')]\n",
    "    return f1s(all_stats.sum(0)), token_len_filters, score_filters, extension_sizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb9d6f-79ac-405d-ae60-7dc33ab81113",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s, len_filters, score_filters, exts = evaluate_combination(['debertav2_xlarge',\n",
    "                                                              'dirgyv3/clean',\n",
    "                                                              'debertav1_xlarge/bugged',\n",
    "                                                              'distilled_v1/clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c484e-5b78-46f8-8e54-6913d51441c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1s[0])#score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
