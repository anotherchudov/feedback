{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d851e06c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 2.754329,
     "end_time": "2022-02-19T00:35:43.384924",
     "exception": false,
     "start_time": "2022-02-19T00:35:40.630595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "sys.path.insert(0, PATH_TO_TRANSFORMERS_WITH_FAST_TOKENIZER_FOR_DEBERTA_V2)\n",
    "import torch as t\n",
    "t.autograd.set_grad_enabled(False)\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import DebertaV2TokenizerFast, DebertaV2Model, DebertaV2Config\n",
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a630e76b",
   "metadata": {
    "papermill": {
     "duration": 18.612853,
     "end_time": "2022-02-19T00:36:02.029117",
     "exception": false,
     "start_time": "2022-02-19T00:35:43.416264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class TvmLongformer(t.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feats = DebertaV2Model.from_pretrained('microsoft/deberta-v2-xlarge')\n",
    "        self.feats.pooler = None\n",
    "        self.class_projector = t.nn.Sequential(\n",
    "            t.nn.LayerNorm(1536),\n",
    "            t.nn.Linear(1536, 15)\n",
    "        )\n",
    "    def forward(self, tokens, mask):\n",
    "        return self.class_projector(self.feats(tokens, mask, return_dict=False)[0])\n",
    "    \n",
    "model = TvmLongformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891b7069",
   "metadata": {
    "papermill": {
     "duration": 0.173463,
     "end_time": "2022-02-19T00:36:02.211308",
     "exception": false,
     "start_time": "2022-02-19T00:36:02.037845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "debertav3_fix_text = lambda x: x.replace('\\n', '‽')\n",
    "class Dataset(t.utils.data.Dataset):\n",
    "    def __init__(self, files):\n",
    "        tokenizer = DebertaV2TokenizerFast.from_pretrained('microsoft/deberta-v2-xlarge')\n",
    "        tokenizer.model_max_length = 4096\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inverse_tokenizer_vocab = {y: x for x, y in tokenizer.vocab.items()}\n",
    "        files = set(files)\n",
    "        self.texts = {}\n",
    "        for file_id in files:\n",
    "            with open(f'train/{file_id}.txt') as f:\n",
    "                self.texts[file_id] = f.read().strip()\n",
    "        self.keys = sorted(self.texts.keys())\n",
    "        self.space_regex = re.compile('[\\s\\n]')\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    def __getitem__(self, ix):\n",
    "        tokens_array = np.zeros(4096, 'i8')\n",
    "        mask_array = np.zeros(4096, 'f4')\n",
    "        offsets_array = np.zeros((4096, 2), 'i4')\n",
    "        \n",
    "        text = self.texts[self.keys[ix]]\n",
    "        key = self.keys[ix]\n",
    "        tokenizer_outs = self.tokenizer(debertav3_fix_text(text), return_offsets_mapping=True)\n",
    "        tokenizer_outs['input_ids'] = [x if x != 126599 else 128000 for x in tokenizer_outs['input_ids']]\n",
    "        tokens = np.array(tokenizer_outs['input_ids'], 'i8')\n",
    "        mask = np.array(tokenizer_outs['attention_mask'], 'f4')\n",
    "        default_offset_mappings = tokenizer_outs['offset_mapping']\n",
    "        ids = tokenizer_outs['input_ids']\n",
    "        num_tokens = len(ids)\n",
    "        offset_mappings = [(0,0)]\n",
    "        for ix in range(1, num_tokens - 1):\n",
    "            a, b = default_offset_mappings[ix]\n",
    "            token = self.inverse_tokenizer_vocab[ids[ix]]\n",
    "            if len(token) > 1 and token[0] == '▁' and ix != 1:\n",
    "                a += 1\n",
    "            offset_mappings.append((a, b))\n",
    "        offset_mappings.append((0,0))\n",
    "        \n",
    "        offsets = np.vstack(offset_mappings).astype('i4')\n",
    "        \n",
    "        \n",
    "        tokens_array[:len(tokens)] = tokens\n",
    "        mask_array[:len(tokens)] = mask\n",
    "        offsets_array[:len(tokens)] = offsets\n",
    "        \n",
    "        index_map = []\n",
    "        current_word = 0\n",
    "        blank = False\n",
    "        for char_ix in range(text.index(text.strip()[0]), len(text)):\n",
    "            if self.space_regex.match(text[char_ix]) is not None:\n",
    "                blank = True\n",
    "            elif blank:\n",
    "                current_word += 1\n",
    "                blank = False\n",
    "            index_map.append(current_word)\n",
    "            \n",
    "        return tokens_array, mask_array, offsets_array, index_map, key, len(tokens)\n",
    "    \n",
    "first_batch = True\n",
    "def collate_fn(ins):\n",
    "    global first_batch\n",
    "    if first_batch:\n",
    "        max_len = 2048\n",
    "        first_batch = False\n",
    "    else:\n",
    "        max_len = (max(x[-1] for x in ins) + 7) // 8 * 8\n",
    "    return tuple(t.from_numpy(np.concatenate([ins[z][x][None, :max_len]\n",
    "                                              for z in range(len(ins))]))\n",
    "                 for x in range(len(ins[0]) - 3)) \\\n",
    "                 + ([x[-3] for x in ins], [x[-2] for x in ins], np.array([x[-1] for x in ins]),)    \n",
    "\n",
    "\n",
    "label_names = ['None', 'Lead', 'Position', 'Evidence', 'Claim',\n",
    "               'Concluding Statement', 'Counterclaim', 'Rebuttal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84398691-8456-4507-9e20-822696a73ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = glob('checkpoints_xlarge_v2/extra_distclean_mse*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f82b36-0278-4fc1-892b-7da3becc18b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by fold\n",
    "#sorted_checkpoints = sorted(checkpoints, key=lambda x: int(x.split('/')[-1].split('_')[3][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19559590-74fb-41b9-acbc-5ee1a7203402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5275e16-403b-498b-b1cd-cfa6221e1ad0",
   "metadata": {
    "papermill": {
     "duration": 3.409657,
     "end_time": "2022-02-19T00:36:05.654335",
     "exception": false,
     "start_time": "2022-02-19T00:36:02.244678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7f1913f-0f6e-4740-bdff-15c02a0f51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('id_to_ix_map.pickle', 'rb') as f:\n",
    "    id_to_ix_map = {x.split('/')[-1].split('.')[0]: y for x, y in pickle.load(f).items()}\n",
    "with open('data_splits.pickle', 'rb') as f:\n",
    "    data_splits = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02fdac34-257a-4306-9c3e-1d81f04997f8",
   "metadata": {
    "papermill": {
     "duration": 72.881277,
     "end_time": "2022-02-19T00:37:18.543568",
     "exception": false,
     "start_time": "2022-02-19T00:36:05.662291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3140/3140 [02:16<00:00, 22.99it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3121/3121 [02:17<00:00, 22.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3139/3139 [02:17<00:00, 22.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3096/3096 [02:15<00:00, 22.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3098/3098 [02:16<00:00, 22.68it/s]\n"
     ]
    }
   ],
   "source": [
    "all_outs = np.zeros((len(glob('train/*.txt')), 2048, 15), 'f4')\n",
    "all_bounds = np.zeros((len(glob('train/*.txt')), 2048, 2), 'i4')\n",
    "all_token_nums = np.zeros((len(glob('train/*.txt')),), 'i4')\n",
    "all_word_indices = []\n",
    "all_sample_ids = []\n",
    "ix = 0\n",
    "for fold_ix, checkpoint in enumerate(sorted_checkpoints):\n",
    "    val_files = data_splits[0][250]['normed'][fold_ix]\n",
    "    dataset = t.utils.data.DataLoader(Dataset(val_files), collate_fn=collate_fn,\n",
    "                                  batch_size=1, num_workers=2)\n",
    "    model.load_state_dict(t.load(checkpoint), strict=False);\n",
    "    for batch in tqdm(dataset):\n",
    "        tokens, mask, bounds, word_indices, sample_ids, num_tokens = batch\n",
    "        num_tokens = num_tokens[0]\n",
    "        batch_size, batch_len = tokens.shape[:2]\n",
    "        outs = t.log_softmax(model(tokens.cuda(), mask.cuda()), -1)\n",
    "        all_outs[ix: ix + batch_size, :num_tokens - 2] = outs[:, 1:num_tokens - 1].cpu().numpy()\n",
    "        all_bounds[ix: ix + batch_size, :num_tokens - 2] = bounds[:, 1:num_tokens - 1]\n",
    "        all_token_nums[ix: ix + batch_size] = num_tokens - 2\n",
    "        all_word_indices.extend(word_indices)\n",
    "        all_sample_ids.extend(sample_ids)\n",
    "        ix += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb76f4-1e25-4893-bf98-61559a66ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85c071-6d46-48a6-a261-d98e18037b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'oof_ps/{model_name}'):\n",
    "    os.makedirs(f'oof_ps/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a2a4df6-01d4-4520-b7bf-9fc6aca9c273",
   "metadata": {
    "papermill": {
     "duration": 0.007433,
     "end_time": "2022-02-19T00:37:18.627455",
     "exception": false,
     "start_time": "2022-02-19T00:37:18.620022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qwe/miniconda3/envs/pytorch2/lib/python3.7/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "for (array, \n",
    "    array_name) in zip((all_outs, all_bounds, all_token_nums, all_word_indices, all_sample_ids),\n",
    "                        'all_outs, all_bounds, all_token_nums, all_word_indices, all_sample_ids'.split(', ')):\n",
    "    np.save(f'oof_ps/{model_name}/{array_name}.npy', array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 109.360148,
   "end_time": "2022-02-19T00:37:21.949556",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-19T00:35:32.589408",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
