{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e687b39e",
   "metadata": {},
   "source": [
    "# CV Calculation\n",
    "\n",
    "- bug : 0.7019\n",
    "- wonho : 0.70236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf186f2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T17:34:40.055095Z",
     "start_time": "2022-03-14T17:34:40.051444Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "DATASET_PATH = ('../../../feedback-prize-2021')\n",
    "\n",
    "sys.path.insert(0, '../codes/new_transformers_branch/transformers/src')\n",
    "sys.path.append('../codes')\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46f10550",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T17:34:59.360759Z",
     "start_time": "2022-03-14T17:34:59.354525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f07aab6c310>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import easydict\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from new_transformers import DebertaV2TokenizerFast\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.autograd.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c98f2ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T17:34:59.563051Z",
     "start_time": "2022-03-14T17:34:59.559669Z"
    }
   },
   "outputs": [],
   "source": [
    "from module.metric import calc_acc, process_sample, make_match_dict\n",
    "\n",
    "from module.utils import get_data_files\n",
    "from module.dataset import get_dataloader\n",
    "from module.loss import get_criterion\n",
    "from module.optimizer import get_optimizer\n",
    "from module.scheduler import get_scheduler\n",
    "from model.model import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad64fdaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T17:34:59.798978Z",
     "start_time": "2022-03-14T17:34:59.796292Z"
    }
   },
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({'model': 'microsoft/deberta-v3-large-ducky',\n",
    "                          'grad_checkpt': True,\n",
    "                          'cnn1d': False,\n",
    "                          'extra_dense': False,\n",
    "                          'device': 0,\n",
    "                          'ddp': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09f850e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T17:35:00.139765Z",
     "start_time": "2022-03-14T17:35:00.131559Z"
    }
   },
   "outputs": [],
   "source": [
    "class Callibration(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size=128,\n",
    "                 n_layers=2,\n",
    "                 device='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(16, hidden_size, padding_idx=15)\n",
    "        self.lstm = nn.LSTM(hidden_size,\n",
    "                            hidden_size,\n",
    "                            n_layers,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 16)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        h = torch.zeros(self.n_layers * 2, batch_size, self.hidden_size)\n",
    "        h = h.to(self.device)\n",
    "\n",
    "        c = torch.zeros(self.n_layers * 2, batch_size, self.hidden_size)\n",
    "        c = c.to(self.device)\n",
    "\n",
    "        return (h, c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        out = self.embedding(x)\n",
    "        \n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.lstm(out, hidden)\n",
    "        \n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_size * 2)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2107445c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T18:23:06.776291Z",
     "start_time": "2022-03-14T18:23:06.768216Z"
    }
   },
   "outputs": [],
   "source": [
    "callibration_model = Callibration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "265bf1ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T17:35:14.571232Z",
     "start_time": "2022-03-14T17:35:06.921120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Ducky Modified DebertaV2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = get_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55ca53ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T17:35:14.575553Z",
     "start_time": "2022-03-14T17:35:14.572481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../feedback-prize-2021/1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osp.join(DATASET_PATH, '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1036ab8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T17:35:14.599598Z",
     "start_time": "2022-03-14T17:35:14.576336Z"
    }
   },
   "outputs": [],
   "source": [
    "fix_text = lambda x: x.replace('\\n', 'â€½')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42837d",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70210f14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T17:35:14.617600Z",
     "start_time": "2022-03-14T17:35:14.601594Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ValDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, files):\n",
    "        tokenizer = DebertaV2TokenizerFast.from_pretrained('microsoft/deberta-v3-large')\n",
    "        tokenizer.model_max_length = 2048\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.texts = {}\n",
    "        self.raw_texts = {}\n",
    "        \n",
    "        for file in files:\n",
    "            file_name = osp.join(DATASET_PATH, 'train', file + '.txt')\n",
    "            with open(file_name) as f:\n",
    "                text = f.read().strip()\n",
    "                self.texts[file] = fix_text(text)\n",
    "                self.raw_texts[file] = text\n",
    "                \n",
    "        self.text_ids = list(self.texts.keys())\n",
    "        self.space_regex = re.compile('[\\s\\n]')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_ids)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        tokens_array = np.zeros(2048, 'i8')\n",
    "        mask_array = np.zeros(2048, 'f4')\n",
    "        offsets_array = np.zeros((2048, 2), 'i4')\n",
    "        \n",
    "        text = self.texts[self.text_ids[ix]]\n",
    "        raw_text = self.raw_texts[self.text_ids[ix]]\n",
    "        text_id = self.text_ids[ix]\n",
    "        \n",
    "        tokenizer_outs = self.tokenizer(text, return_offsets_mapping=True)\n",
    "        tokenizer_outs['input_ids'] = [x if x != 126861 else 128000 for x in tokenizer_outs['input_ids']]\n",
    "        \n",
    "        tokens = np.array(tokenizer_outs['input_ids'], 'i8')\n",
    "        mask = np.array(tokenizer_outs['attention_mask'], 'f4')\n",
    "        offsets = np.vstack(tokenizer_outs['offset_mapping']).astype('i4')\n",
    "        \n",
    "        tokens_array[:len(tokens)] = tokens\n",
    "        tokens_array[len(tokens):] = 0\n",
    "        mask_array[:len(tokens)] = mask\n",
    "        mask_array[len(tokens):] = 0\n",
    "        offsets_array[:len(tokens)] = offsets\n",
    "        offsets_array[len(tokens):] = 0\n",
    "        \n",
    "        index_map = []\n",
    "        current_word = 0\n",
    "        blank = False\n",
    "        for char_ix in range(raw_text.index(raw_text.strip()[0]), len(raw_text)):\n",
    "            if self.space_regex.match(raw_text[char_ix]) is not None:\n",
    "                blank = True\n",
    "            elif blank:\n",
    "                current_word += 1\n",
    "                blank = False\n",
    "            index_map.append(current_word)\n",
    "        \n",
    "        return tokens_array, mask_array, offsets_array, index_map, text_id, len(tokens)\n",
    "    \n",
    "def collate_fn(ins):\n",
    "    max_len = (max(x[-1] for x in ins) + 7) // 8 * 8\n",
    "    return tuple(torch.from_numpy(np.concatenate([ins[z][x][None, :max_len] for z in range(len(ins))])) \n",
    "                 for x in range(len(ins[0]) - 3)) \\\n",
    "                 + ([x[-3] for x in ins], [x[-2] for x in ins], np.array([x[-1] for x in ins]),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f7ba71",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "931ca501",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T18:22:31.586206Z",
     "start_time": "2022-03-14T18:22:31.583319Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot(a):\n",
    "    return np.squeeze(np.eye(16)[a.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bf4d42cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T18:22:40.598639Z",
     "start_time": "2022-03-14T18:22:40.571848Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "START_WITH_I = True\n",
    "LOOK_AHEAD = True\n",
    "\n",
    "def extract_entities(ps, n, return_score=False, callibration=False):\n",
    "    # callibration process\n",
    "    if callibration:\n",
    "        inputs = torch.Tensor(ps.argmax(-1))\n",
    "        inputs[n:] = 15\n",
    "        inputs = inputs.long().unsqueeze(0).cuda()\n",
    "\n",
    "        cal_outs = callibration_model(inputs).argmax(-1)[0].cpu().numpy()\n",
    "        ps = ps + one_hot(cal_outs)[:, :15]\n",
    "    \n",
    "    max_ps = ps.max(-1)\n",
    "    ps = ps.argsort(-1)[...,::-1]\n",
    "    # argmax\n",
    "    cat_ps = ps[:, 0]\n",
    "    # argmax2\n",
    "    cat_ps2 = ps[:, 1]\n",
    "    \n",
    "    all_entities = {}\n",
    "    new_entity = True\n",
    "    current_cat = current_start = current_end = None\n",
    "    \n",
    "    # except for special tokens\n",
    "    for ix in range(1, n - 1):\n",
    "\n",
    "        # logic on new entity\n",
    "        if new_entity:\n",
    "            # Background - ignore\n",
    "            if cat_ps[ix] == 0:\n",
    "                pass\n",
    "\n",
    "            # B-LABEL(1,3,5,7,...) - start entity\n",
    "            elif cat_ps[ix] % 2 == 1:\n",
    "                current_cat = (cat_ps[ix] + 1) // 2\n",
    "                current_start = current_end = ix\n",
    "                new_entity = False\n",
    "                \n",
    "                if current_cat in [6, 7]:\n",
    "                    LOOK_AHEAD = False\n",
    "                else:\n",
    "                    LOOK_AHEAD = True\n",
    "\n",
    "            # I-LABEL(2,4,6,8,...) - conditional start\n",
    "            elif cat_ps[ix] % 2 == 0:\n",
    "                if START_WITH_I:\n",
    "                    # Condition: I-LABEL in argmax with B-LABEL in argmax2\n",
    "                    if cat_ps[ix] == (cat_ps2[ix]+1):\n",
    "                        current_cat = cat_ps[ix] // 2\n",
    "                        current_start = current_end = ix\n",
    "                        new_entity = False\n",
    "                        \n",
    "                        if current_cat in [6, 7]:\n",
    "                            LOOK_AHEAD = False\n",
    "                        else:\n",
    "                            LOOK_AHEAD = True\n",
    "        \n",
    "        # logic on ongoing entity\n",
    "        else:\n",
    "            # Background - save current entity and init current\n",
    "            if cat_ps[ix] == 0:\n",
    "                if LOOK_AHEAD:\n",
    "                    if (cat_ps[ix+1] == current_cat*2) and (cat_ps2[ix] == current_cat*2):\n",
    "                        current_end = ix\n",
    "                    else:\n",
    "                        # update current\n",
    "                        if current_cat not in all_entities:\n",
    "                            all_entities[current_cat] = []\n",
    "                        all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "                        # init current for new start\n",
    "                        new_entity = True\n",
    "                        current_cat = current_start = current_end = None\n",
    "                \n",
    "                else:\n",
    "                    # update current\n",
    "                    if current_cat not in all_entities:\n",
    "                        all_entities[current_cat] = []\n",
    "                    all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "                    # init current for new start\n",
    "                    new_entity = True\n",
    "                    current_cat = current_start = current_end = None\n",
    "\n",
    "            # B-LABEL(1,3,5,7,...) - save current entity and start new\n",
    "            elif cat_ps[ix] % 2 == 1:\n",
    "                if cat_ps[ix] == (current_cat*2-1):\n",
    "                    # update current\n",
    "                    if current_cat not in all_entities:\n",
    "                        all_entities[current_cat] = []\n",
    "                    all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "                    # start new current\n",
    "                    current_cat = (cat_ps[ix] + 1) // 2\n",
    "                    current_start = current_end = ix\n",
    "                    new_entity = False\n",
    "                    \n",
    "                    if current_cat in [6, 7]:\n",
    "                        LOOK_AHEAD = False\n",
    "                    else:\n",
    "                        LOOK_AHEAD = True\n",
    "                \n",
    "                else:\n",
    "                    if LOOK_AHEAD:\n",
    "                        if (cat_ps[ix+1] == current_cat*2) and (cat_ps2[ix] == current_cat*2):\n",
    "                            current_end = ix\n",
    "                        else:\n",
    "                            # update current\n",
    "                            if current_cat not in all_entities:\n",
    "                                all_entities[current_cat] = []\n",
    "                            all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "                            # start new current\n",
    "                            current_cat = (cat_ps[ix] + 1) // 2\n",
    "                            current_start = current_end = ix\n",
    "                            new_entity = False\n",
    "                        \n",
    "                            if current_cat in [6, 7]:\n",
    "                                LOOK_AHEAD = False\n",
    "                            else:\n",
    "                                LOOK_AHEAD = True\n",
    "                            \n",
    "                    else:\n",
    "                        # update current\n",
    "                        if current_cat not in all_entities:\n",
    "                            all_entities[current_cat] = []\n",
    "                        all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "                        # start new current\n",
    "                        current_cat = (cat_ps[ix] + 1) // 2\n",
    "                        current_start = current_end = ix\n",
    "                        new_entity = False\n",
    "                        \n",
    "                        if current_cat in [6, 7]:\n",
    "                            LOOK_AHEAD = False\n",
    "                        else:\n",
    "                            LOOK_AHEAD = True\n",
    "                \n",
    "            # I-LABEL(2,4,6,8,...) - conditional continue\n",
    "            elif cat_ps[ix] % 2 == 0:\n",
    "                # B-LABEL0, I-LABEL0 - continue\n",
    "                if cat_ps[ix] == current_cat*2:\n",
    "                    current_end = ix\n",
    "                # B-LBAEL0, I-LABEL1 - conditional finish current entity\n",
    "                else:\n",
    "                    if LOOK_AHEAD:\n",
    "                        if (cat_ps[ix+1] == current_cat*2) and (cat_ps2[ix] == current_cat*2):\n",
    "                            current_end = ix\n",
    "                        else:\n",
    "                            # update current\n",
    "                            if current_cat not in all_entities:\n",
    "                                all_entities[current_cat] = []\n",
    "                            all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "                            # init current\n",
    "                            new_entity = True\n",
    "                            current_cat = current_start = current_end = None\n",
    "                    else:\n",
    "                        # update current\n",
    "                        if current_cat not in all_entities:\n",
    "                            all_entities[current_cat] = []\n",
    "                        all_entities[current_cat].append((current_start, current_end))\n",
    "\n",
    "                        # init current\n",
    "                        new_entity = True\n",
    "                        current_cat = current_start = current_end = None\n",
    "    \n",
    "    # last entity\n",
    "    if not new_entity:\n",
    "        # update current\n",
    "        if current_cat not in all_entities:\n",
    "            all_entities[current_cat] = []\n",
    "        all_entities[current_cat].append((current_start, current_end))\n",
    "    \n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67c53766",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T17:35:14.662706Z",
     "start_time": "2022-03-14T17:35:14.642607Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def filter_ps(all_entities, ps):\n",
    "    \n",
    "    len_filters = [5, 2, 15, 2, 8, 7, 7]\n",
    "    score_filters = [0.55, 0.50, 0.55, 0.45, 0.50, 0.45, 0.50]\n",
    "    \n",
    "    for cat_ix, min_num_tokens, min_score in zip(range(1, 8), len_filters, score_filters):\n",
    "        \n",
    "        if cat_ix in all_entities:\n",
    "            possible_entities = [entity for entity in all_entities[cat_ix] \n",
    "                                 if entity[1] - entity[0] + 1 >= min_num_tokens\n",
    "                                 and calc_entity_score(entity, ps, cat_ix) * (entity[1] - entity[0])**.2 > min_score]\n",
    "    \n",
    "            if cat_ix in (1, 2, 5):\n",
    "                if len(possible_entities) > 1:\n",
    "                    max_score = -9999\n",
    "                    for possible_entity in possible_entities:\n",
    "                        entity_score = calc_entity_score(possible_entity, ps, cat_ix)\n",
    "                        if entity_score > max_score:\n",
    "                            max_score = entity_score\n",
    "                            biggest_entity = possible_entity\n",
    "                    possible_entities = [biggest_entity]\n",
    "            \n",
    "            all_entities[cat_ix] = possible_entities\n",
    "    \n",
    "    return all_entities.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3c667bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T17:35:14.683031Z",
     "start_time": "2022-03-14T17:35:14.663495Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def calc_entity_score(span, ps, c):\n",
    "    s, e = span\n",
    "    score = (ps[s, c * 2 - 1] + ps[s + 1 : e + 1, c * 2].sum()) / (e - s + 1)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c702513",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T17:35:14.699154Z",
     "start_time": "2022-03-14T17:35:14.683842Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_span_to_word_indices(span, index_map, bounds):\n",
    "    return (index_map[bounds[span[0], 0]], index_map[bounds[span[1], 1] - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e790d",
   "metadata": {},
   "source": [
    "## F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47247f69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T17:35:14.776748Z",
     "start_time": "2022-03-14T17:35:14.768288Z"
    },
    "code_folding": [
     0,
     17,
     71
    ]
   },
   "outputs": [],
   "source": [
    "def calc_overlap2(set_pred, set_gt):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    # Length of each and intersection\n",
    "    try:\n",
    "        len_gt = len(set_gt)\n",
    "        len_pred = len(set_pred)\n",
    "        inter = len(set_gt & set_pred)\n",
    "        overlap_1 = inter / len_gt\n",
    "        overlap_2 = inter/ len_pred\n",
    "        return (overlap_1, overlap_2)\n",
    "    except:  # at least one of the input is NaN\n",
    "        return (0, 0)\n",
    "\n",
    "def score_feedback_comp_micro2(pred_df, gt_df, discourse_type):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "        \n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = gt_df.loc[gt_df['discourse_type'] == discourse_type, \n",
    "                      ['id', 'predictionstring']].reset_index(drop=True)\n",
    "    pred_df = pred_df.loc[pred_df['class'] == discourse_type,\n",
    "                      ['id', 'predictionstring']].reset_index(drop=True)\n",
    "    pred_df['pred_id'] = pred_df.index\n",
    "    gt_df['gt_id'] = gt_df.index\n",
    "    pred_df['predictionstring'] = [set(pred.split(' ')) for pred in pred_df['predictionstring']]\n",
    "    gt_df['predictionstring'] = [set(pred.split(' ')) for pred in gt_df['predictionstring']]\n",
    "    \n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(gt_df,\n",
    "                           left_on='id',\n",
    "                           right_on='id',\n",
    "                           how='outer',\n",
    "                           suffixes=('_pred','_gt')\n",
    "                          )\n",
    "    overlaps = [calc_overlap2(*args) for args in zip(joined.predictionstring_pred, \n",
    "                                                     joined.predictionstring_gt)]\n",
    "    \n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined['potential_TP'] = [(overlap[0] >= 0.5 and overlap[1] >= 0.5) \\\n",
    "                              for overlap in overlaps]\n",
    "    joined['max_overlap'] = [max(*overlap) for overlap in overlaps]\n",
    "    joined_tp = joined.query('potential_TP').reset_index(drop=True)\n",
    "    tp_pred_ids = joined_tp\\\n",
    "        .sort_values('max_overlap', ascending=False) \\\n",
    "        .groupby(['id','gt_id'])['pred_id'].first()\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = set(joined['pred_id'].unique()) - set(tp_pred_ids)\n",
    "\n",
    "    matched_gt_ids = joined_tp['gt_id'].unique()\n",
    "    unmatched_gt_ids = set(joined['gt_id'].unique()) -  set(matched_gt_ids)\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    #calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5*(FP+FN))\n",
    "    return my_f1_score\n",
    "\n",
    "def score_feedback_comp2(pred_df, gt_df, return_class_scores=False):\n",
    "    class_scores = {}\n",
    "    for discourse_type in gt_df.discourse_type.unique():\n",
    "        class_score = score_feedback_comp_micro2(pred_df, gt_df, discourse_type)\n",
    "        class_scores[discourse_type] = class_score\n",
    "    f1 = np.mean([v for v in class_scores.values()])\n",
    "    if return_class_scores:\n",
    "        return f1, class_scores\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08daaba4",
   "metadata": {},
   "source": [
    "## CV Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7878e30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T17:35:19.860166Z",
     "start_time": "2022-03-14T17:35:19.821347Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data_file/id_to_ix_map.pickle', 'rb') as f:\n",
    "    id_to_ix_map = {x.split('/')[-1].split('.')[0]: y for x, y in pickle.load(f).items()}\n",
    "with open('../data_file/data_splits.pickle', 'rb') as f:\n",
    "    data_splits = pickle.load(f)\n",
    "\n",
    "val_files_all = []\n",
    "val_ids_all = []\n",
    "\n",
    "seed = 0\n",
    "for val_fold in range(5):\n",
    "    val_files = data_splits[seed][250]['normed'][val_fold]\n",
    "    val_ids = [id_to_ix_map[x] for x in val_files]\n",
    "    \n",
    "    val_files_all.append(val_files)\n",
    "    val_ids_all.append(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d81b2921",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T18:02:11.904763Z",
     "start_time": "2022-03-14T17:47:48.192639Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3098 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_names = [\n",
    "    \"None\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Evidence\",\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "outs_f = []\n",
    "bounds_f = []\n",
    "token_nums_f = []\n",
    "word_indices_f = []\n",
    "sample_ids_f = []\n",
    "\n",
    "cv_n = 5\n",
    "checkpoints = os.listdir('../result/online_no_wd_noise_filtering0.4_mean_teacher0.99/wonho')\n",
    "checkpoints = sorted(checkpoints, key=lambda checkpoint: checkpoint[checkpoint.index('fold') + 4])\n",
    "checkpoints = [osp.join('../result/online_no_wd_noise_filtering0.4_mean_teacher0.99/wonho', checkpoint) for checkpoint in checkpoints]\n",
    "\n",
    "fold_start_idx = []\n",
    "fold_ix = 0\n",
    "for val_fold in range(cv_n):\n",
    "    fold_start_idx.append(fold_ix)\n",
    "    \n",
    "    val_files = val_files_all[val_fold]\n",
    "    dataset = ValDataset(val_files)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, collate_fn=collate_fn, batch_size=1, num_workers=2, shuffle=False\n",
    "    )\n",
    "\n",
    "    model.eval().cuda()\n",
    "\n",
    "    all_outs = np.zeros((len(val_files), 2048, 15), \"f4\")\n",
    "    all_bounds = np.zeros((len(val_files), 2048, 2), \"i4\")\n",
    "    all_token_nums = np.zeros(len(val_files), \"i4\")\n",
    "    all_word_indices = []\n",
    "    all_sample_ids = []\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoints[val_fold]))\n",
    "    ix = 0\n",
    "    for batch in tqdm(dataloader, leave=False):\n",
    "        tokens, mask, bounds, word_indices, sample_ids, num_tokens = batch\n",
    "        batch_size, batch_len = tokens.shape[:2]\n",
    "        outs = torch.softmax(model(tokens.cuda(), mask.cuda()), -1)\n",
    "\n",
    "        all_outs[ix : ix + batch_size, :batch_len] += outs.cpu().numpy()\n",
    "        all_bounds[ix : ix + batch_size, :batch_len] = bounds\n",
    "        all_token_nums[ix : ix + batch_size] = num_tokens\n",
    "        all_word_indices.extend(word_indices)\n",
    "        all_sample_ids.extend(sample_ids)\n",
    "\n",
    "        ix += batch_size\n",
    "        fold_ix += batch_size\n",
    "    \n",
    "    outs_f.append(all_outs)\n",
    "    bounds_f.append(all_bounds)\n",
    "    token_nums_f.append(all_token_nums)\n",
    "    word_indices_f += all_word_indices\n",
    "    sample_ids_f += all_sample_ids\n",
    "\n",
    "all_outs = np.concatenate(outs_f)\n",
    "all_bounds = np.concatenate(bounds_f)\n",
    "all_token_nums = np.concatenate(token_nums_f)\n",
    "all_word_indices = word_indices_f\n",
    "all_sample_ids = sample_ids_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fe5a907f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T14:12:44.249801Z",
     "start_time": "2022-03-15T13:50:52.865282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "callibration_checkpoints = os.listdir('../result/callibration_random_with_starting/')\n",
    "callibration_model.eval().cuda()\n",
    "\n",
    "sub_sample_ids = []\n",
    "sub_cat_names = []\n",
    "sub_spans = []\n",
    "sub_scores = []\n",
    "\n",
    "fold_i = 1\n",
    "fold_thres = fold_start_idx[fold_i]\n",
    "callibration_model.load_state_dict(torch.load('../result/callibration_random_with_starting/' + callibration_checkpoints[fold_i - 1]))\n",
    "\n",
    "for sample_ix in tqdm(range(len(all_token_nums)), leave=False):\n",
    "    if fold_i != 4 and sample_ix >= fold_thres:\n",
    "        fold_i += 1\n",
    "        fold_thres = fold_start_idx[fold_i]\n",
    "        callibration_model.load_state_dict(torch.load('../result/callibration_random_with_starting/' + callibration_checkpoints[fold_i - 1]))\n",
    "    \n",
    "    predicted_spans = {\n",
    "        x: {\n",
    "            \"entity\": [\n",
    "                map_span_to_word_indices(\n",
    "                    span, all_word_indices[sample_ix], all_bounds[sample_ix]\n",
    "                )\n",
    "                for span in y\n",
    "            ],\n",
    "            \"scores\": [calc_entity_score(span, all_outs[sample_ix], x) for span in y],\n",
    "        }\n",
    "        for x, y in filter_ps(\n",
    "            extract_entities(all_outs[sample_ix], all_token_nums[sample_ix], callibration=True),\n",
    "            all_outs[sample_ix],\n",
    "        )\n",
    "    }\n",
    "    for cat_ix in predicted_spans:\n",
    "        for entity in predicted_spans[cat_ix][\"entity\"]:\n",
    "            sub_sample_ids.append(all_sample_ids[sample_ix])\n",
    "            sub_cat_names.append(label_names[cat_ix])\n",
    "            sub_spans.append(\" \".join(str(x) for x in range(entity[0], entity[1] + 1)))\n",
    "\n",
    "        for scores in predicted_spans[cat_ix][\"scores\"]:\n",
    "            sub_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cb14370d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T14:12:44.296279Z",
     "start_time": "2022-03-15T14:12:44.251073Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": sub_sample_ids,\n",
    "        \"class\": sub_cat_names,\n",
    "        \"predictionstring\": sub_spans,\n",
    "        \"scores\": sub_scores,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e4793a2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T14:12:44.304522Z",
     "start_time": "2022-03-15T14:12:44.297416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predictionstring</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4BB5ADD5A1FE</td>\n",
       "      <td>Lead</td>\n",
       "      <td>3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20...</td>\n",
       "      <td>0.897571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4BB5ADD5A1FE</td>\n",
       "      <td>Position</td>\n",
       "      <td>22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 3...</td>\n",
       "      <td>0.881595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4BB5ADD5A1FE</td>\n",
       "      <td>Claim</td>\n",
       "      <td>40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 5...</td>\n",
       "      <td>0.486816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4BB5ADD5A1FE</td>\n",
       "      <td>Claim</td>\n",
       "      <td>58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 7...</td>\n",
       "      <td>0.834153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4BB5ADD5A1FE</td>\n",
       "      <td>Claim</td>\n",
       "      <td>74 75 76 77 78 79 80 81 82 83 84</td>\n",
       "      <td>0.869926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141913</th>\n",
       "      <td>B9B45507DDF8</td>\n",
       "      <td>Claim</td>\n",
       "      <td>78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 9...</td>\n",
       "      <td>0.763800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141914</th>\n",
       "      <td>B9B45507DDF8</td>\n",
       "      <td>Claim</td>\n",
       "      <td>205 206 207 208 209 210 211 212 213 214 215 21...</td>\n",
       "      <td>0.732773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141915</th>\n",
       "      <td>B9B45507DDF8</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>94 95 96 97 98 99 100 101 102 103 104 105 106 ...</td>\n",
       "      <td>0.845842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141916</th>\n",
       "      <td>B9B45507DDF8</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>217 218 219 220 221 222 223 224 225 226 227 22...</td>\n",
       "      <td>0.825821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141917</th>\n",
       "      <td>B9B45507DDF8</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>260 261 262 263 264 265 266 267 268 269 270 27...</td>\n",
       "      <td>0.881068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141918 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                 class  \\\n",
       "0       4BB5ADD5A1FE                  Lead   \n",
       "1       4BB5ADD5A1FE              Position   \n",
       "2       4BB5ADD5A1FE                 Claim   \n",
       "3       4BB5ADD5A1FE                 Claim   \n",
       "4       4BB5ADD5A1FE                 Claim   \n",
       "...              ...                   ...   \n",
       "141913  B9B45507DDF8                 Claim   \n",
       "141914  B9B45507DDF8                 Claim   \n",
       "141915  B9B45507DDF8              Evidence   \n",
       "141916  B9B45507DDF8              Evidence   \n",
       "141917  B9B45507DDF8  Concluding Statement   \n",
       "\n",
       "                                         predictionstring    scores  \n",
       "0       3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20...  0.897571  \n",
       "1       22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 3...  0.881595  \n",
       "2       40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 5...  0.486816  \n",
       "3       58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 7...  0.834153  \n",
       "4                        74 75 76 77 78 79 80 81 82 83 84  0.869926  \n",
       "...                                                   ...       ...  \n",
       "141913  78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 9...  0.763800  \n",
       "141914  205 206 207 208 209 210 211 212 213 214 215 21...  0.732773  \n",
       "141915  94 95 96 97 98 99 100 101 102 103 104 105 106 ...  0.845842  \n",
       "141916  217 218 219 220 221 222 223 224 225 226 227 22...  0.825821  \n",
       "141917  260 261 262 263 264 265 266 267 268 269 270 27...  0.881068  \n",
       "\n",
       "[141918 rows x 4 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8606bc9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T14:12:48.697441Z",
     "start_time": "2022-03-15T14:12:44.305604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7018252559328992, {'Lead': 0.8377944325481799, 'Position': 0.7223823246878002, 'Evidence': 0.7630203759002283, 'Claim': 0.6689434747543138, 'Concluding Statement': 0.8664773774844206, 'Counterclaim': 0.5750184774575019, 'Rebuttal': 0.47914032869785084})\n"
     ]
    }
   ],
   "source": [
    "sub = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": sub_sample_ids,\n",
    "        \"class\": sub_cat_names,\n",
    "        \"predictionstring\": sub_spans,\n",
    "        \"scores\": sub_scores,\n",
    "    }\n",
    ")\n",
    "\n",
    "df = pd.read_csv(osp.join(DATASET_PATH, \"train.csv\"))\n",
    "\n",
    "val_df = df[[\"id\", \"discourse_type\", \"predictionstring\"]].reset_index(drop=True)\n",
    "val_df.shape\n",
    "\n",
    "print(score_feedback_comp2(sub, val_df, return_class_scores=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5bfea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T06:53:18.956457Z",
     "start_time": "2022-03-04T06:53:18.954173Z"
    }
   },
   "outputs": [],
   "source": [
    "STOP!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec8fb03",
   "metadata": {},
   "source": [
    "## Understanding the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca0445",
   "metadata": {},
   "source": [
    "## Extract Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b70fbae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T06:21:17.747728Z",
     "start_time": "2022-03-04T06:21:17.745763Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_ix = 241\n",
    "out, token_n = all_outs[sample_ix], all_token_nums[sample_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c05d76d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T07:33:02.035049Z",
     "start_time": "2022-03-05T07:33:02.033273Z"
    }
   },
   "outputs": [],
   "source": [
    "index_map, bounds = all_word_indices[sample_ix], all_bounds[sample_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "368f98b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T06:21:36.693931Z",
     "start_time": "2022-03-04T06:21:36.690551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2048, 15), 437)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape, token_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "07119454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T07:33:05.286459Z",
     "start_time": "2022-03-05T07:33:05.283424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 2],\n",
       "       [2, 7],\n",
       "       ...,\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f15a1a41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T06:20:29.753595Z",
     "start_time": "2022-03-05T06:20:29.747578Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def extract_entities(preds, token_n):\n",
    "    class_preds = preds.argmax(-1)\n",
    "    all_entities = {}\n",
    "    current_class = None\n",
    "    current_start = None\n",
    "    for ix in range(1, token_n - 1):\n",
    "#         print('pred cat', class_preds[ix], 'current cat', current_class)\n",
    "        if class_preds[ix] % 2 == 1:\n",
    "            if current_class is not None:\n",
    "                if current_class not in all_entities:\n",
    "                    all_entities[current_class] = []\n",
    "#                 print(f'added class {current_class}!')\n",
    "                all_entities[current_class].append((current_start, ix - 1))\n",
    "            current_class = (class_preds[ix] + 1) // 2\n",
    "            current_start = ix        \n",
    "        \n",
    "    if current_class is not None:\n",
    "        if current_class not in all_entities:\n",
    "            all_entities[current_class] = []\n",
    "        all_entities[current_class].append((current_start, ix))\n",
    "    \n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2280a464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T07:12:29.217660Z",
     "start_time": "2022-03-05T07:12:29.212396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [(1, 24)],\n",
       " 2: [(25, 33)],\n",
       " 4: [(34, 116), (117, 131), (223, 231), (314, 326)],\n",
       " 3: [(132, 142), (143, 222), (232, 313), (327, 386)],\n",
       " 5: [(387, 435)]}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_out = extract_entities(out, token_n)\n",
    "extract_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e0b2e6",
   "metadata": {},
   "source": [
    "### map span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a11c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_span_to_word_indices(span, index_map, bounds):\n",
    "    return (index_map[bounds[span[0], 0]], index_map[bounds[span[1], 1] - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "759c325c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T07:35:34.320441Z",
     "start_time": "2022-03-05T07:35:34.318221Z"
    }
   },
   "outputs": [],
   "source": [
    "span = (1, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "39da9533",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T07:37:16.945242Z",
     "start_time": "2022-03-05T07:37:16.942159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 103)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first token's start index\n",
    "# last tokens's end index\n",
    "bounds[span[0], 0], bounds[span[1], 1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ba56eaf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T07:38:53.458113Z",
     "start_time": "2022-03-05T07:38:53.454260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 19)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the word index\n",
    "index_map[bounds[span[0], 0]], index_map[bounds[span[1], 1] - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "04d8caf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T07:33:08.947493Z",
     "start_time": "2022-03-05T07:33:08.945311Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_spans = {x: [map_span_to_word_indices(span, index_map, bounds) for span in y]\n",
    "                   for x, y in extract_out.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ff869526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T07:33:11.342062Z",
     "start_time": "2022-03-05T07:33:11.339083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [(0, 19)],\n",
       " 2: [(19, 26)],\n",
       " 4: [(26, 97), (98, 107), (187, 192), (265, 277)],\n",
       " 3: [(107, 117), (117, 186), (192, 264), (277, 328)],\n",
       " 5: [(329, 375)]}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abdd31f",
   "metadata": {},
   "source": [
    "### function split_predstrig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "89bec96f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T06:37:15.306403Z",
     "start_time": "2022-03-04T06:37:15.303593Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_predstring(predstring):\n",
    "    vals = predstring.split()\n",
    "    return int(vals[0]), int(vals[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6218f56a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T06:37:40.117914Z",
     "start_time": "2022-03-04T06:37:40.114215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 44)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_predstring(val_df.predictionstring[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "308.164px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
