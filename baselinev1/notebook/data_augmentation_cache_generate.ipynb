{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bfd5972",
   "metadata": {},
   "source": [
    "# Notebook to generate Data Augmentation Cache\n",
    "\n",
    "> Some augmentation like Word2Vec Sentence Replacement tooks a huge amount of time. This notebook is a place to process those heavy augmentations and store the result as a cache. The cache saved here could directly be called later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_fold in range(5):\n",
    "    # create token length\n",
    "    val_files = val_files_all[val_fold]\n",
    "    dataset = Dataset(val_files)\n",
    "    token_nums = [dataset[i][-1] for i in range(len(dataset))]\n",
    "\n",
    "    # argsort to create new sorted indices by token length\n",
    "    ix_sort = np.argsort(token_nums)[::-1]\n",
    "    val_files = [val_files[ix] for ix in ix_sort]\n",
    "    dataset = Dataset(val_files)\n",
    "\n",
    "    # model prediction\n",
    "    \n",
    "    # revert original indices\n",
    "    outs_f.append(all_outs[np.argsort(ix_sort)])\n",
    "    bounds_f.append(all_bounds[np.argsort(ix_sort)])\n",
    "    token_nums_f.append(all_token_nums[np.argsort(ix_sort)])\n",
    "    word_indices_f += [all_word_indices[ix] for ix in np.argsort(ix_sort)]\n",
    "    sample_ids_f += [all_sample_ids[ix] for ix in np.argsort(ix_sort)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4c3a40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T10:07:58.933985Z",
     "start_time": "2022-03-10T10:07:56.906523Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import deque\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append('./codes/new_transformers_branch/transformers/src')\n",
    "\n",
    "from new_transformers import DebertaV2TokenizerFast\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "import gensim\n",
    "from textaugment import Word2vec\n",
    "from textaugment import EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abcf69c",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b8b8eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T10:07:58.939857Z",
     "start_time": "2022-03-10T10:07:58.934954Z"
    }
   },
   "outputs": [],
   "source": [
    "from module.utils import get_all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efacef2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T10:07:58.958103Z",
     "start_time": "2022-03-10T10:07:58.941260Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    parser = argparse.ArgumentParser(description=\"use huggingface models\")\n",
    "    parser.add_argument(\"--dataset_path\", default='../../feedback-prize-2021', type=str)\n",
    "    args = parser.parse_args([])\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1fe9637",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T10:07:58.969241Z",
     "start_time": "2022-03-10T10:07:58.959092Z"
    }
   },
   "outputs": [],
   "source": [
    "args = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d7612ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T10:07:59.739594Z",
     "start_time": "2022-03-10T10:07:58.970394Z"
    }
   },
   "outputs": [],
   "source": [
    "all_texts = get_all_texts(args)\n",
    "df = pd.read_csv(osp.join(args.dataset_path, 'train.csv'))\n",
    "text_ids = df.id.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd15ea",
   "metadata": {},
   "source": [
    "## Generate Text List with all the data provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0b78d7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T10:07:59.746174Z",
     "start_time": "2022-03-10T10:07:59.740548Z"
    }
   },
   "outputs": [],
   "source": [
    "def text2list(text, text_df, clean_text_df=True):\n",
    "    \"\"\"Convert the text to list\n",
    "    This is mainly to work on data augmentation and noise injection\n",
    "\n",
    "    I'm working now quark! -> [[Lead, I'm working\"],\n",
    "                               [Nonez, \" \"],\n",
    "                               [Claim, \"now quark!\"]]\n",
    "\n",
    "    Args:\n",
    "        text (str): literally the text of each text_id returns\n",
    "        text_df (pandas.DataFrame): the dataframe file for each text\n",
    "        clean_text_df (bool): text files and discourse_text in train.csv file doesn't match\n",
    "                            fix the text to which is stored in the \"{text_id}.txt\" files\n",
    "\n",
    "    Returns:\n",
    "        text_list (list): list that stores the divided text and category of each text\n",
    "        text_df (pandas.DataFrame): the dataframe file for each text\n",
    "\n",
    "    \"\"\"\n",
    "    text_df = text_df.copy()\n",
    "\n",
    "    text_list = []\n",
    "    first_sentence = True\n",
    "    last_end_idx = 0\n",
    "    for row in text_df.itertuples():\n",
    "        start_idx = int(row.discourse_start)\n",
    "        end_idx = int(row.discourse_end)\n",
    "        cat = row.discourse_type\n",
    "\n",
    "        # the first sentence that will stored in the list\n",
    "        if first_sentence:\n",
    "            # when the first sentence is not the entity\n",
    "            # 1. store the first sentence with none entity\n",
    "            # 2. store the entity sentence\n",
    "            if start_idx != 0:\n",
    "                text_list.append([\"None\", text[:start_idx]])\n",
    "\n",
    "            # save the entity\n",
    "            text_list.append([cat, text[start_idx:end_idx]])\n",
    "            first_sentence = False\n",
    "            last_end_idx = end_idx\n",
    "        else:\n",
    "            # when there is a middle sentence save it also\n",
    "            if last_end_idx != start_idx:\n",
    "                middle_text = text[last_end_idx:start_idx]\n",
    "                text_list.append([\"None\", middle_text])\n",
    "\n",
    "            # save the entity\n",
    "            text_list.append([cat, text[start_idx:end_idx]])\n",
    "            last_end_idx = end_idx\n",
    "\n",
    "    # when there is sentence left store it\n",
    "    text_len = len(text)\n",
    "    if last_end_idx != text_len:\n",
    "        last_text = text[last_end_idx:text_len]\n",
    "        text_list.append([\"None\", last_text])\n",
    "\n",
    "    if clean_text_df:\n",
    "        discourse_texts = []\n",
    "        for discourse_type, discourse_text in text_list:\n",
    "            if discourse_type != 'None':\n",
    "                discourse_texts.append(discourse_text)\n",
    "\n",
    "        text_df.loc[text_df.index, 'discourse_text'] = discourse_texts\n",
    "\n",
    "    return text_list, text_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4972331",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6dba82353744caf96793b72742c0431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dict = {}\n",
    "for text_id in tqdm(text_ids, total=len(text_ids)):\n",
    "    # load data\n",
    "    text = all_texts[text_id]\n",
    "    text_df = df.query('id == @text_id').reset_index(drop=True).copy()\n",
    "\n",
    "    # convert to list and clean the text_df\n",
    "    text_list, text_df = text2list(text, text_df, clean_text_df=True)\n",
    "\n",
    "    # save as dictionary format\n",
    "    data_dict[text_id] = {}\n",
    "    data_dict[text_id]['text_list'] = text_list\n",
    "    data_dict[text_id]['text_df'] = text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9956649f",
   "metadata": {},
   "source": [
    "## Word2Vec Sentence Exchanging Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60030c4a",
   "metadata": {},
   "source": [
    "### Load Google Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f60453",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.367Z"
    }
   },
   "outputs": [],
   "source": [
    "google_model = gensim.models.KeyedVectors.load_word2vec_format('./augmentation/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "word2vec_model = Word2vec(model=google_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2320665d",
   "metadata": {},
   "source": [
    "### Convert sentence using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54508261",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.368Z"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820d637",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.369Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_sentence(text_id):\n",
    "    text_list = data_dict[text_id]['text_list']\n",
    "    \n",
    "    for i, text in enumerate(text_list):\n",
    "        # replacing sentence with similiar word\n",
    "        if len(text[1].strip()) <= 15:\n",
    "            continue\n",
    "\n",
    "        text_list[i][1] = word2vec_model.augment(text[1])\n",
    "    \n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff3260",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.369Z"
    }
   },
   "outputs": [],
   "source": [
    "test = data_dict[text_ids[0]]['text_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef4354",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.370Z"
    }
   },
   "outputs": [],
   "source": [
    " for i, text in enumerate(test):\n",
    "    # replacing sentence with similiar word\n",
    "    if len(text[1].strip()) <= 15:\n",
    "        continue\n",
    "\n",
    "    test[i][1] = word2vec_model.augment(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d36da",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.371Z"
    }
   },
   "outputs": [],
   "source": [
    "pool = Pool(processes=8)\n",
    "\n",
    "converted_textlist = []\n",
    "with tqdm(total=len(text_ids)) as pbar:\n",
    "    for text_list in pool.imap(convert_sentence, text_ids):\n",
    "        converted_textlist.append(converted_textlist)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad97591",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.371Z"
    }
   },
   "outputs": [],
   "source": [
    "STOP!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ecb46",
   "metadata": {},
   "source": [
    "## Tokenizer Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed39c0a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.372Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('microsoft/deberta-v3-large')\n",
    "tokenizer.model_max_length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f731c5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.373Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c33578",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.373Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer('a b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a11854",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.374Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer('a b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fdd0a8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.375Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer('ab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958878bb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.375Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer('word to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159734c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.376Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer('hello this is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6b7ff",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.377Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer('hello^this^is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ec8b3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.377Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer('wordto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d5d0e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.378Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.decode([1, 725, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6114b3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.378Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.decode([1, 264, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef0c8e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.379Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_outs = tokenizer('ducky', return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d2858",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-10T10:07:57.380Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_outs['offset_mapping']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258.261px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
