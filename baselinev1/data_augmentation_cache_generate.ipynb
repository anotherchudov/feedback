{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bfd5972",
   "metadata": {},
   "source": [
    "# Notebook to generate Data Augmentation Cache\n",
    "\n",
    "> Some augmentation like Word2Vec Sentence Replacement tooks a huge amount of time. This notebook is a place to process those heavy augmentations and store the result as a cache. The cache saved here could directly be called later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a4c3a40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T14:16:14.568230Z",
     "start_time": "2022-03-07T14:16:14.563320Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import deque\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append('./codes/new_transformers_branch/transformers/src')\n",
    "\n",
    "from new_transformers import DebertaV2TokenizerFast\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "import gensim\n",
    "from textaugment import Word2vec\n",
    "from textaugment import EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abcf69c",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02b8b8eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T14:16:03.829159Z",
     "start_time": "2022-03-07T14:16:03.827051Z"
    }
   },
   "outputs": [],
   "source": [
    "from module.utils import get_all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efacef2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T14:16:46.282183Z",
     "start_time": "2022-03-07T14:16:46.279482Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    parser = argparse.ArgumentParser(description=\"use huggingface models\")\n",
    "    parser.add_argument(\"--dataset_path\", default='../../feedback-prize-2021', type=str)\n",
    "    args = parser.parse_args([])\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1fe9637",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T14:16:46.562017Z",
     "start_time": "2022-03-07T14:16:46.559494Z"
    }
   },
   "outputs": [],
   "source": [
    "args = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d7612ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T14:16:57.505723Z",
     "start_time": "2022-03-07T14:16:57.255971Z"
    }
   },
   "outputs": [],
   "source": [
    "all_texts = get_all_texts(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b0b5198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T14:17:51.234989Z",
     "start_time": "2022-03-07T14:17:50.659395Z"
    }
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv(osp.join(args.dataset_path, 'train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd15ea",
   "metadata": {},
   "source": [
    "## Generate Text List with all the data provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b78d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2list(text, text_df, clean_text_df=True):\n",
    "    \"\"\"Convert the text to list\n",
    "    This is mainly to work on data augmentation and noise injection\n",
    "\n",
    "    I'm working now quark! -> [[Lead, I'm working\"],\n",
    "                               [Nonez, \" \"],\n",
    "                               [Claim, \"now quark!\"]]\n",
    "\n",
    "    Args:\n",
    "        text (str): literally the text of each text_id returns\n",
    "        text_df (pandas.DataFrame): the dataframe file for each text\n",
    "        clean_text_df (bool): text files and discourse_text in train.csv file doesn't match\n",
    "                            fix the text to which is stored in the \"{text_id}.txt\" files\n",
    "\n",
    "    Returns:\n",
    "        text_list (list): list that stores the divided text and category of each text\n",
    "        text_df (pandas.DataFrame): the dataframe file for each text\n",
    "\n",
    "    \"\"\"\n",
    "    text_df = text_df.copy()\n",
    "\n",
    "    text_list = []\n",
    "    first_sentence = True\n",
    "    last_end_idx = 0\n",
    "    for row in text_df.itertuples():\n",
    "        start_idx = int(row.discourse_start)\n",
    "        end_idx = int(row.discourse_end)\n",
    "        cat = row.discourse_type\n",
    "\n",
    "        # the first sentence that will stored in the list\n",
    "        if first_sentence:\n",
    "            # when the first sentence is not the entity\n",
    "            # 1. store the first sentence with none entity\n",
    "            # 2. store the entity sentence\n",
    "            if start_idx != 0:\n",
    "                text_list.append([\"None\", text[:start_idx]])\n",
    "\n",
    "            # save the entity\n",
    "            text_list.append([cat, text[start_idx:end_idx]])\n",
    "            first_sentence = False\n",
    "            last_end_idx = end_idx\n",
    "        else:\n",
    "            # when there is a middle sentence save it also\n",
    "            if last_end_idx != start_idx:\n",
    "                middle_text = text[last_end_idx:start_idx]\n",
    "                text_list.append([\"None\", middle_text])\n",
    "\n",
    "            # save the entity\n",
    "            text_list.append([cat, text[start_idx:end_idx]])\n",
    "            last_end_idx = end_idx\n",
    "\n",
    "    # when there is sentence left store it\n",
    "    text_len = len(text)\n",
    "    if last_end_idx != text_len:\n",
    "        last_text = text[last_end_idx:text_len]\n",
    "        text_list.append([\"None\", last_text])\n",
    "\n",
    "    if clean_text_df:\n",
    "        discourse_texts = []\n",
    "        for discourse_type, discourse_text in text_list:\n",
    "            if discourse_type != 'None':\n",
    "                discourse_texts.append(discourse_text)\n",
    "\n",
    "        text_df.loc[text_df.index, 'discourse_text'] = discourse_texts\n",
    "\n",
    "    return text_list, text_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07777944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258.261px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
