{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d718a24",
   "metadata": {},
   "source": [
    "# Online Tokenizer\n",
    "\n",
    "> Literally, the title is all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "97bb1d4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T09:08:05.552901Z",
     "start_time": "2022-03-06T09:08:05.549009Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import re\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "\n",
    "from module.utils import get_data_files\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "sys.path.insert(0, './codes/new_transformers_branch/transformers/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6c3c80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T07:31:31.117765Z",
     "start_time": "2022-03-03T07:31:31.105477Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    parser = argparse.ArgumentParser(description=\"use huggingface models\")\n",
    "    parser.add_argument(\"--dataset_path\", default='../../feedback-prize-2021', type=str)\n",
    "    parser.add_argument(\"--save_path\", default='result', type=str)\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument(\"--min_len\", default=0, type=int)\n",
    "    parser.add_argument(\"--use_groupped_weights\", default=False, type=bool)\n",
    "    parser.add_argument(\"--global_attn\", default=False, type=int)\n",
    "    parser.add_argument(\"--epochs\", default=9, type=int)\n",
    "    parser.add_argument(\"--batch_size\", default=4, type=int)\n",
    "    parser.add_argument(\"--grad_acc_steps\", default=2, type=int)\n",
    "    parser.add_argument(\"--grad_checkpt\", default=True, type=bool)\n",
    "    parser.add_argument(\"--data_prefix\", default='', type=str)\n",
    "    parser.add_argument(\"--max_grad_norm\", default=10.0, type=float)\n",
    "    parser.add_argument(\"--start_eval_at\", default=0, type=int)\n",
    "    parser.add_argument(\"--weight_decay\", default=1e-2, type=float)\n",
    "    parser.add_argument(\"--weights_pow\", default=0.1, type=float)\n",
    "    parser.add_argument(\"--dataset_version\", default=2, type=int)\n",
    "    parser.add_argument(\"--decay_bias\", default=False, type=bool)\n",
    "    parser.add_argument(\"--val_fold\", default=0, type=int)\n",
    "    parser.add_argument(\"--num_worker\", default=8, type=int)\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"do not modify!\")\n",
    "    parser.add_argument(\"--device\", type=int, default=0, help=\"select the gpu device to train\")\n",
    "\n",
    "    # logging\n",
    "    parser.add_argument(\"--wandb_user\", default='ducky', type=str)\n",
    "    parser.add_argument(\"--wandb_project\", default='feedback_deberta_large', type=str)\n",
    "    parser.add_argument(\"--wandb_comment\", default=\"\", type=str, help=\"comment will be added at the back of wandb project name\")\n",
    "    parser.add_argument(\"--print_acc\", default=500, type=int, help=\"print accuracy of each class every `print_acc` steps\")\n",
    "\n",
    "    # optimizer\n",
    "    parser.add_argument(\"--label_smoothing\", default=0.1, type=float)\n",
    "    parser.add_argument(\"--rce_weight\", default=0.1, type=float)\n",
    "    parser.add_argument(\"--ce_weight\", default=0.9, type=float)\n",
    "    parser.add_argument(\"--nesterov\", default=True, type=bool, help=\"use nesterov for SGD\")\n",
    "    parser.add_argument(\"--momentum\", default=0.9, type=float, help=\"momentum for SGD\")\n",
    "\n",
    "    # scheduler\n",
    "    parser.add_argument(\"--lr\", default=3e-5, type=float)\n",
    "    parser.add_argument(\"--min_lr\", default=1e-6, type=float)\n",
    "    parser.add_argument(\"--warmup_steps\", default=500, type=int)\n",
    "    parser.add_argument(\"--gamma\", default=0.8, type=float, help=\"gamma for cosine annealing warmup restart scheduler\")\n",
    "    parser.add_argument(\"--cycle_mult\", default=1.0, type=float, help=\"cycle length adjustment for cosine annealing warmup restart scheduler\")\n",
    "\n",
    "    # model related arguments\n",
    "    parser.add_argument(\"--model\", default=\"microsoft/deberta-v3-large\", type=str)\n",
    "    parser.add_argument(\"--cnn1d\", default=False, type=bool)\n",
    "    parser.add_argument(\"--extra_dense\", default= False, type=bool)\n",
    "    parser.add_argument(\"--dropout_ratio\", default=0.0, type=float)\n",
    "\n",
    "    # swa\n",
    "    parser.add_argument(\"--swa\", action=\"store_true\", help=\"use stochastic weight averaging\")\n",
    "    parser.add_argument(\"--swa_update_per_epoch\", default=3, type=int)\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    if args.local_rank !=-1:\n",
    "        print('[ DDP ] local rank', args.local_rank)\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        dist.init_process_group(backend='nccl')\n",
    "        args.device = torch.device(\"cuda\", args.local_rank)\n",
    "        args.rank = torch.distributed.get_rank()\n",
    "        args.world_size = torch.distributed.get_world_size()  \n",
    "\n",
    "        # checking settings for distributed training\n",
    "        assert args.batch_size % args.world_size == 0, f'--batch_size {args.batch_size} must be multiple of world size'\n",
    "        assert torch.cuda.device_count() > args.local_rank, 'insufficient CUDA devices for DDP command'\n",
    "\n",
    "        args.ddp = True\n",
    "    else:\n",
    "        args.device = torch.device(\"cuda\", args.device)\n",
    "        args.rank = -1\n",
    "        args.ddp = False\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3177934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T07:31:31.959342Z",
     "start_time": "2022-03-03T07:31:31.118731Z"
    }
   },
   "outputs": [],
   "source": [
    "args = get_config()\n",
    "all_texts, token_weights, data, csv, train_ids, val_ids, train_text_ids, val_text_ids = get_data_files(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7eb2543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T07:31:31.968351Z",
     "start_time": "2022-03-03T07:31:31.964345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B72D0B4875B4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_id = train_text_ids[0]\n",
    "text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfcd571e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T07:31:31.986588Z",
     "start_time": "2022-03-03T07:31:31.969376Z"
    }
   },
   "outputs": [],
   "source": [
    "text = all_texts[text_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b6a329",
   "metadata": {},
   "source": [
    "## DebertaV3 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c74ce312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:21:36.822542Z",
     "start_time": "2022-03-03T08:21:36.819880Z"
    }
   },
   "outputs": [],
   "source": [
    "from new_transformers import DebertaV2TokenizerFast\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cca2eb6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T07:32:51.130444Z",
     "start_time": "2022-03-03T07:32:43.473405Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('microsoft/deberta-v3-large')\n",
    "tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3653da92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:22:02.120162Z",
     "start_time": "2022-03-03T08:21:54.168792Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "auto_tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-large')\n",
    "auto_tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b319b47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:18:02.951582Z",
     "start_time": "2022-03-03T08:18:02.947567Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1, 266, 2], [1, 2165, 2]], 'token_type_ids': [[0, 0, 0], [0, 0, 0]], 'attention_mask': [[1, 1, 1], [1, 1, 1]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4bdd59f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:18:15.372975Z",
     "start_time": "2022-03-03T08:18:15.370178Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 266, 507, 2], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('a\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "499b95b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:22:12.839721Z",
     "start_time": "2022-03-03T08:22:12.836358Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 266, 2], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_tokenizer('a\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cf17059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:20:53.194014Z",
     "start_time": "2022-03-03T08:20:53.190471Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 266, 507, 2], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('a\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfa741db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:18:22.206659Z",
     "start_time": "2022-03-03T08:18:22.203842Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 507, 2], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e784830d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:23:50.557485Z",
     "start_time": "2022-03-03T08:23:50.553461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<0x08> <0x0C>'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode([12, 507, 16])\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3cd3277",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:24:23.230237Z",
     "start_time": "2022-03-03T08:24:23.226407Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<0x08> <0x0C>'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa30bc",
   "metadata": {},
   "source": [
    "### newline (\\n) is removed by DebertaV3 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375daa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_text = lambda x: x.replace('\\n', '‽')\n",
    "\n",
    "text = fix_text(f.read().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3629ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_outs = tokenizer(text, return_offsets_mapping=True)\n",
    "\n",
    "# token replacement ‽ -> [MASK]\n",
    "tokenizer_outs['input_ids'] = [x if x != 126861 else 128000 for x in tokenizer_outs['input_ids']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852c511a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### what about prediction string that matters `' '`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c5eec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "char_start = discourse_start\n",
    "char_end = discourse_end\n",
    "word_start = len(full_text[:char_start].split())\n",
    "word_end = word_start + len(full_text[char_start:char_end].split())\n",
    "word_end = min( word_end, len(full_text.split()) )\n",
    "predictionstring = \" \".join( [str(x) for x in range(word_start,word_end)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd36fdec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T07:16:10.756800Z",
     "start_time": "2022-03-03T07:16:10.752888Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', 'a']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'  a'.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ca1c0",
   "metadata": {},
   "source": [
    "### Does Tokenizer `model_max_length` is a problem?\n",
    "\n",
    "> this is weird "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5ed1f1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T12:27:05.734223Z",
     "start_time": "2022-03-05T12:26:57.915949Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('microsoft/deberta-v3-large')\n",
    "tokenizer.model_max_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2bf0801d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T12:27:27.589551Z",
     "start_time": "2022-03-05T12:27:27.584703Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 5). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁do', '▁really', '▁token', 'izer', '▁cut', '▁in', '▁length', '▁5', '?']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('do really tokenizer cut in length 5?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f50d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = re.compile('[0-9a-zA-z]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e43c9",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "> everything is controlled by `text_id`\n",
    "\n",
    "1. text is preprocessed (text -> list)\n",
    "    - strip\n",
    "    - newline replacing\n",
    "    - **change text to list**\n",
    "2. augmentation & noise injection (list -> text)\n",
    "    - one char changing\n",
    "    - one char removing\n",
    "    - [text augmentation](https://www.kaggle.com/c/feedback-prize-2021/discussion/295277)\n",
    "    - **change list to text**\n",
    "3. calculate entity boundaries\n",
    "    - use boundary supported by train.csv\n",
    "    - alternatively could calculate entity boundary with noise elimination\n",
    "4. tokenize the text\n",
    "    - token id\n",
    "    - mask\n",
    "    - offset\n",
    "5. calculate the label by using `entity boundary` and `offset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "192658c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T07:12:33.123429Z",
     "start_time": "2022-03-06T07:12:33.119935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lead': 1,\n",
       " 'Position': 3,\n",
       " 'Evidence': 5,\n",
       " 'Claim': 7,\n",
       " 'Concluding Statement': 9,\n",
       " 'Counterclaim': 11,\n",
       " 'Rebuttal': 13}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = {\n",
    "            'Lead': '#8000ff',\n",
    "            'Position': '#2b7ff6',\n",
    "            'Evidence': '#2adddd',\n",
    "            'Claim': '#80ffb4',\n",
    "            'Concluding Statement': 'd4dd80',\n",
    "            'Counterclaim': '#ff8042',\n",
    "            'Rebuttal': '#ff0000'\n",
    "         }\n",
    "cat2id = dict(zip(colors, range(1, 2 * len(colors), 2)))\n",
    "cat2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d207d9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T12:33:11.385557Z",
     "start_time": "2022-03-05T12:33:11.382471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B72D0B4875B4'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "31fa9b00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T06:47:21.042353Z",
     "start_time": "2022-03-06T06:47:21.039992Z"
    }
   },
   "outputs": [],
   "source": [
    "text = all_texts[text_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ef40f126",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T06:54:58.601920Z",
     "start_time": "2022-03-06T06:54:58.596760Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_df = csv.query('id == @text_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cbf8cf",
   "metadata": {},
   "source": [
    "### Text -> List\n",
    "\n",
    "> clean the text_df to match the text file's content also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "47485d12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T06:47:22.355227Z",
     "start_time": "2022-03-06T06:47:22.351794Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B72D0B4875B4'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "466f102d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T06:54:53.930582Z",
     "start_time": "2022-03-06T06:54:53.922873Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def text2list(text, text_df, clean_text_df=True):\n",
    "    \"\"\"Convert the text to list\n",
    "    This is mainly to work on data augmentation and noise injection\n",
    "    \n",
    "    I'm working now quark! -> [[Lead, I'm working\"],\n",
    "                               [Nonez, \" \"],\n",
    "                               [Claim, \"now quark!\"]]\n",
    "    \n",
    "    Args:\n",
    "        text (str): literally the text of each text_id returns\n",
    "        text_df (pandas.DataFrame): the dataframe file for each text\n",
    "        clean_text_df (bool): text files and discourse_text in train.csv file doesn't match\n",
    "                              fix the text to which is stored in the \"{text_id}.txt\" files\n",
    "        \n",
    "    Returns:\n",
    "        text_list (list): list that stores the divided text and category of each text\n",
    "        text_df (pandas.DataFrame): the dataframe file for each text\n",
    "\n",
    "    \"\"\"\n",
    "    text_df = text_df.copy()\n",
    "    \n",
    "    text_list = []\n",
    "    first_sentence = True\n",
    "    last_end_idx = 0\n",
    "    for row in text_df.itertuples():\n",
    "        start_idx = int(row.discourse_start)\n",
    "        end_idx = int(row.discourse_end)\n",
    "        cat = row.discourse_type\n",
    "\n",
    "        # the first sentence that will stored in the list\n",
    "        if first_sentence:\n",
    "            # when the first sentence is not the entity\n",
    "            # 1. store the first sentence with none entity\n",
    "            # 2. store the entity sentence\n",
    "            if start_idx != 0:\n",
    "                text_list.append([\"None\", text[:start_idx]])\n",
    "\n",
    "            # save the entity\n",
    "            text_list.append([cat, text[start_idx:end_idx]])\n",
    "            first_sentence = False\n",
    "            last_end_idx = end_idx\n",
    "        else:\n",
    "            # when there is a middle sentence save it also\n",
    "            if last_end_idx != start_idx:\n",
    "                middle_text = text[last_end_idx:start_idx]\n",
    "                text_list.append([\"None\", middle_text])\n",
    "\n",
    "            # save the entity\n",
    "            text_list.append([cat, text[start_idx:end_idx]])\n",
    "            last_end_idx = end_idx\n",
    "\n",
    "    # when there is sentence left store it\n",
    "    text_len = len(text)\n",
    "    if last_end_idx != text_len:\n",
    "        last_text = text[last_end_idx:text_len]\n",
    "        text_list.append([\"None\", last_text])\n",
    "        \n",
    "    if clean_text_df:\n",
    "        discourse_texts = []\n",
    "        for discourse_type, discourse_text in text_list:\n",
    "            if discourse_type != 'None':\n",
    "                discourse_texts.append(discourse_text)\n",
    "                \n",
    "        text_df.loc[text_df.index, 'discourse_text'] = discourse_texts\n",
    "        \n",
    "    return text_list, text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "70a3c5fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T06:55:37.829411Z",
     "start_time": "2022-03-06T06:55:37.826053Z"
    }
   },
   "outputs": [],
   "source": [
    "text_list, clean_text_df = text2list(text, text_df, clean_text_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "3ee70d04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T06:55:13.212319Z",
     "start_time": "2022-03-06T06:55:13.209131Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It is unfair because the people's votes might be overuled \""
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.iloc[2].discourse_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "dda4aef0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T06:55:38.318940Z",
     "start_time": "2022-03-06T06:55:38.316065Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It is unfair because the people's votes might be overuled,\""
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_df.iloc[2].discourse_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf1195d",
   "metadata": {},
   "source": [
    "### List -> Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "991ce05a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T06:58:58.226596Z",
     "start_time": "2022-03-06T06:58:58.220009Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def list2text(text_list, text_df, return_df=False):\n",
    "    \"\"\"Convert the text to list\n",
    "    Convert the list to text after data augmentation and noise injection\n",
    "    \n",
    "    [[Lead, I'm working\"],\n",
    "     [None, \" \"],\n",
    "     [Claim, \"now quark!\"]]\n",
    "    -> I'm working now quark!\n",
    "    Args:\n",
    "        text_list (list): list that stores the divided text and category of each text\n",
    "        text_df (pandas.DataFrame): the dataframe file for each text\n",
    "        return_df (bool): If the augmentation is hard enough\n",
    "                          to change the word or sentence different from original\n",
    "                          recalculate the text_df totally with prediction string also\n",
    "    \n",
    "    Returns:\n",
    "        text (str): Merged text from text_list\n",
    "        text_df (optional[pandas.DataFrame]): None, or the dataframe file for each text\n",
    "                                              if return_df is True\n",
    "    \"\"\"\n",
    "    text_df = text_df.copy()\n",
    "    \n",
    "    # convert to text\n",
    "    text = ''.join(np.array(text_list)[:, 1])\n",
    "    \n",
    "    if not return_df:\n",
    "        return text, text_df\n",
    "    \n",
    "    # convert to text_df\n",
    "    text_id = text_df.id.iloc[0]\n",
    "\n",
    "    last_position = 0\n",
    "    discourses = []\n",
    "    for discourse_type, discourse_text in text_list:\n",
    "        text_len = len(discourse_text)\n",
    "        if discourse_type != \"None\":\n",
    "            discourse_start = last_position\n",
    "            discourse_end = last_position + text_len\n",
    "            discourse_rows = {'id': text_id,\n",
    "                              'discourse_start': discourse_start,\n",
    "                              'discourse_end': discourse_end,\n",
    "                              'discourse_text': discourse_text,\n",
    "                              'discourse_type': discourse_type}\n",
    "            discourses.append(discourse_rows)\n",
    "\n",
    "        last_position += text_len\n",
    "    \n",
    "    text_df = pd.DataFrame(discourses)\n",
    "    \n",
    "    # recalculate prediction string\n",
    "    text_df['predictionstring'] = text_df[['discourse_start', 'discourse_end']].apply(calculate_predictionstring, axis=1)\n",
    "    \n",
    "    return text, text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "d5b33225",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T06:58:58.718927Z",
     "start_time": "2022-03-06T06:58:58.715517Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_predictionstring(row):\n",
    "    \"\"\"recalculate prediction string for the augmented text data\n",
    "    \n",
    "    reference - https://www.kaggle.com/c/feedback-prize-2021/discussion/297591\n",
    "    \"\"\"\n",
    "    word_start = len(text[:row.discourse_start].split())\n",
    "    word_end = word_start + len(text[row.discourse_start:row.discourse_end].split())\n",
    "    word_end = min(word_end, len(text.split()))\n",
    "\n",
    "    predictionstring = \" \".join([str(x) for x in range(word_start, word_end)])\n",
    "    \n",
    "    return predictionstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ffbe1924",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T06:59:16.944951Z",
     "start_time": "2022-03-06T06:59:16.942230Z"
    }
   },
   "outputs": [],
   "source": [
    "new_text, new_text_df = list2text(text_list, clean_text_df, return_df=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f24da6",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "b98fe6ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T08:38:12.048093Z",
     "start_time": "2022-03-06T08:38:04.389823Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('microsoft/deberta-v3-large')\n",
    "tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "753b271b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T09:06:53.755698Z",
     "start_time": "2022-03-06T09:06:53.750604Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_outs = tokenizer(new_text, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e3933b",
   "metadata": {},
   "source": [
    "### Entity Boundary Calculation\n",
    "- with `LRU cache`: text is preserved except some minor changed that doens't need to consider\n",
    "- with out `LRU cache`: text is changed a lot so need recalculate the boundary again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "da94e886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T06:59:20.018363Z",
     "start_time": "2022-03-06T06:59:20.016164Z"
    }
   },
   "outputs": [],
   "source": [
    "alphanumeric_re = re.compile('[0-9a-zA-z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6725af5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T13:44:48.859707Z",
     "start_time": "2022-03-06T13:44:48.842609Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "j\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "4eadaced",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T07:02:51.073219Z",
     "start_time": "2022-03-06T07:02:51.063993Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_entity_boundary(text, text_df):\n",
    "    \"\"\"built by sergei chudov\"\"\"\n",
    "    ent_boundaries = []\n",
    "    pointer = 0\n",
    "    \n",
    "    for row in text_df.itertuples():\n",
    "        entity_text = row.discourse_text\n",
    "\n",
    "        # regex to find text start with alphanumeric (a-zA-Z0-9)\n",
    "        entity_text = entity_text[next(alphanumeric_re.finditer(entity_text)).start():]\n",
    "        \n",
    "        # if the first character length is 1, then check the previous text chunk\n",
    "        if len(entity_text.split()[0]) == 1 and pointer != 0:\n",
    "            entity_start_ix = text[pointer:].index(entity_text)\n",
    "            prev_text = text[:pointer + entity_start_ix]\n",
    "            \n",
    "            # current text is not the beginning and the previous text last char is alphanumeric\n",
    "            if pointer + entity_start_ix > 0 and prev_text[-1].isalpha():\n",
    "                cut_word_chunk_size = len(prev_text.split()[-1])\n",
    "                \n",
    "                # if the previous text last word length is not 1\n",
    "                if cut_word_chunk_size > 1:\n",
    "                    entity_text = entity_text[next(alphanumeric_re.finditer(entity_text[1:])).start() + 1:]\n",
    "\n",
    "        offset = text[pointer:].index(entity_text)\n",
    "        starts_at = offset + pointer\n",
    "        ent_boundaries.append((starts_at, starts_at + len(entity_text), row.discourse_type))\n",
    "        pointer = starts_at + len(entity_text)\n",
    "            \n",
    "    return ent_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6237d061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T13:44:40.405359Z",
     "start_time": "2022-03-06T13:44:40.390476Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lru_cache' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1409364/1008340317.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lru_cache' is not defined"
     ]
    }
   ],
   "source": [
    "@lru_cache(maxsize=5)\n",
    "def test(text_id):\n",
    "    print(text_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "cfad9aca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T09:09:22.347106Z",
     "start_time": "2022-03-06T09:09:22.341352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 92, 'Lead'),\n",
       " (93, 130, 'Position'),\n",
       " (131, 189, 'Claim'),\n",
       " (190, 222, 'Claim'),\n",
       " (227, 284, 'Claim'),\n",
       " (285, 356, 'Claim'),\n",
       " (357, 690, 'Evidence'),\n",
       " (691, 783, 'Claim'),\n",
       " (783, 1287, 'Evidence'),\n",
       " (1288, 1445, 'Claim'),\n",
       " (1446, 1540, 'Counterclaim'),\n",
       " (1541, 1572, 'Rebuttal'),\n",
       " (1573, 1902, 'Evidence'),\n",
       " (1903, 2117, 'Concluding Statement')]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_boundary = get_entity_boundary(new_text, new_text_df)\n",
    "entity_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "8706849b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T09:09:48.835392Z",
     "start_time": "2022-03-06T09:09:48.831674Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 92.0, 'Lead'],\n",
       "       [93.0, 130.0, 'Position'],\n",
       "       [131.0, 189.0, 'Claim'],\n",
       "       [190.0, 222.0, 'Claim'],\n",
       "       [226.0, 284.0, 'Claim'],\n",
       "       [285.0, 356.0, 'Claim'],\n",
       "       [357.0, 690.0, 'Evidence'],\n",
       "       [691.0, 783.0, 'Claim'],\n",
       "       [783.0, 1287.0, 'Evidence'],\n",
       "       [1288.0, 1445.0, 'Claim'],\n",
       "       [1446.0, 1540.0, 'Counterclaim'],\n",
       "       [1541.0, 1572.0, 'Rebuttal'],\n",
       "       [1573.0, 1902.0, 'Evidence'],\n",
       "       [1903.0, 2117.0, 'Concluding Statement']], dtype=object)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_entity_boundary = new_text_df[['discourse_start', 'discourse_end', 'discourse_type']].values\n",
    "original_entity_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00ddd0",
   "metadata": {},
   "source": [
    "### Token Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e07ed6b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T09:08:53.037569Z",
     "start_time": "2022-03-06T09:08:53.026536Z"
    }
   },
   "outputs": [],
   "source": [
    "def token_labeling(tokenizer_outs, ent_boundaries, cat2id):\n",
    "    \"\"\"label the tokens\"\"\"\n",
    "\n",
    "    all_boundaries = deque([])\n",
    "    for ent_boundary in ent_boundaries:\n",
    "        for position, boundary_type in zip(ent_boundary[:2], ('start', 'end')):\n",
    "            discourse_type = ent_boundary[-1]\n",
    "            all_boundaries.append((position, discourse_type, boundary_type))\n",
    "            \n",
    "    current_target = 0\n",
    "    targets = np.zeros(len(tokenizer_outs['input_ids']), dtype='i8')\n",
    "    token_positions = np.vstack(tokenizer_outs['offset_mapping']).astype('i4')\n",
    "    \n",
    "    for token_ix in range(len(tokenizer_outs['input_ids'])):\n",
    "        token_start_ix, token_end_ix = tokenizer_outs['offset_mapping'][token_ix]\n",
    "        \n",
    "        cur_pos, cur_cat_type, cur_bound_type = all_boundaries[0]\n",
    "\n",
    "        if token_end_ix != 0 \\\n",
    "           and (cur_bound_type == 'end' and token_end_ix >= cur_pos) \\\n",
    "           or (cur_bound_type == 'start' and token_end_ix > cur_pos):\n",
    "            \n",
    "            if len(all_boundaries) > 1:\n",
    "                next_pos, next_dis_type, next_bound_type = all_boundaries[1]\n",
    "            if cur_bound_type == 'start':\n",
    "                # token map {'Lead': 1, 'Position': 3, ..., 'Rebuttal': 13}\n",
    "                current_target = cat2id[cur_cat_type]\n",
    "                targets[token_ix] = current_target\n",
    "                \n",
    "                if token_end_ix == next_pos:\n",
    "                    current_target = 0\n",
    "                    all_boundaries.popleft()\n",
    "                else:\n",
    "                    current_target += 1\n",
    "            else:\n",
    "                # If there is more entity left to consider and current is already on the next pos\n",
    "                if len(all_boundaries) > 1 and token_end_ix > next_pos:\n",
    "                    \n",
    "                    # can this actually happen?\n",
    "                    if token_start_ix >= next_pos:\n",
    "                        assert text[cur_pos - 1] == '¨'\n",
    "\n",
    "                    all_boundaries.popleft()\n",
    "                    current_target = cat2id[cur_cat_type]\n",
    "                    targets[token_ix] = current_target\n",
    "                    current_target += 1\n",
    "                else:\n",
    "                    if token_start_ix >= cur_pos:\n",
    "                        current_target = 0\n",
    "\n",
    "                    targets[token_ix] = current_target\n",
    "                    current_target = 0\n",
    "\n",
    "            all_boundaries.popleft()\n",
    "            if not all_boundaries:\n",
    "                break\n",
    "        else:\n",
    "            targets[token_ix] = current_target\n",
    "            \n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8fe706b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T14:49:37.556698Z",
     "start_time": "2022-03-06T14:49:37.552382Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 1258, 355, 428, 272, 262, 11992, 1575, 269, 298, 3092, 260, 273, 418, 280, 297, 428, 306, 280, 368, 1299, 260, 279, 11992, 1575, 269, 379, 10425, 260, 325, 269, 10425, 401, 262, 355, 280, 268, 5125, 520, 282, 360, 6013, 569, 261, 262, 11992, 1575, 269, 16348, 261, 263, 306, 372, 298, 794, 356, 1251, 264, 262, 355, 280, 268, 4713, 260, 1244, 265, 305, 261, 267, 347, 1281, 262, 1123, 1647, 520, 298, 282, 265, 356, 772, 260, 5216, 261, 262, 1123, 1647, 702, 280, 297, 912, 267, 262, 1129, 270, 1574, 278, 74392, 742, 270, 262, 11992, 1575, 260, 471, 337, 273, 849, 264, 1647, 270, 266, 35018, 1574, 304, 262, 11992, 1575, 3110, 322, 5147, 270, 262, 59483, 273, 338, 286, 10487, 312, 326, 446, 264, 1647, 260, 11727, 261, 262, 11992, 1575, 280, 268, 1647, 85399, 268, 262, 1123, 1647, 260, 344, 908, 264, 360, 92364, 262, 1123, 1647, 261, 262, 355, 277, 262, 11992, 1575, 281, 16348, 260, 369, 262, 355, 328, 281, 277, 262, 11992, 1575, 281, 16348, 393, 261, 306, 520, 298, 413, 355, 280, 268, 5125, 352, 914, 264, 308, 11992, 1647, 260, 273, 391, 363, 311, 338, 409, 308, 1647, 264, 282, 6935, 324, 291, 269, 501, 919, 579, 262, 11992, 1575, 403, 282, 28114, 260, 369, 266, 6053, 666, 262, 355, 2116, 328, 308, 1840, 281, 261, 393, 579, 281, 343, 340, 355, 328, 520, 298, 2993, 262, 355, 31778, 268, 510, 316, 1574, 260, 12208, 3488, 261, 262, 11992, 1575, 269, 16348, 263, 520, 298, 599, 314, 262, 1123, 1647, 260, 4676, 261, 402, 337, 262, 355, 280, 268, 5125, 424, 264, 262, 11992, 1575, 261, 262, 1575, 520, 298, 402, 794, 1251, 264, 262, 1725, 355, 527, 264, 1647, 270, 760, 260, 1258, 355, 418, 280, 297, 431, 599, 272, 308, 5125, 424, 264, 7051, 328, 269, 277, 262, 11992, 1575, 261, 304, 306, 281, 2367, 262, 582, 260, 279, 11992, 1575, 281, 412, 322, 265, 857, 10765, 263, 306, 296, 504, 363, 264, 1000, 306, 418, 280, 297, 334, 260, 273, 418, 280, 297, 428, 272, 262, 7691, 265, 316, 658, 403, 282, 10986, 277, 262, 781, 272, 337, 311, 604, 702, 280, 297, 334, 760, 306, 296, 9933, 262, 781, 260, 7616, 261, 262, 11992, 1575, 520, 9933, 347, 1000, 348, 401, 306, 418, 280, 297, 334, 349, 260, 502, 9326, 261, 262, 11992, 1575, 1037, 282, 5131, 264, 2116, 262, 723, 265, 316, 426, 2080, 260, 329, 596, 265, 58189, 520, 282, 16348, 261, 308, 5125, 85399, 262, 1123, 1647, 261, 263, 306, 520, 298, 282, 615, 264, 4713, 260, 7282, 378, 1725, 270, 262, 11992, 1575, 298, 7051, 262, 723, 265, 262, 780, 1017, 260, 606, 355, 1037, 282, 5131, 264, 1647, 270, 316, 353, 1574, 260, 507, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 4), (4, 11), (11, 17), (17, 22), (22, 26), (26, 36), (36, 44), (44, 47), (47, 51), (51, 62), (62, 63), (63, 65), (65, 69), (69, 70), (70, 71), (71, 77), (77, 82), (82, 83), (83, 85), (85, 91), (91, 92), (92, 96), (96, 106), (106, 114), (114, 117), (117, 122), (122, 129), (129, 130), (130, 133), (133, 136), (136, 143), (143, 151), (151, 155), (155, 162), (162, 163), (163, 164), (164, 170), (170, 176), (176, 179), (179, 184), (184, 186), (186, 188), (188, 189), (189, 193), (193, 203), (203, 211), (211, 214), (214, 221), (221, 222), (222, 226), (226, 231), (231, 235), (235, 239), (239, 243), (243, 247), (247, 257), (257, 260), (260, 264), (264, 271), (271, 272), (272, 273), (273, 282), (282, 283), (284, 290), (290, 293), (293, 297), (297, 298), (298, 301), (301, 306), (306, 312), (312, 316), (316, 324), (324, 329), (329, 335), (335, 339), (339, 342), (342, 345), (345, 349), (349, 355), (355, 356), (356, 364), (364, 365), (365, 369), (369, 377), (377, 382), (382, 388), (388, 389), (389, 390), (390, 397), (397, 400), (400, 404), (404, 413), (413, 417), (417, 427), (427, 430), (430, 437), (437, 445), (445, 449), (449, 453), (453, 463), (463, 471), (471, 472), (472, 475), (475, 478), (478, 480), (480, 487), (487, 490), (490, 495), (495, 499), (499, 501), (501, 512), (512, 522), (522, 526), (526, 530), (530, 540), (540, 548), (548, 553), (553, 556), (556, 563), (563, 567), (567, 571), (571, 580), (580, 582), (582, 588), (588, 593), (593, 600), (600, 603), (603, 608), (608, 614), (614, 617), (617, 622), (622, 623), (623, 631), (631, 632), (632, 636), (636, 646), (646, 654), (654, 655), (655, 656), (656, 661), (661, 670), (670, 671), (671, 675), (675, 683), (683, 688), (688, 689), (690, 693), (693, 702), (702, 705), (705, 710), (710, 716), (716, 720), (720, 728), (728, 733), (733, 734), (734, 738), (738, 745), (745, 748), (748, 752), (752, 762), (762, 770), (770, 774), (774, 781), (781, 782), (782, 785), (785, 789), (789, 796), (796, 800), (800, 804), (804, 807), (807, 811), (811, 821), (821, 829), (829, 833), (833, 840), (840, 845), (845, 846), (846, 851), (851, 857), (857, 861), (861, 866), (866, 873), (873, 874), (874, 875), (875, 881), (881, 886), (886, 894), (894, 897), (897, 903), (903, 913), (913, 918), (918, 919), (919, 921), (921, 926), (926, 929), (929, 933), (933, 939), (939, 944), (944, 950), (950, 955), (955, 958), (958, 961), (961, 969), (969, 972), (972, 977), (977, 980), (980, 988), (988, 995), (995, 999), (999, 1003), (1003, 1013), (1013, 1021), (1021, 1028), (1028, 1031), (1031, 1041), (1041, 1042), (1042, 1045), (1045, 1047), (1047, 1057), (1057, 1063), (1063, 1067), (1067, 1074), (1074, 1081), (1081, 1085), (1085, 1091), (1091, 1099), (1099, 1103), (1103, 1104), (1104, 1109), (1109, 1113), (1113, 1117), (1117, 1123), (1123, 1129), (1129, 1136), (1136, 1140), (1140, 1146), (1146, 1150), (1150, 1160), (1160, 1164), (1164, 1171), (1171, 1175), (1175, 1176), (1176, 1179), (1179, 1183), (1183, 1193), (1193, 1194), (1194, 1200), (1200, 1210), (1210, 1211), (1211, 1215), (1215, 1225), (1225, 1233), (1233, 1236), (1236, 1243), (1243, 1247), (1247, 1253), (1253, 1257), (1257, 1262), (1262, 1268), (1268, 1272), (1272, 1280), (1280, 1285), (1285, 1286), (1287, 1299), (1299, 1300), (1300, 1305), (1305, 1308), (1308, 1312), (1312, 1319), (1319, 1320), (1320, 1321), (1321, 1327), (1327, 1330), (1330, 1333), (1333, 1337), (1337, 1347), (1347, 1355), (1355, 1356), (1356, 1360), (1360, 1368), (1368, 1374), (1374, 1378), (1378, 1383), (1383, 1387), (1387, 1397), (1397, 1400), (1400, 1404), (1404, 1412), (1412, 1419), (1419, 1424), (1424, 1427), (1427, 1432), (1432, 1436), (1436, 1444), (1444, 1445), (1445, 1450), (1450, 1457), (1457, 1461), (1461, 1462), (1462, 1463), (1463, 1470), (1470, 1475), (1475, 1480), (1480, 1486), (1486, 1492), (1492, 1495), (1495, 1498), (1498, 1507), (1507, 1511), (1511, 1514), (1514, 1517), (1517, 1521), (1521, 1531), (1531, 1539), (1539, 1540), (1540, 1544), (1544, 1549), (1549, 1553), (1553, 1561), (1561, 1565), (1565, 1571), (1571, 1572), (1572, 1576), (1576, 1586), (1586, 1594), (1594, 1598), (1598, 1603), (1603, 1606), (1606, 1609), (1609, 1615), (1615, 1622), (1622, 1626), (1626, 1631), (1631, 1636), (1636, 1640), (1640, 1643), (1643, 1646), (1646, 1652), (1652, 1657), (1657, 1661), (1661, 1662), (1662, 1663), (1663, 1668), (1668, 1669), (1669, 1671), (1671, 1675), (1675, 1676), (1676, 1677), (1677, 1683), (1683, 1688), (1688, 1692), (1692, 1697), (1697, 1700), (1700, 1704), (1704, 1712), (1712, 1719), (1719, 1722), (1722, 1730), (1730, 1733), (1733, 1737), (1737, 1742), (1742, 1747), (1747, 1750), (1750, 1754), (1754, 1761), (1761, 1767), (1767, 1768), (1768, 1769), (1769, 1774), (1774, 1782), (1782, 1787), (1787, 1792), (1792, 1799), (1799, 1803), (1803, 1808), (1808, 1809), (1809, 1819), (1819, 1820), (1820, 1824), (1824, 1834), (1834, 1842), (1842, 1848), (1848, 1855), (1855, 1860), (1860, 1866), (1866, 1871), (1871, 1879), (1879, 1884), (1884, 1888), (1888, 1889), (1889, 1890), (1890, 1895), (1895, 1900), (1900, 1901), (1902, 1905), (1905, 1914), (1914, 1915), (1915, 1919), (1919, 1929), (1929, 1937), (1937, 1944), (1944, 1947), (1947, 1955), (1955, 1958), (1958, 1965), (1965, 1969), (1969, 1976), (1976, 1979), (1979, 1983), (1983, 1989), (1989, 1996), (1996, 1997), (1997, 2002), (2002, 2008), (2008, 2011), (2011, 2020), (2020, 2026), (2026, 2029), (2029, 2036), (2036, 2037), (2037, 2043), (2043, 2049), (2049, 2058), (2058, 2062), (2062, 2070), (2070, 2075), (2075, 2076), (2076, 2080), (2080, 2085), (2085, 2091), (2091, 2095), (2095, 2098), (2098, 2103), (2103, 2106), (2106, 2115), (2115, 2116), (2116, 2125), (2125, 2131), (2131, 2139), (2139, 2143), (2143, 2147), (2147, 2157), (2157, 2165), (2165, 2169), (2169, 2178), (2178, 2182), (2182, 2189), (2189, 2192), (2192, 2196), (2196, 2203), (2203, 2210), (2210, 2211), (2211, 2217), (2217, 2224), (2224, 2231), (2231, 2234), (2234, 2242), (2242, 2245), (2245, 2250), (2250, 2254), (2254, 2258), (2258, 2262), (2262, 2272), (2272, 2273), (2276, 2277), (0, 0)]}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "e229f67d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T09:08:53.645808Z",
     "start_time": "2022-03-06T09:08:53.638228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  3,  4,  4,  4,  4,  4,  4,  7,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  7,  8,  8,  8,  8,  8,  0,\n",
       "        7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  7,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  5,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  5,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  5,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 14, 14, 14, 14, 14, 14,\n",
       "        5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_labeling(tokenizer_outs, entity_boundary, cat2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "812452a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T15:24:19.291560Z",
     "start_time": "2022-03-06T15:24:19.283440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  3,  4,  4,  4,  4,  4,  4,  7,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  7,  8,  8,  8,  8,  8,  0,\n",
       "        7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  7,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  5,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  5,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  5,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 14, 14, 14, 14, 14, 14,\n",
       "        5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = token_labeling(tokenizer_outs, original_entity_boundary, cat2id)\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20372c1b",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2c39fc6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T15:17:51.667115Z",
     "start_time": "2022-03-06T15:17:51.663414Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_one_hot(indices, num_labels):\n",
    "    array = np.zeros((len(indices), num_labels))\n",
    "    array[np.arange(len(indices)), indices.astype('i4')] = 1\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "03c7c63e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T15:24:21.211665Z",
     "start_time": "2022-03-06T15:24:21.207472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467, 15)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = make_one_hot(token, 15)\n",
    "token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "2c00fdd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T15:24:22.254349Z",
     "start_time": "2022-03-06T15:24:22.250649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "56830989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T15:27:52.714645Z",
     "start_time": "2022-03-06T15:27:52.711314Z"
    }
   },
   "outputs": [],
   "source": [
    "label = np.zeros((2048, 15), dtype='f4')\n",
    "label[:len(token)] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "0710288a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T15:27:53.607476Z",
     "start_time": "2022-03-06T15:27:53.603584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 15)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580fd04b",
   "metadata": {},
   "source": [
    "### Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a61c2dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T05:49:12.419474Z",
     "start_time": "2022-03-03T05:49:12.414987Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedbackDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, text_ids, csv, all_texts, token_weights\n",
    "    ):\n",
    "        self.csv = csv\n",
    "        self.all_texts = all_texts\n",
    "        self.text_ids = text_ids\n",
    "        self.class_names = class_names\n",
    "        self.token_weights = token_weights\n",
    "        \n",
    "        # store original data as desired dictionary format\n",
    "        initialize_data_dict()\n",
    "        \n",
    "        self.space_regex = re.compile(\"[\\s\\n]\")\n",
    "        \n",
    "    def initialize_data_dict(self, preprocess=False):\n",
    "        \"\"\"save original data by dictionary with text_id key\n",
    "        \n",
    "        preprocess includes\n",
    "        - strip\n",
    "        - newline exchange \n",
    "        \n",
    "        {\n",
    "         ...\n",
    "         text_id: {'text_list': text_list, 'text_df': text_df},\n",
    "         text_id: {'text_list': text_list, 'text_df': text_df}\n",
    "         text_id: {'text_list': text_list, 'text_df': text_df}\n",
    "         ...\n",
    "         }\n",
    "\n",
    "        1. text_list\n",
    "        2. test_df\n",
    "        \"\"\"\n",
    "        self.original_data = {}\n",
    "        for text_id in text_ids:\n",
    "            # load original data\n",
    "            text = self.all_texts[text_id]\n",
    "            text_df = self.csv.query('id == @text_id').reset_index(drop=True).copy()\n",
    "            \n",
    "            # convert to the dictionary format\n",
    "            self.original_data[text_id] = {}\n",
    "            self.original_data[text_id]['text_list'] = text2list(text, text_df)\n",
    "            self.original_data[text_id]['text_df'] = text_df\n",
    "        \n",
    "    def noise_injection(self, text):\n",
    "        ...\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.strip()\n",
    "        \n",
    "        # newline is removed from debertav3 tokenizer\n",
    "        text = text.replace('\\n', '‽')\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        text_id = self.text_ids[idx]\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ae5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __getitem__(self, idx):\n",
    "        i = self.ids[idx]\n",
    "\n",
    "        # load text data & text dataframe\n",
    "        text_id = self.val_text_ids[idx]\n",
    "        text = self.all_texts[text_id]\n",
    "        sample_df = self.csv.query(\"id == @text_id\")\n",
    "\n",
    "        # load ground truth prediction string for f1macro metric\n",
    "        gt_dict = {}\n",
    "        for class_i in range(1, 8):\n",
    "            class_name = self.class_names[class_i]\n",
    "            class_df = sample_df.query(\"discourse_type == @class_name\")\n",
    "            if len(class_df):\n",
    "                gt_dict[class_i] = [\n",
    "                    (x[0], x[1])\n",
    "                    for x in class_df.predictionstring.map(split_predstring)\n",
    "                ]\n",
    "\n",
    "        # load valid data\n",
    "        tokens = self.data[\"tokens\"][i]\n",
    "        attention_mask = self.data[\"attention_masks\"][i]\n",
    "        num_tokens = self.data[\"num_tokens\"][i, 0]\n",
    "        token_bounds = self.data[\"token_offsets\"][i]\n",
    "        cbio_labels = self.data[\"cbio_labels\"][i]\n",
    "\n",
    "        # class weight per token\n",
    "        class_weight = np.zeros_like(attention_mask)\n",
    "        argmax_labels = cbio_labels.argmax(-1)\n",
    "\n",
    "        for class_i in range(1, 15):\n",
    "            class_weight[argmax_labels == class_i] = self.token_weights[class_i]\n",
    "\n",
    "        class_none_index = argmax_labels == 0\n",
    "        class_none_index[num_tokens - 1 :] = False\n",
    "        class_weight[class_none_index] = self.token_weights[0]\n",
    "        class_weight[0] = 0\n",
    "\n",
    "        # ???\n",
    "        index_map = []\n",
    "        current_word = 0\n",
    "        blank = False\n",
    "        for char_ix in range(text.index(text.strip()[0]), len(text)):\n",
    "            if self.space_regex.match(text[char_ix]) is not None:\n",
    "                blank = True\n",
    "            elif blank:\n",
    "                current_word += 1\n",
    "                blank = False\n",
    "            index_map.append(current_word)\n",
    "\n",
    "        return (\n",
    "            tokens,\n",
    "            attention_mask,\n",
    "            cbio_labels,\n",
    "            class_weight,\n",
    "            token_bounds,\n",
    "            gt_dict,\n",
    "            index_map,\n",
    "            num_tokens,\n",
    "        )\n",
    "\n",
    "\n",
    "first_batch = True\n",
    "\n",
    "\n",
    "def train_collate_fn(ins):\n",
    "    global first_batch\n",
    "    if first_batch:\n",
    "        max_len = 2048\n",
    "        first_batch = False\n",
    "    else:\n",
    "        max_len = (max(x[-1] for x in ins) + 7) // 8 * 8\n",
    "\n",
    "    return tuple(\n",
    "        torch.from_numpy(\n",
    "            np.concatenate([ins[z][x][None, :max_len] for z in range(len(ins))])\n",
    "        )\n",
    "        for x in range(len(ins[0]) - 1)\n",
    "    )\n",
    "\n",
    "\n",
    "def val_collate_fn(ins):\n",
    "    max_len = (max(x[-1] for x in ins) + 7) // 8 * 8\n",
    "    return tuple(\n",
    "        torch.from_numpy(\n",
    "            np.concatenate([ins[z][x][None, :max_len] for z in range(len(ins))])\n",
    "        )\n",
    "        for x in range(len(ins[0]) - 3)\n",
    "    ) + (\n",
    "        [x[-3] for x in ins],\n",
    "        [x[-2] for x in ins],\n",
    "        np.array([x[-1] for x in ins]),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    args,\n",
    "    train_ids,\n",
    "    val_ids,\n",
    "    data,\n",
    "    csv,\n",
    "    all_texts,\n",
    "    val_text_ids,\n",
    "    class_names,\n",
    "    token_weights,\n",
    "):\n",
    "    train_dataset = TrainDataset(\n",
    "        train_ids, data, args.label_smoothing, token_weights, args.data_prefix\n",
    "    )\n",
    "    val_dataset = ValDataset(\n",
    "        val_ids, data, csv, all_texts, val_text_ids, class_names, token_weights\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        collate_fn=train_collate_fn,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_worker,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        collate_fn=val_collate_fn,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=8,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207.649px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
