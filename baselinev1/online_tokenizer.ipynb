{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d718a24",
   "metadata": {},
   "source": [
    "# Online Tokenizer\n",
    "\n",
    "> Literally, the title is all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bb1d4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T07:31:31.098324Z",
     "start_time": "2022-03-03T07:31:29.503060Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from module.utils import get_data_files\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "sys.path.insert(0, './codes/new_transformers_branch/transformers/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6c3c80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T07:31:31.117765Z",
     "start_time": "2022-03-03T07:31:31.105477Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    parser = argparse.ArgumentParser(description=\"use huggingface models\")\n",
    "    parser.add_argument(\"--dataset_path\", default='../../feedback-prize-2021', type=str)\n",
    "    parser.add_argument(\"--save_path\", default='result', type=str)\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument(\"--min_len\", default=0, type=int)\n",
    "    parser.add_argument(\"--use_groupped_weights\", default=False, type=bool)\n",
    "    parser.add_argument(\"--global_attn\", default=False, type=int)\n",
    "    parser.add_argument(\"--epochs\", default=9, type=int)\n",
    "    parser.add_argument(\"--batch_size\", default=4, type=int)\n",
    "    parser.add_argument(\"--grad_acc_steps\", default=2, type=int)\n",
    "    parser.add_argument(\"--grad_checkpt\", default=True, type=bool)\n",
    "    parser.add_argument(\"--data_prefix\", default='', type=str)\n",
    "    parser.add_argument(\"--max_grad_norm\", default=10.0, type=float)\n",
    "    parser.add_argument(\"--start_eval_at\", default=0, type=int)\n",
    "    parser.add_argument(\"--weight_decay\", default=1e-2, type=float)\n",
    "    parser.add_argument(\"--weights_pow\", default=0.1, type=float)\n",
    "    parser.add_argument(\"--dataset_version\", default=2, type=int)\n",
    "    parser.add_argument(\"--decay_bias\", default=False, type=bool)\n",
    "    parser.add_argument(\"--val_fold\", default=0, type=int)\n",
    "    parser.add_argument(\"--num_worker\", default=8, type=int)\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"do not modify!\")\n",
    "    parser.add_argument(\"--device\", type=int, default=0, help=\"select the gpu device to train\")\n",
    "\n",
    "    # logging\n",
    "    parser.add_argument(\"--wandb_user\", default='ducky', type=str)\n",
    "    parser.add_argument(\"--wandb_project\", default='feedback_deberta_large', type=str)\n",
    "    parser.add_argument(\"--wandb_comment\", default=\"\", type=str, help=\"comment will be added at the back of wandb project name\")\n",
    "    parser.add_argument(\"--print_acc\", default=500, type=int, help=\"print accuracy of each class every `print_acc` steps\")\n",
    "\n",
    "    # optimizer\n",
    "    parser.add_argument(\"--label_smoothing\", default=0.1, type=float)\n",
    "    parser.add_argument(\"--rce_weight\", default=0.1, type=float)\n",
    "    parser.add_argument(\"--ce_weight\", default=0.9, type=float)\n",
    "    parser.add_argument(\"--nesterov\", default=True, type=bool, help=\"use nesterov for SGD\")\n",
    "    parser.add_argument(\"--momentum\", default=0.9, type=float, help=\"momentum for SGD\")\n",
    "\n",
    "    # scheduler\n",
    "    parser.add_argument(\"--lr\", default=3e-5, type=float)\n",
    "    parser.add_argument(\"--min_lr\", default=1e-6, type=float)\n",
    "    parser.add_argument(\"--warmup_steps\", default=500, type=int)\n",
    "    parser.add_argument(\"--gamma\", default=0.8, type=float, help=\"gamma for cosine annealing warmup restart scheduler\")\n",
    "    parser.add_argument(\"--cycle_mult\", default=1.0, type=float, help=\"cycle length adjustment for cosine annealing warmup restart scheduler\")\n",
    "\n",
    "    # model related arguments\n",
    "    parser.add_argument(\"--model\", default=\"microsoft/deberta-v3-large\", type=str)\n",
    "    parser.add_argument(\"--cnn1d\", default=False, type=bool)\n",
    "    parser.add_argument(\"--extra_dense\", default= False, type=bool)\n",
    "    parser.add_argument(\"--dropout_ratio\", default=0.0, type=float)\n",
    "\n",
    "    # swa\n",
    "    parser.add_argument(\"--swa\", action=\"store_true\", help=\"use stochastic weight averaging\")\n",
    "    parser.add_argument(\"--swa_update_per_epoch\", default=3, type=int)\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    if args.local_rank !=-1:\n",
    "        print('[ DDP ] local rank', args.local_rank)\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        dist.init_process_group(backend='nccl')\n",
    "        args.device = torch.device(\"cuda\", args.local_rank)\n",
    "        args.rank = torch.distributed.get_rank()\n",
    "        args.world_size = torch.distributed.get_world_size()  \n",
    "\n",
    "        # checking settings for distributed training\n",
    "        assert args.batch_size % args.world_size == 0, f'--batch_size {args.batch_size} must be multiple of world size'\n",
    "        assert torch.cuda.device_count() > args.local_rank, 'insufficient CUDA devices for DDP command'\n",
    "\n",
    "        args.ddp = True\n",
    "    else:\n",
    "        args.device = torch.device(\"cuda\", args.device)\n",
    "        args.rank = -1\n",
    "        args.ddp = False\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3177934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T07:31:31.959342Z",
     "start_time": "2022-03-03T07:31:31.118731Z"
    }
   },
   "outputs": [],
   "source": [
    "args = get_config()\n",
    "all_texts, token_weights, data, csv, train_ids, val_ids, train_text_ids, val_text_ids = get_data_files(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "105d1823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T07:31:31.968351Z",
     "start_time": "2022-03-03T07:31:31.964345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B72D0B4875B4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_id = train_text_ids[0]\n",
    "text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb2ae9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T07:31:31.986588Z",
     "start_time": "2022-03-03T07:31:31.969376Z"
    }
   },
   "outputs": [],
   "source": [
    "text = all_texts[text_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3231566c",
   "metadata": {},
   "source": [
    "## DebertaV3 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55a39c96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:21:36.822542Z",
     "start_time": "2022-03-03T08:21:36.819880Z"
    }
   },
   "outputs": [],
   "source": [
    "from new_transformers import DebertaV2TokenizerFast\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5422169",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T07:32:51.130444Z",
     "start_time": "2022-03-03T07:32:43.473405Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('microsoft/deberta-v3-large')\n",
    "tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "273f0bce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:22:02.120162Z",
     "start_time": "2022-03-03T08:21:54.168792Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "auto_tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-large')\n",
    "auto_tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef82c731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:18:02.951582Z",
     "start_time": "2022-03-03T08:18:02.947567Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1, 266, 2], [1, 2165, 2]], 'token_type_ids': [[0, 0, 0], [0, 0, 0]], 'attention_mask': [[1, 1, 1], [1, 1, 1]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "383cb309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:18:15.372975Z",
     "start_time": "2022-03-03T08:18:15.370178Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 266, 507, 2], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('a\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9b4a101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:22:12.839721Z",
     "start_time": "2022-03-03T08:22:12.836358Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 266, 2], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_tokenizer('a\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "731c87c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:20:53.194014Z",
     "start_time": "2022-03-03T08:20:53.190471Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 266, 507, 2], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('a\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca23c3ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:18:22.206659Z",
     "start_time": "2022-03-03T08:18:22.203842Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 507, 2], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e72f432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:23:50.557485Z",
     "start_time": "2022-03-03T08:23:50.553461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<0x08> <0x0C>'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode([12, 507, 16])\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "136d9b75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T08:24:23.230237Z",
     "start_time": "2022-03-03T08:24:23.226407Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<0x08> <0x0C>'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161fc3e",
   "metadata": {},
   "source": [
    "### newline (\\n) is removed by DebertaV3 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_text = lambda x: x.replace('\\n', '‽')\n",
    "\n",
    "text = fix_text(f.read().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_outs = tokenizer(text, return_offsets_mapping=True)\n",
    "\n",
    "# token replacement ‽ -> [MASK]\n",
    "tokenizer_outs['input_ids'] = [x if x != 126861 else 128000 for x in tokenizer_outs['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc17ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_start = discourse_start\n",
    "char_end = discourse_end\n",
    "word_start = len(full_text[:char_start].split())\n",
    "word_end = word_start + len(full_text[char_start:char_end].split())\n",
    "word_end = min( word_end, len(full_text.split()) )\n",
    "predictionstring = \" \".join( [str(x) for x in range(word_start,word_end)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8c35cc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T07:16:10.756800Z",
     "start_time": "2022-03-03T07:16:10.752888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', 'a']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'  a'.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = re.compile('[0-9a-zA-z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a61c2dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T05:49:12.419474Z",
     "start_time": "2022-03-03T05:49:12.414987Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedbackDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, text_ids, csv, all_texts, token_weights\n",
    "    ):\n",
    "        self.csv = csv\n",
    "        self.all_texts = all_texts\n",
    "        self.text_ids = text_ids\n",
    "        self.class_names = class_names\n",
    "        self.token_weights = token_weights\n",
    "        \n",
    "        self.space_regex = re.compile(\"[\\s\\n]\")\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        text = text.strip()\n",
    "        \n",
    "        # newline is removed from debertav3 tokenizer\n",
    "        text = text.replace('\\n', '‽')\n",
    "        text = text.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ae5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __getitem__(self, idx):\n",
    "        i = self.ids[idx]\n",
    "\n",
    "        # load text data & text dataframe\n",
    "        text_id = self.val_text_ids[idx]\n",
    "        text = self.all_texts[text_id]\n",
    "        sample_df = self.csv.query(\"id == @text_id\")\n",
    "\n",
    "        # load ground truth prediction string for f1macro metric\n",
    "        gt_dict = {}\n",
    "        for class_i in range(1, 8):\n",
    "            class_name = self.class_names[class_i]\n",
    "            class_df = sample_df.query(\"discourse_type == @class_name\")\n",
    "            if len(class_df):\n",
    "                gt_dict[class_i] = [\n",
    "                    (x[0], x[1])\n",
    "                    for x in class_df.predictionstring.map(split_predstring)\n",
    "                ]\n",
    "\n",
    "        # load valid data\n",
    "        tokens = self.data[\"tokens\"][i]\n",
    "        attention_mask = self.data[\"attention_masks\"][i]\n",
    "        num_tokens = self.data[\"num_tokens\"][i, 0]\n",
    "        token_bounds = self.data[\"token_offsets\"][i]\n",
    "        cbio_labels = self.data[\"cbio_labels\"][i]\n",
    "\n",
    "        # class weight per token\n",
    "        class_weight = np.zeros_like(attention_mask)\n",
    "        argmax_labels = cbio_labels.argmax(-1)\n",
    "\n",
    "        for class_i in range(1, 15):\n",
    "            class_weight[argmax_labels == class_i] = self.token_weights[class_i]\n",
    "\n",
    "        class_none_index = argmax_labels == 0\n",
    "        class_none_index[num_tokens - 1 :] = False\n",
    "        class_weight[class_none_index] = self.token_weights[0]\n",
    "        class_weight[0] = 0\n",
    "\n",
    "        # ???\n",
    "        index_map = []\n",
    "        current_word = 0\n",
    "        blank = False\n",
    "        for char_ix in range(text.index(text.strip()[0]), len(text)):\n",
    "            if self.space_regex.match(text[char_ix]) is not None:\n",
    "                blank = True\n",
    "            elif blank:\n",
    "                current_word += 1\n",
    "                blank = False\n",
    "            index_map.append(current_word)\n",
    "\n",
    "        return (\n",
    "            tokens,\n",
    "            attention_mask,\n",
    "            cbio_labels,\n",
    "            class_weight,\n",
    "            token_bounds,\n",
    "            gt_dict,\n",
    "            index_map,\n",
    "            num_tokens,\n",
    "        )\n",
    "\n",
    "\n",
    "first_batch = True\n",
    "\n",
    "\n",
    "def train_collate_fn(ins):\n",
    "    global first_batch\n",
    "    if first_batch:\n",
    "        max_len = 2048\n",
    "        first_batch = False\n",
    "    else:\n",
    "        max_len = (max(x[-1] for x in ins) + 7) // 8 * 8\n",
    "\n",
    "    return tuple(\n",
    "        torch.from_numpy(\n",
    "            np.concatenate([ins[z][x][None, :max_len] for z in range(len(ins))])\n",
    "        )\n",
    "        for x in range(len(ins[0]) - 1)\n",
    "    )\n",
    "\n",
    "\n",
    "def val_collate_fn(ins):\n",
    "    max_len = (max(x[-1] for x in ins) + 7) // 8 * 8\n",
    "    return tuple(\n",
    "        torch.from_numpy(\n",
    "            np.concatenate([ins[z][x][None, :max_len] for z in range(len(ins))])\n",
    "        )\n",
    "        for x in range(len(ins[0]) - 3)\n",
    "    ) + (\n",
    "        [x[-3] for x in ins],\n",
    "        [x[-2] for x in ins],\n",
    "        np.array([x[-1] for x in ins]),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    args,\n",
    "    train_ids,\n",
    "    val_ids,\n",
    "    data,\n",
    "    csv,\n",
    "    all_texts,\n",
    "    val_text_ids,\n",
    "    class_names,\n",
    "    token_weights,\n",
    "):\n",
    "    train_dataset = TrainDataset(\n",
    "        train_ids, data, args.label_smoothing, token_weights, args.data_prefix\n",
    "    )\n",
    "    val_dataset = ValDataset(\n",
    "        val_ids, data, csv, all_texts, val_text_ids, class_names, token_weights\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        collate_fn=train_collate_fn,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_worker,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        collate_fn=val_collate_fn,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=8,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
